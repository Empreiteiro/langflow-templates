{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-q1VgY",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "Agent-yHysx",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-q1VgY{œdataTypeœ:œChatInputœ,œidœ:œChatInput-q1VgYœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Agent-yHysx{œfieldNameœ:œinput_valueœ,œidœ:œAgent-yHysxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-q1VgY",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-q1VgYœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Agent-yHysx",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œAgent-yHysxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt Template",
            "id": "Prompt Template-gtXmW",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_prompt",
            "id": "Agent-yHysx",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Prompt Template-gtXmW{œdataTypeœ:œPrompt Templateœ,œidœ:œPrompt Template-gtXmWœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Agent-yHysx{œfieldNameœ:œsystem_promptœ,œidœ:œAgent-yHysxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Prompt Template-gtXmW",
        "sourceHandle": "{œdataTypeœ:œPrompt Templateœ,œidœ:œPrompt Template-gtXmWœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}",
        "target": "Agent-yHysx",
        "targetHandle": "{œfieldNameœ:œsystem_promptœ,œidœ:œAgent-yHysxœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Agent",
            "id": "Agent-yHysx",
            "name": "response",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "StructuredOutput-FZySV",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__Agent-yHysx{œdataTypeœ:œAgentœ,œidœ:œAgent-yHysxœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}-StructuredOutput-FZySV{œfieldNameœ:œinput_valueœ,œidœ:œStructuredOutput-FZySVœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "Agent-yHysx",
        "sourceHandle": "{œdataTypeœ:œAgentœ,œidœ:œAgent-yHysxœ,œnameœ:œresponseœ,œoutput_typesœ:[œMessageœ]}",
        "target": "StructuredOutput-FZySV",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œStructuredOutput-FZySVœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "LanguageModelComponent",
            "id": "LanguageModelComponent-xejSI",
            "name": "model_output",
            "output_types": [
              "LanguageModel"
            ]
          },
          "targetHandle": {
            "fieldName": "llm",
            "id": "StructuredOutput-FZySV",
            "inputTypes": [
              "LanguageModel"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__LanguageModelComponent-xejSI{œdataTypeœ:œLanguageModelComponentœ,œidœ:œLanguageModelComponent-xejSIœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-StructuredOutput-FZySV{œfieldNameœ:œllmœ,œidœ:œStructuredOutput-FZySVœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "LanguageModelComponent-xejSI",
        "sourceHandle": "{œdataTypeœ:œLanguageModelComponentœ,œidœ:œLanguageModelComponent-xejSIœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}",
        "target": "StructuredOutput-FZySV",
        "targetHandle": "{œfieldNameœ:œllmœ,œidœ:œStructuredOutput-FZySVœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ParserComponent",
            "id": "ParserComponent-nUZ5M",
            "name": "parsed_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_text",
            "id": "CustomComponent-vef9p",
            "inputTypes": [
              "Message",
              "Data",
              "DataFrame"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__ParserComponent-nUZ5M{œdataTypeœ:œParserComponentœ,œidœ:œParserComponent-nUZ5Mœ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-vef9p{œfieldNameœ:œinput_textœ,œidœ:œCustomComponent-vef9pœ,œinputTypesœ:[œMessageœ,œDataœ,œDataFrameœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "ParserComponent-nUZ5M",
        "sourceHandle": "{œdataTypeœ:œParserComponentœ,œidœ:œParserComponent-nUZ5Mœ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-vef9p",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œCustomComponent-vef9pœ,œinputTypesœ:[œMessageœ,œDataœ,œDataFrameœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ParserComponent",
            "id": "ParserComponent-E7pD3",
            "name": "parsed_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_text",
            "id": "ModalConverterComponent-NViIi",
            "inputTypes": [
              "Message",
              "Data",
              "DataFrame"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__ParserComponent-E7pD3{œdataTypeœ:œParserComponentœ,œidœ:œParserComponent-E7pD3œ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}-ModalConverterComponent-NViIi{œfieldNameœ:œinput_textœ,œidœ:œModalConverterComponent-NViIiœ,œinputTypesœ:[œMessageœ,œDataœ,œDataFrameœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "ParserComponent-E7pD3",
        "sourceHandle": "{œdataTypeœ:œParserComponentœ,œidœ:œParserComponent-E7pD3œ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ModalConverterComponent-NViIi",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œModalConverterComponent-NViIiœ,œinputTypesœ:[œMessageœ,œDataœ,œDataFrameœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ParserComponent",
            "id": "ParserComponent-ro3V7",
            "name": "parsed_text",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_text",
            "id": "ModalConverterComponent-XOkQp",
            "inputTypes": [
              "Message",
              "Data",
              "DataFrame"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__ParserComponent-ro3V7{œdataTypeœ:œParserComponentœ,œidœ:œParserComponent-ro3V7œ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}-ModalConverterComponent-XOkQp{œfieldNameœ:œinput_textœ,œidœ:œModalConverterComponent-XOkQpœ,œinputTypesœ:[œMessageœ,œDataœ,œDataFrameœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "ParserComponent-ro3V7",
        "sourceHandle": "{œdataTypeœ:œParserComponentœ,œidœ:œParserComponent-ro3V7œ,œnameœ:œparsed_textœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ModalConverterComponent-XOkQp",
        "targetHandle": "{œfieldNameœ:œinput_textœ,œidœ:œModalConverterComponent-XOkQpœ,œinputTypesœ:[œMessageœ,œDataœ,œDataFrameœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ModalConverterComponent",
            "id": "CustomComponent-vef9p",
            "name": "audio_output",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "dynamic_audio_data",
            "id": "DynamicCreateData-rTbao",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-vef9p{œdataTypeœ:œModalConverterComponentœ,œidœ:œCustomComponent-vef9pœ,œnameœ:œaudio_outputœ,œoutput_typesœ:[œDataœ]}-DynamicCreateData-rTbao{œfieldNameœ:œdynamic_audio_dataœ,œidœ:œDynamicCreateData-rTbaoœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-vef9p",
        "sourceHandle": "{œdataTypeœ:œModalConverterComponentœ,œidœ:œCustomComponent-vef9pœ,œnameœ:œaudio_outputœ,œoutput_typesœ:[œDataœ]}",
        "target": "DynamicCreateData-rTbao",
        "targetHandle": "{œfieldNameœ:œdynamic_audio_dataœ,œidœ:œDynamicCreateData-rTbaoœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ModalConverterComponent",
            "id": "ModalConverterComponent-NViIi",
            "name": "image_output",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "dynamic_image_data",
            "id": "DynamicCreateData-rTbao",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__ModalConverterComponent-NViIi{œdataTypeœ:œModalConverterComponentœ,œidœ:œModalConverterComponent-NViIiœ,œnameœ:œimage_outputœ,œoutput_typesœ:[œDataœ]}-DynamicCreateData-rTbao{œfieldNameœ:œdynamic_image_dataœ,œidœ:œDynamicCreateData-rTbaoœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "ModalConverterComponent-NViIi",
        "sourceHandle": "{œdataTypeœ:œModalConverterComponentœ,œidœ:œModalConverterComponent-NViIiœ,œnameœ:œimage_outputœ,œoutput_typesœ:[œDataœ]}",
        "target": "DynamicCreateData-rTbao",
        "targetHandle": "{œfieldNameœ:œdynamic_image_dataœ,œidœ:œDynamicCreateData-rTbaoœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ModalConverterComponent",
            "id": "ModalConverterComponent-XOkQp",
            "name": "video_output",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "dynamic_video_data",
            "id": "DynamicCreateData-rTbao",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__ModalConverterComponent-XOkQp{œdataTypeœ:œModalConverterComponentœ,œidœ:œModalConverterComponent-XOkQpœ,œnameœ:œvideo_outputœ,œoutput_typesœ:[œDataœ]}-DynamicCreateData-rTbao{œfieldNameœ:œdynamic_video_dataœ,œidœ:œDynamicCreateData-rTbaoœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "ModalConverterComponent-XOkQp",
        "sourceHandle": "{œdataTypeœ:œModalConverterComponentœ,œidœ:œModalConverterComponent-XOkQpœ,œnameœ:œvideo_outputœ,œoutput_typesœ:[œDataœ]}",
        "target": "DynamicCreateData-rTbao",
        "targetHandle": "{œfieldNameœ:œdynamic_video_dataœ,œidœ:œDynamicCreateData-rTbaoœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "DynamicCreateData",
            "id": "DynamicCreateData-rTbao",
            "name": "form_data",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-oDJcX",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__DynamicCreateData-rTbao{œdataTypeœ:œDynamicCreateDataœ,œidœ:œDynamicCreateData-rTbaoœ,œnameœ:œform_dataœ,œoutput_typesœ:[œDataœ]}-ChatOutput-oDJcX{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-oDJcXœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "DynamicCreateData-rTbao",
        "sourceHandle": "{œdataTypeœ:œDynamicCreateDataœ,œidœ:œDynamicCreateData-rTbaoœ,œnameœ:œform_dataœ,œoutput_typesœ:[œDataœ]}",
        "target": "ChatOutput-oDJcX",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-oDJcXœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "StructuredOutput",
            "id": "StructuredOutput-FZySV",
            "name": "structured_output",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_data",
            "id": "ParserComponent-nUZ5M",
            "inputTypes": [
              "DataFrame",
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__StructuredOutput-FZySV{œdataTypeœ:œStructuredOutputœ,œidœ:œStructuredOutput-FZySVœ,œnameœ:œstructured_outputœ,œoutput_typesœ:[œDataœ]}-ParserComponent-nUZ5M{œfieldNameœ:œinput_dataœ,œidœ:œParserComponent-nUZ5Mœ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "StructuredOutput-FZySV",
        "sourceHandle": "{œdataTypeœ:œStructuredOutputœ,œidœ:œStructuredOutput-FZySVœ,œnameœ:œstructured_outputœ,œoutput_typesœ:[œDataœ]}",
        "target": "ParserComponent-nUZ5M",
        "targetHandle": "{œfieldNameœ:œinput_dataœ,œidœ:œParserComponent-nUZ5Mœ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "StructuredOutput",
            "id": "StructuredOutput-FZySV",
            "name": "structured_output",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_data",
            "id": "ParserComponent-E7pD3",
            "inputTypes": [
              "DataFrame",
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__StructuredOutput-FZySV{œdataTypeœ:œStructuredOutputœ,œidœ:œStructuredOutput-FZySVœ,œnameœ:œstructured_outputœ,œoutput_typesœ:[œDataœ]}-ParserComponent-E7pD3{œfieldNameœ:œinput_dataœ,œidœ:œParserComponent-E7pD3œ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "StructuredOutput-FZySV",
        "sourceHandle": "{œdataTypeœ:œStructuredOutputœ,œidœ:œStructuredOutput-FZySVœ,œnameœ:œstructured_outputœ,œoutput_typesœ:[œDataœ]}",
        "target": "ParserComponent-E7pD3",
        "targetHandle": "{œfieldNameœ:œinput_dataœ,œidœ:œParserComponent-E7pD3œ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "StructuredOutput",
            "id": "StructuredOutput-FZySV",
            "name": "structured_output",
            "output_types": [
              "Data"
            ]
          },
          "targetHandle": {
            "fieldName": "input_data",
            "id": "ParserComponent-ro3V7",
            "inputTypes": [
              "DataFrame",
              "Data"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__StructuredOutput-FZySV{œdataTypeœ:œStructuredOutputœ,œidœ:œStructuredOutput-FZySVœ,œnameœ:œstructured_outputœ,œoutput_typesœ:[œDataœ]}-ParserComponent-ro3V7{œfieldNameœ:œinput_dataœ,œidœ:œParserComponent-ro3V7œ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "StructuredOutput-FZySV",
        "sourceHandle": "{œdataTypeœ:œStructuredOutputœ,œidœ:œStructuredOutput-FZySVœ,œnameœ:œstructured_outputœ,œoutput_typesœ:[œDataœ]}",
        "target": "ParserComponent-ro3V7",
        "targetHandle": "{œfieldNameœ:œinput_dataœ,œidœ:œParserComponent-ro3V7œ,œinputTypesœ:[œDataFrameœ,œDataœ],œtypeœ:œotherœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "id": "note-fmi5O",
          "node": {
            "description": "The goal of this workflow is to receive a text instruction and transform that instruction into three different multimodal formats: audio, image, and video.",
            "display_name": "",
            "documentation": "",
            "template": {}
          },
          "type": "note"
        },
        "dragging": false,
        "height": 324,
        "id": "note-fmi5O",
        "measured": {
          "height": 324,
          "width": 324
        },
        "position": {
          "x": -1177.3641101633057,
          "y": 828.4494077409549
        },
        "resizing": false,
        "selected": false,
        "type": "noteNode",
        "width": 324
      },
      {
        "data": {
          "id": "CustomComponent-vef9p",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": true,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Convert text to audio, image, or video using AI models.",
            "display_name": "Modal Converter",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_text",
              "output_type",
              "audio_image_provider",
              "video_provider",
              "openai_api_key",
              "gemini_api_key",
              "voice",
              "audio_format",
              "speed",
              "image_model",
              "image_size",
              "num_images",
              "video_model",
              "aspect_ratio"
            ],
            "frozen": false,
            "icon": "repeat",
            "legacy": false,
            "metadata": {
              "code_hash": "4f721424c63d",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "requests",
                    "version": "2.32.5"
                  },
                  {
                    "name": "openai",
                    "version": "1.109.1"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  },
                  {
                    "name": "google",
                    "version": "1.56.0"
                  }
                ],
                "total_dependencies": 4
              },
              "module": "custom_components.modal_converter"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Audio Data",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "generate_audio",
                "name": "audio_output",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "aspect_ratio": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Aspect Ratio",
                "dynamic": false,
                "external_options": {},
                "info": "Video format ratio",
                "name": "aspect_ratio",
                "options": [
                  "16:9",
                  "9:16"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "16:9"
              },
              "audio_format": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Audio Format",
                "dynamic": false,
                "external_options": {},
                "info": "Select audio format",
                "name": "audio_format",
                "options": [
                  "mp3",
                  "opus",
                  "aac",
                  "flac",
                  "wav",
                  "pcm"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "mp3"
              },
              "audio_image_provider": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "audio_image_provider",
                "options": [
                  "OpenAI"
                ],
                "options_metadata": [
                  {
                    "icon": "OpenAI"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "OpenAI"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\r\nimport base64\r\nimport requests\r\nimport time\r\nfrom openai import OpenAI\r\n\r\nfrom lfx.custom import Component\r\nfrom lfx.io import HandleInput, Output, TabInput, MessageTextInput, SecretStrInput, DropdownInput, IntInput\r\nfrom lfx.schema import Data, Message\r\n\r\n# Import Google GenAI for Veo video generation\r\nfrom google import genai\r\nfrom google.genai import types\r\n\r\n\r\nclass ModalConverterComponent(Component):\r\n    display_name = \"Modal Converter\"\r\n    description = \"Convert text to audio, image, or video using AI models.\"\r\n    icon = \"repeat\"\r\n    beta = True\r\n\r\n    inputs = [\r\n        HandleInput(\r\n            name=\"input_text\",\r\n            display_name=\"Input\",\r\n            input_types=[\"Message\", \"Data\", \"DataFrame\"],\r\n            info=\"Text input to convert to audio, image, or video\",\r\n            required=True,\r\n        ),\r\n        TabInput(\r\n            name=\"output_type\",\r\n            display_name=\"Output Type\",\r\n            options=[\"Audio\", \"Image\", \"Video\"],\r\n            info=\"Select the desired output media type\",\r\n            real_time_refresh=True,\r\n            value=\"Audio\",\r\n        ),\r\n        # Model provider for Audio and Image \r\n        DropdownInput(\r\n            name=\"audio_image_provider\",\r\n            display_name=\"Model Provider\",\r\n            options=[\"OpenAI\"],\r\n            value=\"OpenAI\",\r\n            real_time_refresh=True,\r\n            show=False,\r\n            options_metadata=[{\"icon\": \"OpenAI\"}],\r\n        ),\r\n        # Model provider for Video\r\n        DropdownInput(\r\n            name=\"video_provider\",\r\n            display_name=\"Model Provider\",\r\n            options=[\"Google Generative AI\"],\r\n            value=\"Google Generative AI\",\r\n            real_time_refresh=True,\r\n            show=False,\r\n            options_metadata=[{\"icon\": \"GoogleGenerativeAI\"}],\r\n        ),\r\n        SecretStrInput(\r\n            name=\"openai_api_key\",\r\n            display_name=\"OpenAI API Key\",\r\n            info=\"Your OpenAI API key for generating audio, images, and videos\",\r\n            required=True,\r\n            show=False,\r\n        ),\r\n        SecretStrInput(\r\n            name=\"gemini_api_key\",\r\n            display_name=\"Gemini API Key\",\r\n            info=\"Your Google Gemini API key for generating videos with Veo\",\r\n            required=True,\r\n            show=False,\r\n        ),\r\n        # Audio-specific options\r\n        DropdownInput(\r\n            name=\"voice\",\r\n            display_name=\"Voice\",\r\n            info=\"Select the voice for audio generation\",\r\n            options=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\r\n            value=\"alloy\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        DropdownInput(\r\n            name=\"audio_format\",\r\n            display_name=\"Audio Format\",\r\n            info=\"Select audio format\",\r\n            options=[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"],\r\n            value=\"mp3\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        IntInput(\r\n            name=\"speed\",\r\n            display_name=\"Speed\",\r\n            info=\"Speed of the generated audio. Values range from 0.25 to 4.0.\",\r\n            value=1,\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        # Image-specific options\r\n        DropdownInput(\r\n            name=\"image_model\",\r\n            display_name=\"Image Model\",\r\n            options=[\"dall-e-2\", \"dall-e-3\"],\r\n            value=\"dall-e-3\",\r\n            info=\"The DALL·E model version to use\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        DropdownInput(\r\n            name=\"image_size\",\r\n            display_name=\"Image Size\",\r\n            value=\"1024x1024\",\r\n            info=\"Size of the generated image\",\r\n            options=[\"256x256\", \"512x512\", \"1024x1024\", \"1792x1024\", \"1024x1792\"],\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        IntInput(\r\n            name=\"num_images\",\r\n            display_name=\"Number of Images\",\r\n            value=1,\r\n            info=\"Number of images to generate\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        # Video-specific options for Google Veo\r\n        DropdownInput(\r\n            name=\"video_model\",\r\n            display_name=\"Model\",\r\n            options=[\r\n                \"veo-3.0-generate-preview\",  # Latest Veo 3.0 model\r\n                \"veo-2.0-generate-001\",  # Veo 2.0 model (requires GCP billing)\r\n                \"models/veo-2.0-generate-001\",  # Full format for Veo 2.0\r\n            ],\r\n            value=\"veo-3.0-generate-preview\",\r\n            info=\"Veo model to use for video generation\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        DropdownInput(\r\n            name=\"aspect_ratio\",\r\n            display_name=\"Aspect Ratio\",\r\n            options=[\r\n                \"16:9\",  # Widescreen\r\n                \"9:16\",  # Portrait/Vertical\r\n            ],\r\n            value=\"16:9\",\r\n            info=\"Video format ratio\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Audio Data\",\r\n            name=\"audio_output\",\r\n            method=\"generate_audio\",\r\n        )\r\n    ]\r\n\r\n    def update_outputs(self, frontend_node: dict, field_name: str, field_value: Any) -> dict:\r\n        \"\"\"Dynamically show only the relevant output based on the selected output type.\"\"\"\r\n        if field_name == \"output_type\":\r\n            # Start with empty outputs\r\n            frontend_node[\"outputs\"] = []\r\n\r\n            # Add only the selected output type\r\n            if field_value == \"Audio\":\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Audio Data\",\r\n                        name=\"audio_output\",\r\n                        method=\"generate_audio\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Audio Base64\",\r\n                        name=\"audio_base64\",\r\n                        method=\"generate_audio_base64\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Markdown\",\r\n                        name=\"markdown_output\",\r\n                        method=\"generate_markdown\",\r\n                    ).to_dict()\r\n                )\r\n            elif field_value == \"Image\":\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Image Data\",\r\n                        name=\"image_output\",\r\n                        method=\"generate_image\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Image URL\",\r\n                        name=\"image_url\",\r\n                        method=\"generate_image_url\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Markdown\",\r\n                        name=\"markdown_output\",\r\n                        method=\"generate_markdown\",\r\n                    ).to_dict()\r\n                )\r\n            elif field_value == \"Video\":\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Video Data\",\r\n                        name=\"video_output\",\r\n                        method=\"generate_video\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Video URLs\",\r\n                        name=\"video_urls\",\r\n                        method=\"generate_video_urls\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Markdown\",\r\n                        name=\"markdown_output\",\r\n                        method=\"generate_markdown\",\r\n                    ).to_dict()\r\n                )\r\n\r\n        return frontend_node\r\n\r\n    def update_build_config(self, build_config, field_value, field_name=None):\r\n        if field_name == \"output_type\":\r\n            # Extract output type from the selected value\r\n            output_type = field_value if isinstance(field_value, str) else \"Audio\"\r\n\r\n            # Define field visibility map\r\n            field_map = {\r\n                \"Audio\": [\"audio_image_provider\", \"openai_api_key\", \"voice\", \"audio_format\", \"speed\"],\r\n                \"Image\": [\"audio_image_provider\", \"openai_api_key\", \"image_model\", \"image_size\", \"num_images\"],\r\n                \"Video\": [\"video_provider\", \"gemini_api_key\", \"video_model\", \"aspect_ratio\"],\r\n            }\r\n\r\n            # Hide all dynamic fields first\r\n            for field_name in [\"audio_image_provider\", \"video_provider\", \"openai_api_key\", \"gemini_api_key\", \"voice\", \"audio_format\", \"speed\", \"image_model\", \"image_size\", \"num_images\", \"video_model\", \"aspect_ratio\"]:\r\n                if field_name in build_config:\r\n                    build_config[field_name][\"show\"] = False\r\n\r\n            # Show fields based on selected output type\r\n            if output_type in field_map:\r\n                for field_name in field_map[output_type]:\r\n                    if field_name in build_config:\r\n                        build_config[field_name][\"show\"] = True\r\n\r\n        elif field_name == \"audio_image_provider\":\r\n            # For audio and image, always show OpenAI API key\r\n            if field_value == \"OpenAI\":\r\n                build_config[\"openai_api_key\"][\"display_name\"] = \"OpenAI API Key\"\r\n                build_config[\"openai_api_key\"][\"info\"] = \"Your OpenAI API key for generating audio and images\"\r\n                build_config[\"openai_api_key\"][\"show\"] = True\r\n                if \"gemini_api_key\" in build_config:\r\n                    build_config[\"gemini_api_key\"][\"show\"] = False\r\n\r\n        elif field_name == \"video_provider\":\r\n            # For video mode, only show gemini_api_key when Google Generative AI is selected\r\n            if field_value == \"Google Generative AI\":\r\n                if \"gemini_api_key\" in build_config:\r\n                    build_config[\"gemini_api_key\"][\"display_name\"] = \"Gemini API Key\"\r\n                    build_config[\"gemini_api_key\"][\"info\"] = \"Your Google Gemini API key for generating videos with Veo\"\r\n                    build_config[\"gemini_api_key\"][\"show\"] = True\r\n\r\n        return build_config\r\n\r\n    def _extract_text_from_input(self):\r\n        \"\"\"Extract text from various input types.\"\"\"\r\n        input_value = self.input_text[0] if isinstance(self.input_text, list) else self.input_text\r\n\r\n        # Handle string input\r\n        if isinstance(input_value, str):\r\n            return input_value\r\n\r\n        # Handle Message input\r\n        if hasattr(input_value, 'text'):\r\n            return input_value.text\r\n\r\n        # Handle Data input\r\n        if hasattr(input_value, 'data'):\r\n            if isinstance(input_value.data, dict) and 'text' in input_value.data:\r\n                return input_value.data['text']\r\n            elif isinstance(input_value.data, str):\r\n                return input_value.data\r\n\r\n        # Handle DataFrame input\r\n        if hasattr(input_value, 'to_message'):\r\n            message = input_value.to_message()\r\n            return message.text if hasattr(message, 'text') else str(message)\r\n\r\n        # Fallback\r\n        return str(input_value)\r\n\r\n    def _get_api_key(self):\r\n        \"\"\"Get API key based on selected provider and output type.\"\"\"\r\n        # For video generation, always use Gemini API\r\n        if hasattr(self, 'output_type') and self.output_type == \"Video\":\r\n            api_key = self.gemini_api_key\r\n        else:\r\n            # For audio and image, always use OpenAI\r\n            api_key = self.openai_api_key\r\n            \r\n        if hasattr(api_key, 'get_secret_value'):\r\n            return api_key.get_secret_value()\r\n        return str(api_key)\r\n\r\n    def generate_audio(self) -> Data:\r\n        \"\"\"Generate audio from text using OpenAI TTS API.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Data(data={\"error\": \"No text provided for audio generation\"})\r\n\r\n            api_key = self._get_api_key()\r\n            voice = getattr(self, 'voice', 'alloy') or 'alloy'\r\n            format_ = getattr(self, 'audio_format', 'mp3') or 'mp3'\r\n            speed = getattr(self, 'speed', 1) or 1\r\n\r\n            url = \"https://api.openai.com/v1/audio/speech\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {api_key}\",\r\n                \"Content-Type\": \"application/json\"\r\n            }\r\n            payload = {\r\n                \"model\": \"tts-1\",\r\n                \"input\": text,\r\n                \"voice\": voice,\r\n                \"response_format\": format_,\r\n                \"speed\": float(speed)\r\n            }\r\n\r\n            self.status = f\"Generating audio with voice '{voice}' in '{format_}' format at speed {speed}x...\"\r\n            self.log(f\"Making TTS request with voice: {voice}, format: {format_}, speed: {speed}\")\r\n\r\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\r\n            \r\n            if response.status_code == 200 and response.content:\r\n                audio_base64 = base64.b64encode(response.content).decode('utf-8')\r\n                audio_size_kb = len(response.content) / 1024\r\n                \r\n                self.status = f\"Audio generated successfully! Size: {audio_size_kb:.1f} KB\"\r\n                self.log(f\"Audio generated successfully. Size: {len(response.content)} bytes\")\r\n                \r\n                return Data(data={\r\n                    \"audio_base64\": audio_base64,\r\n                    \"format\": format_,\r\n                    \"voice\": voice,\r\n                    \"speed\": speed,\r\n                    \"size_bytes\": len(response.content),\r\n                    \"text\": text[:100] + \"...\" if len(text) > 100 else text,\r\n                })\r\n            else:\r\n                error_msg = f\"API Error {response.status_code}: {response.text}\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Data(data={\"error\": error_msg})\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating audio: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Data(data={\"error\": error_msg})\r\n\r\n    def generate_audio_base64(self) -> Message:\r\n        \"\"\"Generate audio and return only the base64 as text message.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Message(text=\"Error: No text provided for audio generation\")\r\n\r\n            api_key = self._get_api_key()\r\n            voice = getattr(self, 'voice', 'alloy') or 'alloy'\r\n            format_ = getattr(self, 'audio_format', 'mp3') or 'mp3'\r\n            speed = getattr(self, 'speed', 1) or 1\r\n\r\n            url = \"https://api.openai.com/v1/audio/speech\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {api_key}\",\r\n                \"Content-Type\": \"application/json\"\r\n            }\r\n            payload = {\r\n                \"model\": \"tts-1\",\r\n                \"input\": text,\r\n                \"voice\": voice,\r\n                \"response_format\": format_,\r\n                \"speed\": float(speed)\r\n            }\r\n\r\n            self.status = f\"Generating audio with voice '{voice}' in '{format_}' format at speed {speed}x...\"\r\n            self.log(f\"Making TTS request with voice: {voice}, format: {format_}, speed: {speed}\")\r\n\r\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\r\n            \r\n            if response.status_code == 200 and response.content:\r\n                audio_base64 = base64.b64encode(response.content).decode('utf-8')\r\n                audio_size_kb = len(response.content) / 1024\r\n                \r\n                self.status = f\"Audio generated successfully! Size: {audio_size_kb:.1f} KB\"\r\n                self.log(f\"Audio generated successfully. Size: {len(response.content)} bytes\")\r\n                \r\n                return Message(text=audio_base64)\r\n            else:\r\n                error_msg = f\"API Error {response.status_code}: {response.text}\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating audio: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def generate_markdown(self) -> Message:\r\n        \"\"\"Generate markdown output based on the selected output type.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Message(text=\"Error: No text provided for markdown generation\")\r\n\r\n            output_type = getattr(self, 'output_type', 'Audio') or 'Audio'\r\n\r\n            if output_type == \"Audio\":\r\n                return self._generate_audio_markdown(text)\r\n            elif output_type == \"Image\":\r\n                return self._generate_image_markdown(text)\r\n            elif output_type == \"Video\":\r\n                return self._generate_video_markdown(text)\r\n            else:\r\n                return Message(text=\"Error: Unknown output type\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def _generate_audio_markdown(self, text: str) -> Message:\r\n        \"\"\"Generate HTML audio player code for the generated audio.\"\"\"\r\n        try:\r\n            api_key = self._get_api_key()\r\n            voice = getattr(self, 'voice', 'alloy') or 'alloy'\r\n            format_ = getattr(self, 'audio_format', 'mp3') or 'mp3'\r\n            speed = getattr(self, 'speed', 1) or 1\r\n\r\n            url = \"https://api.openai.com/v1/audio/speech\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {api_key}\",\r\n                \"Content-Type\": \"application/json\"\r\n            }\r\n            payload = {\r\n                \"model\": \"tts-1\",\r\n                \"input\": text,\r\n                \"voice\": voice,\r\n                \"response_format\": format_,\r\n                \"speed\": float(speed)\r\n            }\r\n\r\n            self.status = f\"Generating audio with voice '{voice}' in '{format_}' format at speed {speed}x...\"\r\n            self.log(f\"Making TTS request with voice: {voice}, format: {format_}, speed: {speed}\")\r\n\r\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\r\n            \r\n            if response.status_code == 200 and response.content:\r\n                audio_base64 = base64.b64encode(response.content).decode('utf-8')\r\n                audio_size_kb = len(response.content) / 1024\r\n                \r\n                self.status = f\"Audio generated successfully! Size: {audio_size_kb:.1f} KB\"\r\n                self.log(f\"Audio generated successfully. Size: {len(response.content)} bytes\")\r\n                \r\n                # Generate HTML audio player code\r\n                html_code = f'<audio controls>\\n  <source src=\"data:audio/{format_};base64,{audio_base64}\" type=\"audio/{format_}\">\\n</audio>'\r\n                \r\n                return Message(text=html_code)\r\n            else:\r\n                error_msg = f\"API Error {response.status_code}: {response.text}\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating audio markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def _generate_image_markdown(self, text: str) -> Message:\r\n        \"\"\"Generate markdown image code for the generated image.\"\"\"\r\n        try:\r\n            # Check if only one image is requested\r\n            n = getattr(self, 'num_images', 1) or 1\r\n            if n != 1:\r\n                return Message(text=\"Markdown Image output is only available when generating a single image (Number of Images = 1)\")\r\n\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'image_model', 'dall-e-3') or 'dall-e-3'\r\n            size = getattr(self, 'image_size', '1024x1024') or '1024x1024'\r\n\r\n            client = OpenAI(api_key=api_key)\r\n\r\n            self.status = f\"Generating image using {model} model...\"\r\n            self.log(f\"Making image generation request with model: {model}, size: {size}, count: {n}\")\r\n\r\n            response = client.images.generate(\r\n                model=model,\r\n                prompt=text,\r\n                n=n,\r\n                size=size\r\n            )\r\n\r\n            # Return markdown image code for the first image\r\n            if response.data and len(response.data) > 0:\r\n                image_url = response.data[0].url\r\n                # Create a description from the prompt\r\n                description = text[:50] + \"...\" if len(text) > 50 else text\r\n                markdown_code = f\"![{description}]({image_url})\"\r\n                \r\n                self.status = f\"Generated image markdown successfully!\"\r\n                self.log(f\"Generated image markdown: {markdown_code}\")\r\n                \r\n                return Message(text=markdown_code)\r\n            else:\r\n                error_msg = \"No image URL generated\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating image markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def _generate_video_markdown(self, text: str) -> Message:\r\n        \"\"\"Generate HTML video player code for the generated video.\"\"\"\r\n        try:\r\n            # First, generate the video using the main method\r\n            video_result = self.generate_video()\r\n            \r\n            # Check if video generation was successful\r\n            if hasattr(video_result, 'data') and isinstance(video_result.data, dict):\r\n                if 'error' in video_result.data:\r\n                    return Message(text=f\"Error: {video_result.data['error']}\")\r\n                \r\n                # Get the primary video URL\r\n                video_url = video_result.data.get('video_url')\r\n                if not video_url:\r\n                    return Message(text=\"Error: No video URL generated\")\r\n                \r\n                # Determine aspect ratio for dimensions\r\n                aspect_ratio = getattr(self, 'aspect_ratio', '16:9') or '16:9'\r\n                if aspect_ratio == \"16:9\":\r\n                    width, height = 640, 360\r\n                elif aspect_ratio == \"9:16\":\r\n                    width, height = 360, 640\r\n                else:\r\n                    width, height = 640, 360  # Default to 16:9\r\n                \r\n                # Generate HTML video player code\r\n                html_code = f'<video width=\"{width}\" height=\"{height}\" controls>\\n  <source src=\"{video_url}\">\\n</video>'\r\n                \r\n                self.status = f\"Generated video HTML for {aspect_ratio} aspect ratio\"\r\n                return Message(text=html_code)\r\n            else:\r\n                return Message(text=\"Error: Invalid video generation result\")\r\n                \r\n        except Exception as e:\r\n            error_msg = f\"Error generating video markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def generate_image(self) -> Data:\r\n        \"\"\"Generate image from text using OpenAI DALL-E API.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Data(data={\"error\": \"No text provided for image generation\"})\r\n\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'image_model', 'dall-e-3') or 'dall-e-3'\r\n            size = getattr(self, 'image_size', '1024x1024') or '1024x1024'\r\n            n = getattr(self, 'num_images', 1) or 1\r\n\r\n            client = OpenAI(api_key=api_key)\r\n\r\n            self.status = f\"Generating image using {model} model...\"\r\n            self.log(f\"Making image generation request with model: {model}, size: {size}, count: {n}\")\r\n\r\n            response = client.images.generate(\r\n                model=model,\r\n                prompt=text,\r\n                n=n,\r\n                size=size\r\n            )\r\n\r\n            image_urls = [data.url for data in response.data]\r\n            self.status = f\"Generated {len(image_urls)} image(s) successfully!\"\r\n            self.log(f\"Generated {len(image_urls)} images\")\r\n\r\n            # Se apenas uma imagem foi gerada, retornar a URL diretamente\r\n            if n == 1 and len(image_urls) == 1:\r\n                return Data(data={\r\n                    \"image_url\": image_urls[0],\r\n                    \"model\": model,\r\n                    \"size\": size,\r\n                    \"count\": n,\r\n                    \"prompt\": text[:100] + \"...\" if len(text) > 100 else text,\r\n                })\r\n            else:\r\n                return Data(data={\r\n                    \"image_urls\": image_urls,\r\n                    \"model\": model,\r\n                    \"size\": size,\r\n                    \"count\": n,\r\n                    \"prompt\": text[:100] + \"...\" if len(text) > 100 else text,\r\n                })\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating image: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Data(data={\"error\": error_msg})\r\n\r\n    def generate_image_url(self) -> Message:\r\n        \"\"\"Generate image and return only the URL as text message.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Message(text=\"Error: No text provided for image generation\")\r\n\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'image_model', 'dall-e-3') or 'dall-e-3'\r\n            size = getattr(self, 'image_size', '1024x1024') or '1024x1024'\r\n            n = getattr(self, 'num_images', 1) or 1\r\n\r\n            client = OpenAI(api_key=api_key)\r\n\r\n            self.status = f\"Generating image using {model} model...\"\r\n            self.log(f\"Making image generation request with model: {model}, size: {size}, count: {n}\")\r\n\r\n            response = client.images.generate(\r\n                model=model,\r\n                prompt=text,\r\n                n=n,\r\n                size=size\r\n            )\r\n\r\n            # Return only the first image URL as text\r\n            if response.data and len(response.data) > 0:\r\n                image_url = response.data[0].url\r\n                self.status = f\"Generated image URL successfully!\"\r\n                self.log(f\"Generated image URL: {image_url}\")\r\n                \r\n                return Message(text=image_url)\r\n            else:\r\n                error_msg = \"No image URL generated\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating image URL: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n\r\n\r\n    def generate_video(self) -> Data:\r\n        \"\"\"Generate video from text using Google Veo.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Data(data={\"error\": \"No text provided for video generation\"})\r\n            \r\n            return self._generate_video_with_veo(text)\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating video: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Data(data={\"error\": error_msg})\r\n\r\n    def _generate_video_with_veo(self, text: str) -> Data:\r\n        \"\"\"Generate video using Google Veo.\"\"\"\r\n        try:\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'video_model', 'veo-3.0-generate-preview') or 'veo-3.0-generate-preview'\r\n            aspect_ratio = getattr(self, 'aspect_ratio', '16:9') or '16:9'\r\n\r\n            # Create client with API key\r\n            client = genai.Client(api_key=api_key)\r\n\r\n            self.status = f\"Generating video using {model} model...\"\r\n            self.log(f\"Making video generation request with model: {model}, aspect_ratio: {aspect_ratio}\")\r\n\r\n            # Generate video using the selected model\r\n            operation = client.models.generate_videos(\r\n                model=model,\r\n                prompt=text,\r\n                config=types.GenerateVideosConfig(\r\n                    aspect_ratio=aspect_ratio,\r\n                ),\r\n            )\r\n\r\n            self.status = f\"Waiting for video generation completion using {model}...\"\r\n            \r\n            # Poll for completion with proper interval (20 seconds as per documentation)\r\n            while not operation.done:\r\n                time.sleep(20)\r\n                operation = client.operations.get(operation)\r\n\r\n            # Process generated videos\r\n            video_urls = []\r\n            video_data = []\r\n            \r\n            for n, generated_video in enumerate(operation.response.generated_videos):\r\n                if hasattr(generated_video, 'video') and generated_video.video:\r\n                    video_info = {\r\n                        \"video_id\": n,\r\n                        \"video_object\": generated_video.video\r\n                    }\r\n                    \r\n                    # Add URI if available (needs API key appended for download)\r\n                    if hasattr(generated_video.video, 'uri'):\r\n                        video_url = f\"{generated_video.video.uri}&key={api_key}\"\r\n                        video_info[\"video_uri\"] = video_url\r\n                        video_urls.append(video_url)\r\n                    \r\n                    video_data.append(video_info)\r\n\r\n            if not video_data:\r\n                raise ValueError(\"No video was generated.\")\r\n\r\n            self.status = f\"Video(s) generated successfully using {model}. Total: {len(video_data)}\"\r\n            \r\n            # Return the first video URL as main output, with detailed data available\r\n            primary_video_url = video_urls[0] if video_urls else None\r\n            \r\n            return Data(data={\r\n                \"video_url\": primary_video_url,  # Direct link to first video\r\n                \"video_urls\": video_urls,        # List of all links\r\n                \"video_count\": len(video_data),\r\n                \"videos\": video_data,            # Complete data\r\n                \"model_used\": model,             # Model used\r\n                \"prompt_used\": text,\r\n                \"aspect_ratio\": aspect_ratio,\r\n                \"provider\": \"Gemini\"\r\n            })\r\n\r\n        except Exception as e:\r\n            error_message = str(e)\r\n            self.status = f\"Error with model {model}: {error_message}\"\r\n            \r\n            # Provide helpful error info\r\n            return Data(data={\r\n                \"error\": error_message,\r\n                \"model_attempted\": model,\r\n                \"video_count\": 0,\r\n                \"provider\": \"Gemini\",\r\n                \"suggestion\": \"Try using veo-3.0-generate-preview for the latest model, or check your API key and billing setup\"\r\n            })\r\n\r\n    def generate_video_urls(self) -> Message:\r\n        \"\"\"Generate video and return only the URLs as text message.\"\"\"\r\n        try:\r\n            # First, generate the video using the main method\r\n            video_result = self.generate_video()\r\n            \r\n            # Check if video generation was successful\r\n            if hasattr(video_result, 'data') and isinstance(video_result.data, dict):\r\n                if 'error' in video_result.data:\r\n                    return Message(text=f\"Error: {video_result.data['error']}\")\r\n                \r\n                # Get video URLs\r\n                video_urls = video_result.data.get('video_urls', [])\r\n                if not video_urls:\r\n                    return Message(text=\"Error: No video URLs generated\")\r\n                \r\n                # Return URLs as comma-separated text\r\n                urls_text = \", \".join(video_urls)\r\n                self.status = f\"Generated {len(video_urls)} video URL(s) successfully!\"\r\n                return Message(text=urls_text)\r\n            else:\r\n                return Message(text=\"Error: Invalid video generation result\")\r\n                \r\n        except Exception as e:\r\n            error_msg = f\"Error generating video URLs: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n\r\n"
              },
              "gemini_api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Gemini API Key",
                "dynamic": false,
                "info": "Your Google Gemini API key for generating videos with Veo",
                "input_types": [],
                "load_from_db": true,
                "name": "gemini_api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": false,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "image_model": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Image Model",
                "dynamic": false,
                "external_options": {},
                "info": "The DALL·E model version to use",
                "name": "image_model",
                "options": [
                  "dall-e-2",
                  "dall-e-3"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "dall-e-3"
              },
              "image_size": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Image Size",
                "dynamic": false,
                "external_options": {},
                "info": "Size of the generated image",
                "name": "image_size",
                "options": [
                  "256x256",
                  "512x512",
                  "1024x1024",
                  "1792x1024",
                  "1024x1792"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "1024x1024"
              },
              "input_text": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "Text input to convert to audio, image, or video",
                "input_types": [
                  "Message",
                  "Data",
                  "DataFrame"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_text",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "num_images": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Images",
                "dynamic": false,
                "info": "Number of images to generate",
                "list": false,
                "list_add_label": "Add More",
                "name": "num_images",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1
              },
              "openai_api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "Your OpenAI API key for generating audio, images, and videos",
                "input_types": [],
                "load_from_db": true,
                "name": "openai_api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": false,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "output_type": {
                "_input_type": "TabInput",
                "advanced": false,
                "display_name": "Output Type",
                "dynamic": false,
                "info": "Select the desired output media type",
                "name": "output_type",
                "options": [
                  "Audio",
                  "Image",
                  "Video"
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "tab",
                "value": "Audio"
              },
              "speed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Speed",
                "dynamic": false,
                "info": "Speed of the generated audio. Values range from 0.25 to 4.0.",
                "list": false,
                "list_add_label": "Add More",
                "name": "speed",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1
              },
              "video_model": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model",
                "dynamic": false,
                "external_options": {},
                "info": "Veo model to use for video generation",
                "name": "video_model",
                "options": [
                  "veo-3.0-generate-preview",
                  "veo-2.0-generate-001",
                  "models/veo-2.0-generate-001"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "veo-3.0-generate-preview"
              },
              "video_provider": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "video_provider",
                "options": [
                  "Google Generative AI"
                ],
                "options_metadata": [
                  {
                    "icon": "GoogleGenerativeAI"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "Google Generative AI"
              },
              "voice": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Voice",
                "dynamic": false,
                "external_options": {},
                "info": "Select the voice for audio generation",
                "name": "voice",
                "options": [
                  "alloy",
                  "echo",
                  "fable",
                  "onyx",
                  "nova",
                  "shimmer"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "alloy"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ModalConverterComponent"
        },
        "dragging": false,
        "id": "CustomComponent-vef9p",
        "measured": {
          "height": 261,
          "width": 320
        },
        "position": {
          "x": 1145.6606782762408,
          "y": 828.4494077409549
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ChatInput-q1VgY",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "https://docs.langflow.org/chat-input-and-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "context_id",
              "files"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.7.1",
            "metadata": {
              "code_hash": "7a26c54d89ed",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 1
              },
              "module": "lfx.components.input_output.chat.ChatInput"
            },
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chat Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from lfx.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom lfx.base.io.chat import ChatComponent\nfrom lfx.inputs.inputs import BoolInput\nfrom lfx.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom lfx.schema.message import Message\nfrom lfx.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    documentation: str = \"https://docs.langflow.org/chat-input-and-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        # Ensure files is a list and filter out empty/None values\n        files = self.files if self.files else []\n        if files and not isinstance(files, list):\n            files = [files]\n        # Filter out None/empty values\n        files = [f for f in files if f is not None and f != \"\"]\n\n        session_id = self.session_id or self.graph.session_id or \"\"\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=session_id,\n            context_id=self.context_id,\n            files=files,\n        )\n        if session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "files": {
                "_input_type": "FileInput",
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "csv",
                  "json",
                  "pdf",
                  "txt",
                  "md",
                  "mdx",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "list_add_label": "Add More",
                "name": "files",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "temp_file": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "Create materials for a toothpaste advertising campaign."
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ChatInput"
        },
        "dragging": false,
        "id": "ChatInput-q1VgY",
        "measured": {
          "height": 203,
          "width": 320
        },
        "position": {
          "x": -772.6414439738858,
          "y": 828.4494077409549
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Agent-yHysx",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Define the agent's instructions, then enter a task to complete using tools.",
            "display_name": "Agent",
            "documentation": "https://docs.langflow.org/agents",
            "edited": false,
            "field_order": [
              "agent_llm",
              "api_key",
              "base_url",
              "project_id",
              "max_output_tokens",
              "max_tokens",
              "model_kwargs",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "max_retries",
              "timeout",
              "system_prompt",
              "context_id",
              "n_messages",
              "format_instructions",
              "output_schema",
              "tools",
              "input_value",
              "handle_parsing_errors",
              "verbose",
              "max_iterations",
              "agent_description",
              "add_current_date_tool"
            ],
            "frozen": false,
            "icon": "bot",
            "last_updated": "2026-01-27T13:37:24.018Z",
            "legacy": false,
            "lf_version": "1.7.1",
            "metadata": {
              "code_hash": "fba2d73636e5",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "langchain_core",
                    "version": "0.3.81"
                  },
                  {
                    "name": "pydantic",
                    "version": "2.11.10"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 3
              },
              "module": "lfx.components.models_and_agents.agent.AgentComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Response",
                "group_outputs": false,
                "loop_types": null,
                "method": "message_response",
                "name": "response",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_frontend_node_flow_id": {
                "value": "48ce6b32-e37c-4ff6-9b83-95c1569bba78"
              },
              "_frontend_node_folder_id": {
                "value": "3f1d0bf7-bd50-4fa9-a5d3-0974cfa99642"
              },
              "_type": "Component",
              "add_current_date_tool": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Current Date",
                "dynamic": false,
                "info": "If true, will add a tool to the agent that returns the current date.",
                "list": false,
                "list_add_label": "Add More",
                "name": "add_current_date_tool",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              },
              "agent_description": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Agent Description [Deprecated]",
                "dynamic": false,
                "info": "The description of the agent. This is only used when in Tool Mode. Defaults to 'A helpful assistant with access to the following tools:' and tools are added dynamically. This feature is deprecated and will be removed in future versions.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "agent_description",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "A helpful assistant with access to the following tools:"
              },
              "agent_llm": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {
                  "fields": {
                    "data": {
                      "node": {
                        "display_name": "Connect other models",
                        "icon": "CornerDownLeft",
                        "name": "connect_other_models"
                      }
                    }
                  }
                },
                "info": "The provider of the language model that the agent will use to generate responses.",
                "input_types": [],
                "name": "agent_llm",
                "options": [
                  "Anthropic",
                  "Google Generative AI",
                  "OpenAI",
                  "IBM watsonx.ai",
                  "Ollama"
                ],
                "options_metadata": [
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "OpenAI"
                  },
                  {
                    "icon": "WatsonxAI"
                  },
                  {
                    "icon": "Ollama"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "refresh_button": false,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "OpenAI"
              },
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "base_url": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Base URL",
                "dynamic": false,
                "info": "The base URL of the API.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "base_url",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import json\nimport re\n\nfrom langchain_core.tools import StructuredTool, Tool\nfrom pydantic import ValidationError\n\nfrom lfx.base.agents.agent import LCToolsAgentComponent\nfrom lfx.base.agents.events import ExceptionWithMessageError\nfrom lfx.base.models.model_input_constants import (\n    ALL_PROVIDER_FIELDS,\n    MODEL_DYNAMIC_UPDATE_FIELDS,\n    MODEL_PROVIDERS_DICT,\n    MODEL_PROVIDERS_LIST,\n    MODELS_METADATA,\n)\nfrom lfx.base.models.model_utils import get_model_name\nfrom lfx.components.helpers import CurrentDateComponent\nfrom lfx.components.langchain_utilities.tool_calling import ToolCallingAgentComponent\nfrom lfx.components.models_and_agents.memory import MemoryComponent\nfrom lfx.custom.custom_component.component import get_component_toolkit\nfrom lfx.custom.utils import update_component_build_config\nfrom lfx.helpers.base_model import build_model_from_schema\nfrom lfx.inputs.inputs import BoolInput, SecretStrInput, StrInput\nfrom lfx.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output, TableInput\nfrom lfx.log.logger import logger\nfrom lfx.schema.data import Data\nfrom lfx.schema.dotdict import dotdict\nfrom lfx.schema.message import Message\nfrom lfx.schema.table import EditMode\n\n\ndef set_advanced_true(component_input):\n    component_input.advanced = True\n    return component_input\n\n\nclass AgentComponent(ToolCallingAgentComponent):\n    display_name: str = \"Agent\"\n    description: str = \"Define the agent's instructions, then enter a task to complete using tools.\"\n    documentation: str = \"https://docs.langflow.org/agents\"\n    icon = \"bot\"\n    beta = False\n    name = \"Agent\"\n\n    memory_inputs = [set_advanced_true(component_input) for component_input in MemoryComponent().inputs]\n\n    # Filter out json_mode from OpenAI inputs since we handle structured output differently\n    if \"OpenAI\" in MODEL_PROVIDERS_DICT:\n        openai_inputs_filtered = [\n            input_field\n            for input_field in MODEL_PROVIDERS_DICT[\"OpenAI\"][\"inputs\"]\n            if not (hasattr(input_field, \"name\") and input_field.name == \"json_mode\")\n        ]\n    else:\n        openai_inputs_filtered = []\n\n    inputs = [\n        DropdownInput(\n            name=\"agent_llm\",\n            display_name=\"Model Provider\",\n            info=\"The provider of the language model that the agent will use to generate responses.\",\n            options=[*MODEL_PROVIDERS_LIST],\n            value=\"OpenAI\",\n            real_time_refresh=True,\n            refresh_button=False,\n            input_types=[],\n            options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST if key in MODELS_METADATA],\n            external_options={\n                \"fields\": {\n                    \"data\": {\n                        \"node\": {\n                            \"name\": \"connect_other_models\",\n                            \"display_name\": \"Connect other models\",\n                            \"icon\": \"CornerDownLeft\",\n                        }\n                    }\n                },\n            },\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"API Key\",\n            info=\"The API key to use for the model.\",\n            required=True,\n        ),\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"The base URL of the API.\",\n            required=True,\n            show=False,\n        ),\n        StrInput(\n            name=\"project_id\",\n            display_name=\"Project ID\",\n            info=\"The project ID of the model.\",\n            required=True,\n            show=False,\n        ),\n        IntInput(\n            name=\"max_output_tokens\",\n            display_name=\"Max Output Tokens\",\n            info=\"The maximum number of tokens to generate.\",\n            show=False,\n        ),\n        *openai_inputs_filtered,\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Agent Instructions\",\n            info=\"System Prompt: Initial instructions and context provided to guide the agent's behavior.\",\n            value=\"You are a helpful assistant that can use tools to answer questions and perform tasks.\",\n            advanced=False,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Chat History Messages\",\n            value=100,\n            info=\"Number of chat history messages to retrieve.\",\n            advanced=True,\n            show=True,\n        ),\n        MultilineInput(\n            name=\"format_instructions\",\n            display_name=\"Output Format Instructions\",\n            info=\"Generic Template for structured output formatting. Valid only with Structured response.\",\n            value=(\n                \"You are an AI that extracts structured JSON objects from unstructured text. \"\n                \"Use a predefined schema with expected types (str, int, float, bool, dict). \"\n                \"Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. \"\n                \"Fill missing or ambiguous values with defaults: null for missing values. \"\n                \"Remove exact duplicates but keep variations that have different field values. \"\n                \"Always return valid JSON in the expected format, never throw errors. \"\n                \"If multiple objects can be extracted, return them all in the structured format.\"\n            ),\n            advanced=True,\n        ),\n        TableInput(\n            name=\"output_schema\",\n            display_name=\"Output Schema\",\n            info=(\n                \"Schema Validation: Define the structure and data types for structured output. \"\n                \"No validation if no output schema.\"\n            ),\n            advanced=True,\n            required=False,\n            value=[],\n            table_schema=[\n                {\n                    \"name\": \"name\",\n                    \"display_name\": \"Name\",\n                    \"type\": \"str\",\n                    \"description\": \"Specify the name of the output field.\",\n                    \"default\": \"field\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n                {\n                    \"name\": \"description\",\n                    \"display_name\": \"Description\",\n                    \"type\": \"str\",\n                    \"description\": \"Describe the purpose of the output field.\",\n                    \"default\": \"description of field\",\n                    \"edit_mode\": EditMode.POPOVER,\n                },\n                {\n                    \"name\": \"type\",\n                    \"display_name\": \"Type\",\n                    \"type\": \"str\",\n                    \"edit_mode\": EditMode.INLINE,\n                    \"description\": (\"Indicate the data type of the output field (e.g., str, int, float, bool, dict).\"),\n                    \"options\": [\"str\", \"int\", \"float\", \"bool\", \"dict\"],\n                    \"default\": \"str\",\n                },\n                {\n                    \"name\": \"multiple\",\n                    \"display_name\": \"As List\",\n                    \"type\": \"boolean\",\n                    \"description\": \"Set to True if this output field should be a list of the specified type.\",\n                    \"default\": \"False\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n            ],\n        ),\n        *LCToolsAgentComponent.get_base_inputs(),\n        # removed memory inputs from agent component\n        # *memory_inputs,\n        BoolInput(\n            name=\"add_current_date_tool\",\n            display_name=\"Current Date\",\n            advanced=True,\n            info=\"If true, will add a tool to the agent that returns the current date.\",\n            value=True,\n        ),\n    ]\n    outputs = [\n        Output(name=\"response\", display_name=\"Response\", method=\"message_response\"),\n    ]\n\n    async def get_agent_requirements(self):\n        \"\"\"Get the agent requirements for the agent.\"\"\"\n        llm_model, display_name = await self.get_llm()\n        if llm_model is None:\n            msg = \"No language model selected. Please choose a model to proceed.\"\n            raise ValueError(msg)\n        self.model_name = get_model_name(llm_model, display_name=display_name)\n\n        # Get memory data\n        self.chat_history = await self.get_memory_data()\n        await logger.adebug(f\"Retrieved {len(self.chat_history)} chat history messages\")\n        if isinstance(self.chat_history, Message):\n            self.chat_history = [self.chat_history]\n\n        # Add current date tool if enabled\n        if self.add_current_date_tool:\n            if not isinstance(self.tools, list):  # type: ignore[has-type]\n                self.tools = []\n            current_date_tool = (await CurrentDateComponent(**self.get_base_args()).to_toolkit()).pop(0)\n\n            if not isinstance(current_date_tool, StructuredTool):\n                msg = \"CurrentDateComponent must be converted to a StructuredTool\"\n                raise TypeError(msg)\n            self.tools.append(current_date_tool)\n\n        # Set shared callbacks for tracing the tools used by the agent\n        self.set_tools_callbacks(self.tools, self._get_shared_callbacks())\n\n        return llm_model, self.chat_history, self.tools\n\n    async def message_response(self) -> Message:\n        try:\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            # Set up and run agent\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=self.system_prompt,\n            )\n            agent = self.create_agent_runnable()\n            result = await self.run_agent(agent)\n\n            # Store result for potential JSON output\n            self._agent_result = result\n\n        except (ValueError, TypeError, KeyError) as e:\n            await logger.aerror(f\"{type(e).__name__}: {e!s}\")\n            raise\n        except ExceptionWithMessageError as e:\n            await logger.aerror(f\"ExceptionWithMessageError occurred: {e}\")\n            raise\n        # Avoid catching blind Exception; let truly unexpected exceptions propagate\n        except Exception as e:\n            await logger.aerror(f\"Unexpected error: {e!s}\")\n            raise\n        else:\n            return result\n\n    def _preprocess_schema(self, schema):\n        \"\"\"Preprocess schema to ensure correct data types for build_model_from_schema.\"\"\"\n        processed_schema = []\n        for field in schema:\n            processed_field = {\n                \"name\": str(field.get(\"name\", \"field\")),\n                \"type\": str(field.get(\"type\", \"str\")),\n                \"description\": str(field.get(\"description\", \"\")),\n                \"multiple\": field.get(\"multiple\", False),\n            }\n            # Ensure multiple is handled correctly\n            if isinstance(processed_field[\"multiple\"], str):\n                processed_field[\"multiple\"] = processed_field[\"multiple\"].lower() in [\n                    \"true\",\n                    \"1\",\n                    \"t\",\n                    \"y\",\n                    \"yes\",\n                ]\n            processed_schema.append(processed_field)\n        return processed_schema\n\n    async def build_structured_output_base(self, content: str):\n        \"\"\"Build structured output with optional BaseModel validation.\"\"\"\n        json_pattern = r\"\\{.*\\}\"\n        schema_error_msg = \"Try setting an output schema\"\n\n        # Try to parse content as JSON first\n        json_data = None\n        try:\n            json_data = json.loads(content)\n        except json.JSONDecodeError:\n            json_match = re.search(json_pattern, content, re.DOTALL)\n            if json_match:\n                try:\n                    json_data = json.loads(json_match.group())\n                except json.JSONDecodeError:\n                    return {\"content\": content, \"error\": schema_error_msg}\n            else:\n                return {\"content\": content, \"error\": schema_error_msg}\n\n        # If no output schema provided, return parsed JSON without validation\n        if not hasattr(self, \"output_schema\") or not self.output_schema or len(self.output_schema) == 0:\n            return json_data\n\n        # Use BaseModel validation with schema\n        try:\n            processed_schema = self._preprocess_schema(self.output_schema)\n            output_model = build_model_from_schema(processed_schema)\n\n            # Validate against the schema\n            if isinstance(json_data, list):\n                # Multiple objects\n                validated_objects = []\n                for item in json_data:\n                    try:\n                        validated_obj = output_model.model_validate(item)\n                        validated_objects.append(validated_obj.model_dump())\n                    except ValidationError as e:\n                        await logger.aerror(f\"Validation error for item: {e}\")\n                        # Include invalid items with error info\n                        validated_objects.append({\"data\": item, \"validation_error\": str(e)})\n                return validated_objects\n\n            # Single object\n            try:\n                validated_obj = output_model.model_validate(json_data)\n                return [validated_obj.model_dump()]  # Return as list for consistency\n            except ValidationError as e:\n                await logger.aerror(f\"Validation error: {e}\")\n                return [{\"data\": json_data, \"validation_error\": str(e)}]\n\n        except (TypeError, ValueError) as e:\n            await logger.aerror(f\"Error building structured output: {e}\")\n            # Fallback to parsed JSON without validation\n            return json_data\n\n    async def json_response(self) -> Data:\n        \"\"\"Convert agent response to structured JSON Data output with schema validation.\"\"\"\n        # Always use structured chat agent for JSON response mode for better JSON formatting\n        try:\n            system_components = []\n\n            # 1. Agent Instructions (system_prompt)\n            agent_instructions = getattr(self, \"system_prompt\", \"\") or \"\"\n            if agent_instructions:\n                system_components.append(f\"{agent_instructions}\")\n\n            # 2. Format Instructions\n            format_instructions = getattr(self, \"format_instructions\", \"\") or \"\"\n            if format_instructions:\n                system_components.append(f\"Format instructions: {format_instructions}\")\n\n            # 3. Schema Information from BaseModel\n            if hasattr(self, \"output_schema\") and self.output_schema and len(self.output_schema) > 0:\n                try:\n                    processed_schema = self._preprocess_schema(self.output_schema)\n                    output_model = build_model_from_schema(processed_schema)\n                    schema_dict = output_model.model_json_schema()\n                    schema_info = (\n                        \"You are given some text that may include format instructions, \"\n                        \"explanations, or other content alongside a JSON schema.\\n\\n\"\n                        \"Your task:\\n\"\n                        \"- Extract only the JSON schema.\\n\"\n                        \"- Return it as valid JSON.\\n\"\n                        \"- Do not include format instructions, explanations, or extra text.\\n\\n\"\n                        \"Input:\\n\"\n                        f\"{json.dumps(schema_dict, indent=2)}\\n\\n\"\n                        \"Output (only JSON schema):\"\n                    )\n                    system_components.append(schema_info)\n                except (ValidationError, ValueError, TypeError, KeyError) as e:\n                    await logger.aerror(f\"Could not build schema for prompt: {e}\", exc_info=True)\n\n            # Combine all components\n            combined_instructions = \"\\n\\n\".join(system_components) if system_components else \"\"\n            llm_model, self.chat_history, self.tools = await self.get_agent_requirements()\n            self.set(\n                llm=llm_model,\n                tools=self.tools or [],\n                chat_history=self.chat_history,\n                input_value=self.input_value,\n                system_prompt=combined_instructions,\n            )\n\n            # Create and run structured chat agent\n            try:\n                structured_agent = self.create_agent_runnable()\n            except (NotImplementedError, ValueError, TypeError) as e:\n                await logger.aerror(f\"Error with structured chat agent: {e}\")\n                raise\n            try:\n                result = await self.run_agent(structured_agent)\n            except (\n                ExceptionWithMessageError,\n                ValueError,\n                TypeError,\n                RuntimeError,\n            ) as e:\n                await logger.aerror(f\"Error with structured agent result: {e}\")\n                raise\n            # Extract content from structured agent result\n            if hasattr(result, \"content\"):\n                content = result.content\n            elif hasattr(result, \"text\"):\n                content = result.text\n            else:\n                content = str(result)\n\n        except (\n            ExceptionWithMessageError,\n            ValueError,\n            TypeError,\n            NotImplementedError,\n            AttributeError,\n        ) as e:\n            await logger.aerror(f\"Error with structured chat agent: {e}\")\n            # Fallback to regular agent\n            content_str = \"No content returned from agent\"\n            return Data(data={\"content\": content_str, \"error\": str(e)})\n\n        # Process with structured output validation\n        try:\n            structured_output = await self.build_structured_output_base(content)\n\n            # Handle different output formats\n            if isinstance(structured_output, list) and structured_output:\n                if len(structured_output) == 1:\n                    return Data(data=structured_output[0])\n                return Data(data={\"results\": structured_output})\n            if isinstance(structured_output, dict):\n                return Data(data=structured_output)\n            return Data(data={\"content\": content})\n\n        except (ValueError, TypeError) as e:\n            await logger.aerror(f\"Error in structured output processing: {e}\")\n            return Data(data={\"content\": content, \"error\": str(e)})\n\n    async def get_memory_data(self):\n        # TODO: This is a temporary fix to avoid message duplication. We should develop a function for this.\n        messages = (\n            await MemoryComponent(**self.get_base_args())\n            .set(\n                session_id=self.graph.session_id,\n                context_id=self.context_id,\n                order=\"Ascending\",\n                n_messages=self.n_messages,\n            )\n            .retrieve_messages()\n        )\n        return [\n            message for message in messages if getattr(message, \"id\", None) != getattr(self.input_value, \"id\", None)\n        ]\n\n    async def get_llm(self):\n        if not isinstance(self.agent_llm, str):\n            return self.agent_llm, None\n\n        try:\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if not provider_info:\n                msg = f\"Invalid model provider: {self.agent_llm}\"\n                raise ValueError(msg)\n\n            component_class = provider_info.get(\"component_class\")\n            display_name = component_class.display_name\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\", \"\")\n\n            return self._build_llm_model(component_class, inputs, prefix), display_name\n\n        except (AttributeError, ValueError, TypeError, RuntimeError) as e:\n            await logger.aerror(f\"Error building {self.agent_llm} language model: {e!s}\")\n            msg = f\"Failed to initialize language model: {e!s}\"\n            raise ValueError(msg) from e\n\n    def _build_llm_model(self, component, inputs, prefix=\"\"):\n        model_kwargs = {}\n        for input_ in inputs:\n            if hasattr(self, f\"{prefix}{input_.name}\"):\n                model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n        return component.set(**model_kwargs).build_model()\n\n    def set_component_params(self, component):\n        provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n        if provider_info:\n            inputs = provider_info.get(\"inputs\")\n            prefix = provider_info.get(\"prefix\")\n            # Filter out json_mode and only use attributes that exist on this component\n            model_kwargs = {}\n            for input_ in inputs:\n                if hasattr(self, f\"{prefix}{input_.name}\"):\n                    model_kwargs[input_.name] = getattr(self, f\"{prefix}{input_.name}\")\n\n            return component.set(**model_kwargs)\n        return component\n\n    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:\n        \"\"\"Delete specified fields from build_config.\"\"\"\n        for field in fields:\n            if build_config is not None and field in build_config:\n                build_config.pop(field, None)\n\n    def update_input_types(self, build_config: dotdict) -> dotdict:\n        \"\"\"Update input types for all fields in build_config.\"\"\"\n        for key, value in build_config.items():\n            if isinstance(value, dict):\n                if value.get(\"input_types\") is None:\n                    build_config[key][\"input_types\"] = []\n            elif hasattr(value, \"input_types\") and value.input_types is None:\n                value.input_types = []\n        return build_config\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: str, field_name: str | None = None\n    ) -> dotdict:\n        # Iterate over all providers in the MODEL_PROVIDERS_DICT\n        # Existing logic for updating build_config\n        if field_name in (\"agent_llm\",):\n            build_config[\"agent_llm\"][\"value\"] = field_value\n            provider_info = MODEL_PROVIDERS_DICT.get(field_value)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call the component class's update_build_config method\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, \"model_name\"\n                    )\n\n            provider_configs: dict[str, tuple[dict, list[dict]]] = {\n                provider: (\n                    MODEL_PROVIDERS_DICT[provider][\"fields\"],\n                    [\n                        MODEL_PROVIDERS_DICT[other_provider][\"fields\"]\n                        for other_provider in MODEL_PROVIDERS_DICT\n                        if other_provider != provider\n                    ],\n                )\n                for provider in MODEL_PROVIDERS_DICT\n            }\n            if field_value in provider_configs:\n                fields_to_add, fields_to_delete = provider_configs[field_value]\n\n                # Delete fields from other providers\n                for fields in fields_to_delete:\n                    self.delete_fields(build_config, fields)\n\n                # Add provider-specific fields\n                if field_value == \"OpenAI\" and not any(field in build_config for field in fields_to_add):\n                    build_config.update(fields_to_add)\n                else:\n                    build_config.update(fields_to_add)\n                # Reset input types for agent_llm\n                build_config[\"agent_llm\"][\"input_types\"] = []\n                build_config[\"agent_llm\"][\"display_name\"] = \"Model Provider\"\n            elif field_value == \"connect_other_models\":\n                # Delete all provider fields\n                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)\n                # # Update with custom component\n                custom_component = DropdownInput(\n                    name=\"agent_llm\",\n                    display_name=\"Language Model\",\n                    info=\"The provider of the language model that the agent will use to generate responses.\",\n                    options=[*MODEL_PROVIDERS_LIST],\n                    real_time_refresh=True,\n                    refresh_button=False,\n                    input_types=[\"LanguageModel\"],\n                    placeholder=\"Awaiting model input.\",\n                    options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST if key in MODELS_METADATA],\n                    external_options={\n                        \"fields\": {\n                            \"data\": {\n                                \"node\": {\n                                    \"name\": \"connect_other_models\",\n                                    \"display_name\": \"Connect other models\",\n                                    \"icon\": \"CornerDownLeft\",\n                                },\n                            }\n                        },\n                    },\n                )\n                build_config.update({\"agent_llm\": custom_component.to_dict()})\n            # Update input types for all fields\n            build_config = self.update_input_types(build_config)\n\n            # Validate required keys\n            default_keys = [\n                \"code\",\n                \"_type\",\n                \"agent_llm\",\n                \"tools\",\n                \"input_value\",\n                \"add_current_date_tool\",\n                \"system_prompt\",\n                \"agent_description\",\n                \"max_iterations\",\n                \"handle_parsing_errors\",\n                \"verbose\",\n            ]\n            missing_keys = [key for key in default_keys if key not in build_config]\n            if missing_keys:\n                msg = f\"Missing required keys in build_config: {missing_keys}\"\n                raise ValueError(msg)\n        if (\n            isinstance(self.agent_llm, str)\n            and self.agent_llm in MODEL_PROVIDERS_DICT\n            and field_name in MODEL_DYNAMIC_UPDATE_FIELDS\n        ):\n            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)\n            if provider_info:\n                component_class = provider_info.get(\"component_class\")\n                component_class = self.set_component_params(component_class)\n                prefix = provider_info.get(\"prefix\")\n                if component_class and hasattr(component_class, \"update_build_config\"):\n                    # Call each component class's update_build_config method\n                    # remove the prefix from the field_name\n                    if isinstance(field_name, str) and isinstance(prefix, str):\n                        field_name_without_prefix = field_name.replace(prefix, \"\")\n                    else:\n                        field_name_without_prefix = field_name\n                    build_config = await update_component_build_config(\n                        component_class, build_config, field_value, field_name_without_prefix\n                    )\n        return dotdict({k: v.to_dict() if hasattr(v, \"to_dict\") else v for k, v in build_config.items()})\n\n    async def _get_tools(self) -> list[Tool]:\n        component_toolkit = get_component_toolkit()\n        tools_names = self._build_tools_names()\n        agent_description = self.get_tool_description()\n        # TODO: Agent Description Depreciated Feature to be removed\n        description = f\"{agent_description}{tools_names}\"\n\n        tools = component_toolkit(component=self).get_tools(\n            tool_name=\"Call_Agent\",\n            tool_description=description,\n            # here we do not use the shared callbacks as we are exposing the agent as a tool\n            callbacks=self.get_langchain_callbacks(),\n        )\n        if hasattr(self, \"tools_metadata\"):\n            tools = component_toolkit(component=self, metadata=self.tools_metadata).update_tools_metadata(tools=tools)\n\n        return tools\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "format_instructions": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Output Format Instructions",
                "dynamic": false,
                "info": "Generic Template for structured output formatting. Valid only with Structured response.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "format_instructions",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "You are an AI that extracts structured JSON objects from unstructured text. Use a predefined schema with expected types (str, int, float, bool, dict). Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. Fill missing or ambiguous values with defaults: null for missing values. Remove exact duplicates but keep variations that have different field values. Always return valid JSON in the expected format, never throw errors. If multiple objects can be extracted, return them all in the structured format."
              },
              "handle_parsing_errors": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Handle Parse Errors",
                "dynamic": false,
                "info": "Should the Agent fix errors when reading user input for better processing?",
                "list": false,
                "list_add_label": "Add More",
                "name": "handle_parsing_errors",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input provided by the user for the agent to process.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "is_refresh": false,
              "max_iterations": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Iterations",
                "dynamic": false,
                "info": "The maximum number of attempts the agent can make to complete its task before it stops.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_iterations",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 15
              },
              "max_output_tokens": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Max Output Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_output_tokens",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": ""
              },
              "max_retries": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Retries",
                "dynamic": false,
                "info": "The maximum number of retries to make when generating.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_retries",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 5
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "list_add_label": "Add More",
                "name": "max_tokens",
                "override_skip": false,
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "Additional keyword arguments to pass to the model.",
                "list": false,
                "list_add_label": "Add More",
                "name": "model_kwargs",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "track_in_telemetry": false,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": true,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "external_options": {},
                "info": "To see the model names, first choose a provider. Then, enter your API key and click the refresh button next to the model name.",
                "name": "model_name",
                "options": [
                  "gpt-4o-mini",
                  "gpt-4o",
                  "gpt-4.1",
                  "gpt-4.1-mini",
                  "gpt-4.1-nano",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "gpt-5.1",
                  "gpt-5",
                  "gpt-5-mini",
                  "gpt-5-nano",
                  "gpt-5-chat-latest",
                  "o1",
                  "o3-mini",
                  "o3",
                  "o3-pro",
                  "o4-mini",
                  "o4-mini-high"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": false,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "gpt-4o-mini"
              },
              "n_messages": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Chat History Messages",
                "dynamic": false,
                "info": "Number of chat history messages to retrieve.",
                "list": false,
                "list_add_label": "Add More",
                "name": "n_messages",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 100
              },
              "openai_api_base": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "OpenAI API Base",
                "dynamic": false,
                "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "openai_api_base",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "output_schema": {
                "_input_type": "TableInput",
                "advanced": true,
                "display_name": "Output Schema",
                "dynamic": false,
                "info": "Schema Validation: Define the structure and data types for structured output. No validation if no output schema.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "output_schema",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "table_icon": "Table",
                "table_schema": [
                  {
                    "default": "field",
                    "description": "Specify the name of the output field.",
                    "display_name": "Name",
                    "edit_mode": "inline",
                    "name": "name",
                    "type": "str"
                  },
                  {
                    "default": "description of field",
                    "description": "Describe the purpose of the output field.",
                    "display_name": "Description",
                    "edit_mode": "popover",
                    "name": "description",
                    "type": "str"
                  },
                  {
                    "default": "str",
                    "description": "Indicate the data type of the output field (e.g., str, int, float, bool, dict).",
                    "display_name": "Type",
                    "edit_mode": "inline",
                    "name": "type",
                    "options": [
                      "str",
                      "int",
                      "float",
                      "bool",
                      "dict"
                    ],
                    "type": "str"
                  },
                  {
                    "default": "False",
                    "description": "Set to True if this output field should be a list of the specified type.",
                    "display_name": "As List",
                    "edit_mode": "inline",
                    "name": "multiple",
                    "type": "boolean"
                  }
                ],
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "trigger_icon": "Table",
                "trigger_text": "Open table",
                "type": "table",
                "value": []
              },
              "project_id": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Project ID",
                "dynamic": false,
                "info": "The project ID of the model.",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "project_id",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "list_add_label": "Add More",
                "name": "seed",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Agent Instructions",
                "dynamic": false,
                "info": "System Prompt: Initial instructions and context provided to guide the agent's behavior.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "You are a helpful assistant that can use tools to answer questions and perform tasks."
              },
              "temperature": {
                "_input_type": "SliderInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "max_label": "",
                "max_label_icon": "",
                "min_label": "",
                "min_label_icon": "",
                "name": "temperature",
                "override_skip": false,
                "placeholder": "",
                "range_spec": {
                  "max": 1,
                  "min": 0,
                  "step": 0.01,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "title_case": false,
                "tool_mode": false,
                "track_in_telemetry": false,
                "type": "slider",
                "value": 0.1
              },
              "timeout": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Timeout",
                "dynamic": false,
                "info": "The timeout for requests to OpenAI completion API.",
                "list": false,
                "list_add_label": "Add More",
                "name": "timeout",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 700
              },
              "tools": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Tools",
                "dynamic": false,
                "info": "These are the tools that the agent can use to help with tasks.",
                "input_types": [
                  "Tool"
                ],
                "list": true,
                "list_add_label": "Add More",
                "name": "tools",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "verbose": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Verbose",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "verbose",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Agent"
        },
        "dragging": false,
        "id": "Agent-yHysx",
        "measured": {
          "height": 591,
          "width": 320
        },
        "position": {
          "x": -7.035265788730783,
          "y": 828.4494077409549
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "StructuredOutput-FZySV",
          "node": {
            "base_classes": [
              "Data",
              "DataFrame"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Uses an LLM to generate structured data. Ideal for extraction and consistency.",
            "display_name": "Structured Output",
            "documentation": "https://docs.langflow.org/structured-output",
            "edited": false,
            "field_order": [
              "llm",
              "input_value",
              "system_prompt",
              "schema_name",
              "output_schema"
            ],
            "frozen": false,
            "icon": "braces",
            "legacy": false,
            "lf_version": "1.7.1",
            "metadata": {
              "code_hash": "ce82f9d7948c",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "pydantic",
                    "version": "2.11.10"
                  },
                  {
                    "name": "trustcall",
                    "version": "0.0.39"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 3
              },
              "module": "lfx.components.llm_operations.structured_output.StructuredOutputComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Structured Output",
                "group_outputs": false,
                "method": "build_structured_output",
                "name": "structured_output",
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Structured Output",
                "group_outputs": false,
                "method": "build_structured_dataframe",
                "name": "dataframe_output",
                "tool_mode": true,
                "types": [
                  "DataFrame"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from pydantic import BaseModel, Field, create_model\nfrom trustcall import create_extractor\n\nfrom lfx.base.models.chat_result import get_chat_result\nfrom lfx.custom.custom_component.component import Component\nfrom lfx.helpers.base_model import build_model_from_schema\nfrom lfx.io import (\n    HandleInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n    TableInput,\n)\nfrom lfx.log.logger import logger\nfrom lfx.schema.data import Data\nfrom lfx.schema.dataframe import DataFrame\nfrom lfx.schema.table import EditMode\n\n\nclass StructuredOutputComponent(Component):\n    display_name = \"Structured Output\"\n    description = \"Uses an LLM to generate structured data. Ideal for extraction and consistency.\"\n    documentation: str = \"https://docs.langflow.org/structured-output\"\n    name = \"StructuredOutput\"\n    icon = \"braces\"\n\n    inputs = [\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"The language model to use to generate the structured output.\",\n            input_types=[\"LanguageModel\"],\n            required=True,\n        ),\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Message\",\n            info=\"The input message to the language model.\",\n            tool_mode=True,\n            required=True,\n        ),\n        MultilineInput(\n            name=\"system_prompt\",\n            display_name=\"Format Instructions\",\n            info=\"The instructions to the language model for formatting the output.\",\n            value=(\n                \"You are an AI that extracts structured JSON objects from unstructured text. \"\n                \"Use a predefined schema with expected types (str, int, float, bool, dict). \"\n                \"Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. \"\n                \"Fill missing or ambiguous values with defaults: null for missing values. \"\n                \"Remove exact duplicates but keep variations that have different field values. \"\n                \"Always return valid JSON in the expected format, never throw errors. \"\n                \"If multiple objects can be extracted, return them all in the structured format.\"\n            ),\n            required=True,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"schema_name\",\n            display_name=\"Schema Name\",\n            info=\"Provide a name for the output data schema.\",\n            advanced=True,\n        ),\n        TableInput(\n            name=\"output_schema\",\n            display_name=\"Output Schema\",\n            info=\"Define the structure and data types for the model's output.\",\n            required=True,\n            # TODO: remove deault value\n            table_schema=[\n                {\n                    \"name\": \"name\",\n                    \"display_name\": \"Name\",\n                    \"type\": \"str\",\n                    \"description\": \"Specify the name of the output field.\",\n                    \"default\": \"field\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n                {\n                    \"name\": \"description\",\n                    \"display_name\": \"Description\",\n                    \"type\": \"str\",\n                    \"description\": \"Describe the purpose of the output field.\",\n                    \"default\": \"description of field\",\n                    \"edit_mode\": EditMode.POPOVER,\n                },\n                {\n                    \"name\": \"type\",\n                    \"display_name\": \"Type\",\n                    \"type\": \"str\",\n                    \"edit_mode\": EditMode.INLINE,\n                    \"description\": (\"Indicate the data type of the output field (e.g., str, int, float, bool, dict).\"),\n                    \"options\": [\"str\", \"int\", \"float\", \"bool\", \"dict\"],\n                    \"default\": \"str\",\n                },\n                {\n                    \"name\": \"multiple\",\n                    \"display_name\": \"As List\",\n                    \"type\": \"boolean\",\n                    \"description\": \"Set to True if this output field should be a list of the specified type.\",\n                    \"default\": \"False\",\n                    \"edit_mode\": EditMode.INLINE,\n                },\n            ],\n            value=[\n                {\n                    \"name\": \"field\",\n                    \"description\": \"description of field\",\n                    \"type\": \"str\",\n                    \"multiple\": \"False\",\n                }\n            ],\n        ),\n    ]\n\n    outputs = [\n        Output(\n            name=\"structured_output\",\n            display_name=\"Structured Output\",\n            method=\"build_structured_output\",\n        ),\n        Output(\n            name=\"dataframe_output\",\n            display_name=\"Structured Output\",\n            method=\"build_structured_dataframe\",\n        ),\n    ]\n\n    def build_structured_output_base(self):\n        schema_name = self.schema_name or \"OutputModel\"\n\n        if not hasattr(self.llm, \"with_structured_output\"):\n            msg = \"Language model does not support structured output.\"\n            raise TypeError(msg)\n        if not self.output_schema:\n            msg = \"Output schema cannot be empty\"\n            raise ValueError(msg)\n\n        output_model_ = build_model_from_schema(self.output_schema)\n        output_model = create_model(\n            schema_name,\n            __doc__=f\"A list of {schema_name}.\",\n            objects=(\n                list[output_model_],\n                Field(\n                    description=f\"A list of {schema_name}.\",  # type: ignore[valid-type]\n                    min_length=1,  # help ensure non-empty output\n                ),\n            ),\n        )\n        # Tracing config\n        config_dict = {\n            \"run_name\": self.display_name,\n            \"project_name\": self.get_project_name(),\n            \"callbacks\": self.get_langchain_callbacks(),\n        }\n        # Generate structured output using Trustcall first, then fallback to Langchain if it fails\n        result = self._extract_output_with_trustcall(output_model, config_dict)\n        if result is None:\n            result = self._extract_output_with_langchain(output_model, config_dict)\n\n        # OPTIMIZATION NOTE: Simplified processing based on trustcall response structure\n        # Handle non-dict responses (shouldn't happen with trustcall, but defensive)\n        if not isinstance(result, dict):\n            return result\n\n        # Extract first response and convert BaseModel to dict\n        responses = result.get(\"responses\", [])\n        if not responses:\n            return result\n\n        # Convert BaseModel to dict (creates the \"objects\" key)\n        first_response = responses[0]\n        structured_data = first_response\n        if isinstance(first_response, BaseModel):\n            structured_data = first_response.model_dump()\n        # Extract the objects array (guaranteed to exist due to our Pydantic model structure)\n        return structured_data.get(\"objects\", structured_data)\n\n    def build_structured_output(self) -> Data:\n        output = self.build_structured_output_base()\n        if not isinstance(output, list) or not output:\n            # handle empty or unexpected type case\n            msg = \"No structured output returned\"\n            raise ValueError(msg)\n        if len(output) == 1:\n            return Data(data=output[0])\n        if len(output) > 1:\n            # Multiple outputs - wrap them in a results container\n            return Data(data={\"results\": output})\n        return Data()\n\n    def build_structured_dataframe(self) -> DataFrame:\n        output = self.build_structured_output_base()\n        if not isinstance(output, list) or not output:\n            # handle empty or unexpected type case\n            msg = \"No structured output returned\"\n            raise ValueError(msg)\n        if len(output) == 1:\n            # For single dictionary, wrap in a list to create DataFrame with one row\n            return DataFrame([output[0]])\n        if len(output) > 1:\n            # Multiple outputs - convert to DataFrame directly\n            return DataFrame(output)\n        return DataFrame()\n\n    def _extract_output_with_trustcall(self, schema: BaseModel, config_dict: dict) -> list[BaseModel] | None:\n        try:\n            llm_with_structured_output = create_extractor(self.llm, tools=[schema], tool_choice=schema.__name__)\n            result = get_chat_result(\n                runnable=llm_with_structured_output,\n                system_message=self.system_prompt,\n                input_value=self.input_value,\n                config=config_dict,\n            )\n        except Exception as e:  # noqa: BLE001\n            logger.warning(\n                f\"Trustcall extraction failed, falling back to Langchain: {e} \"\n                \"(Note: This may not be an error—some models or configurations do not support tool calling. \"\n                \"Falling back is normal in such cases.)\"\n            )\n            return None\n        return result or None  # langchain fallback is used if error occurs or the result is empty\n\n    def _extract_output_with_langchain(self, schema: BaseModel, config_dict: dict) -> list[BaseModel] | None:\n        try:\n            llm_with_structured_output = self.llm.with_structured_output(schema)\n            result = get_chat_result(\n                runnable=llm_with_structured_output,\n                system_message=self.system_prompt,\n                input_value=self.input_value,\n                config=config_dict,\n            )\n            if isinstance(result, BaseModel):\n                result = result.model_dump()\n                result = result.get(\"objects\", result)\n        except Exception as fallback_error:\n            msg = (\n                f\"Model does not support tool calling (trustcall failed) \"\n                f\"and fallback with_structured_output also failed: {fallback_error}\"\n            )\n            raise ValueError(msg) from fallback_error\n\n        return result or None\n"
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Input Message",
                "dynamic": false,
                "info": "The input message to the language model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "llm": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Language Model",
                "dynamic": false,
                "info": "The language model to use to generate the structured output.",
                "input_types": [
                  "LanguageModel"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "llm",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "output_schema": {
                "_input_type": "TableInput",
                "advanced": false,
                "display_name": "Output Schema",
                "dynamic": false,
                "info": "Define the structure and data types for the model's output.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "output_schema",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "table_icon": "Table",
                "table_schema": [
                  {
                    "default": "field",
                    "description": "Specify the name of the output field.",
                    "display_name": "Name",
                    "edit_mode": "inline",
                    "formatter": "text",
                    "name": "name",
                    "type": "str"
                  },
                  {
                    "default": "description of field",
                    "description": "Describe the purpose of the output field.",
                    "display_name": "Description",
                    "edit_mode": "popover",
                    "formatter": "text",
                    "name": "description",
                    "type": "str"
                  },
                  {
                    "default": "str",
                    "description": "Indicate the data type of the output field (e.g., str, int, float, bool, dict).",
                    "display_name": "Type",
                    "edit_mode": "inline",
                    "formatter": "text",
                    "name": "type",
                    "options": [
                      "str",
                      "int",
                      "float",
                      "bool",
                      "dict"
                    ],
                    "type": "str"
                  },
                  {
                    "default": "False",
                    "description": "Set to True if this output field should be a list of the specified type.",
                    "display_name": "As List",
                    "edit_mode": "inline",
                    "formatter": "text",
                    "name": "multiple",
                    "type": "boolean"
                  }
                ],
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "trigger_icon": "Table",
                "trigger_text": "Open table",
                "type": "table",
                "value": [
                  {
                    "description": "description of field",
                    "multiple": "False",
                    "name": "audio_script",
                    "type": "str"
                  },
                  {
                    "description": "description of field",
                    "multiple": "False",
                    "name": "video_script",
                    "type": "str"
                  },
                  {
                    "description": "description of field",
                    "multiple": "False",
                    "name": "image_prompt",
                    "type": "str"
                  }
                ]
              },
              "schema_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Schema Name",
                "dynamic": false,
                "info": "Provide a name for the output data schema.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "schema_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "system_prompt": {
                "_input_type": "MultilineInput",
                "advanced": true,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Format Instructions",
                "dynamic": false,
                "info": "The instructions to the language model for formatting the output.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_prompt",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "You are an AI that extracts structured JSON objects from unstructured text. Use a predefined schema with expected types (str, int, float, bool, dict). Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all. Fill missing or ambiguous values with defaults: null for missing values. Remove exact duplicates but keep variations that have different field values. Always return valid JSON in the expected format, never throw errors. If multiple objects can be extracted, return them all in the structured format."
              }
            },
            "tool_mode": false
          },
          "selected_output": "structured_output",
          "showNode": true,
          "type": "StructuredOutput"
        },
        "dragging": false,
        "id": "StructuredOutput-FZySV",
        "measured": {
          "height": 347,
          "width": 320
        },
        "position": {
          "x": 373.3525283083599,
          "y": 828.4494077409549
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "Prompt Template-gtXmW",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": []
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt Template",
            "documentation": "https://docs.langflow.org/components-prompts",
            "edited": false,
            "error": null,
            "field_order": [
              "template",
              "tool_placeholder"
            ],
            "frozen": false,
            "full_path": null,
            "icon": "braces",
            "is_composition": null,
            "is_input": null,
            "is_output": null,
            "legacy": false,
            "lf_version": "1.7.1",
            "metadata": {
              "code_hash": "7382d03ce412",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 1
              },
              "module": "lfx.components.models_and_agents.prompt.PromptComponent"
            },
            "minimized": false,
            "name": "",
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Prompt",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "build_prompt",
                "name": "prompt",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": 0,
            "replacement": null,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from lfx.base.prompts.api_utils import process_prompt_template\nfrom lfx.custom.custom_component.component import Component\nfrom lfx.inputs.inputs import DefaultPromptField\nfrom lfx.io import MessageTextInput, Output, PromptInput\nfrom lfx.schema.message import Message\nfrom lfx.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt Template\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    documentation: str = \"https://docs.langflow.org/components-prompts\"\n    icon = \"braces\"\n    trace_type = \"prompt\"\n    name = \"Prompt Template\"\n    priority = 0  # Set priority to 0 to make it appear first\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n        MessageTextInput(\n            name=\"tool_placeholder\",\n            display_name=\"Tool Placeholder\",\n            tool_mode=True,\n            advanced=True,\n            info=\"A placeholder input for tool mode.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    async def update_frontend_node(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = await super().update_frontend_node(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "_input_type": "PromptInput",
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "template",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "track_in_telemetry": false,
                "type": "prompt",
                "value": "# Role\nYou are an expert Multimedia Content Architect and Creative Director. Your goal is to take a general topic or instruction and expand it into a complete content production package containing assets for Audio, Video, and Static Imagery.\n\n# Task\nAnalyze the USER_INSTRUCTION and generate a creative output structured strictly as a JSON object.\n\n# JSON Structure Requirements\nYou must output a single valid JSON object containing exactly three keys: `audio_script`, `video_script`, and `image_prompt`.\n\n## 1. Key: \"audio_script\"\n- **Purpose:** Text-to-Speech (TTS) generation.\n- **Content:** The verbatim spoken words.\n- **Guidelines:**\n  - Write for the ear, not the eye (conversational, natural flow).\n  - Include bracketed cues for the voice actor/AI (e.g., [Warm tone], [Pause], [Upbeat]).\n  - Specify the recommended speaking pace and tone at the beginning.\n\n## 2. Key: \"video_script\"\n- **Purpose:** A guide for a video editor or AI video generator.\n- **Content:** A chronological breakdown of visual scenes.\n- **Guidelines:**\n  - Describe the visual action, camera angles, and text overlays (if any).\n  - Ensure the visuals sync logically with the sentiment of the audio script.\n\n## 3. Key: \"image_prompt\"\n- **Purpose:** Generation of a high-quality thumbnail or cover image via models like Midjourney or DALL-E 3.\n- **Content:** A highly descriptive prompt.\n- **Guidelines:**\n  - Structure: [Subject] + [Environment] + [Art Style/Medium] + [Lighting/Color] + [Technical Parameters].\n  - Avoid abstract concepts; focus on visual elements.\n  - Include aspect ratio recommendations (e.g., --ar 16:9).\n\n# Output Format\nReturn **only** the JSON object. Do not include conversational filler or markdown formatting outside the JSON code block.\n"
              },
              "tool_placeholder": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Tool Placeholder",
                "dynamic": false,
                "info": "A placeholder input for tool mode.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tool_placeholder",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": true,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "Prompt Template"
        },
        "dragging": false,
        "id": "Prompt Template-gtXmW",
        "measured": {
          "height": 277,
          "width": 320
        },
        "position": {
          "x": -390.8923777497867,
          "y": 828.4494077409549
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "LanguageModelComponent-xejSI",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Runs a language model given a specified provider.",
            "display_name": "Language Model",
            "documentation": "https://docs.langflow.org/components-models",
            "edited": false,
            "field_order": [
              "provider",
              "model_name",
              "api_key",
              "base_url_ibm_watsonx",
              "project_id",
              "ollama_base_url",
              "input_value",
              "system_message",
              "stream",
              "temperature"
            ],
            "frozen": false,
            "icon": "brain-circuit",
            "last_updated": "2026-01-27T13:37:24.020Z",
            "legacy": false,
            "lf_version": "1.7.1",
            "metadata": {
              "code_hash": "694ffc4b17b8",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "requests",
                    "version": "2.32.5"
                  },
                  {
                    "name": "langchain_anthropic",
                    "version": "0.3.14"
                  },
                  {
                    "name": "langchain_ibm",
                    "version": "0.3.20"
                  },
                  {
                    "name": "langchain_ollama",
                    "version": "0.3.10"
                  },
                  {
                    "name": "langchain_openai",
                    "version": "0.3.35"
                  },
                  {
                    "name": "pydantic",
                    "version": "2.11.10"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 7
              },
              "keywords": [
                "model",
                "llm",
                "language model",
                "large language model"
              ],
              "module": "lfx.components.models_and_agents.language_model.LanguageModelComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Model Response",
                "group_outputs": false,
                "loop_types": null,
                "method": "text_response",
                "name": "text_output",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Language Model",
                "group_outputs": false,
                "loop_types": null,
                "method": "build_model",
                "name": "model_output",
                "options": null,
                "required_inputs": null,
                "selected": "LanguageModel",
                "tool_mode": true,
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "priority": 0,
            "template": {
              "_frontend_node_flow_id": {
                "value": "48ce6b32-e37c-4ff6-9b83-95c1569bba78"
              },
              "_frontend_node_folder_id": {
                "value": "3f1d0bf7-bd50-4fa9-a5d3-0974cfa99642"
              },
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "Model Provider API key",
                "input_types": [],
                "load_from_db": false,
                "name": "api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "base_url_ibm_watsonx": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "watsonx API Endpoint",
                "dynamic": false,
                "external_options": {},
                "info": "The base URL of the API (IBM watsonx.ai only)",
                "name": "base_url_ibm_watsonx",
                "options": [
                  "https://us-south.ml.cloud.ibm.com",
                  "https://eu-de.ml.cloud.ibm.com",
                  "https://eu-gb.ml.cloud.ibm.com",
                  "https://au-syd.ml.cloud.ibm.com",
                  "https://jp-tok.ml.cloud.ibm.com",
                  "https://ca-tor.ml.cloud.ibm.com"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "https://us-south.ml.cloud.ibm.com"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\n\nimport requests\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_ibm import ChatWatsonx\nfrom langchain_ollama import ChatOllama\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom lfx.base.models.anthropic_constants import ANTHROPIC_MODELS\nfrom lfx.base.models.google_generative_ai_constants import GOOGLE_GENERATIVE_AI_MODELS\nfrom lfx.base.models.google_generative_ai_model import ChatGoogleGenerativeAIFixed\nfrom lfx.base.models.model import LCModelComponent\nfrom lfx.base.models.model_utils import get_ollama_models, is_valid_ollama_url\nfrom lfx.base.models.openai_constants import OPENAI_CHAT_MODEL_NAMES, OPENAI_REASONING_MODEL_NAMES\nfrom lfx.field_typing import LanguageModel\nfrom lfx.field_typing.range_spec import RangeSpec\nfrom lfx.inputs.inputs import BoolInput, MessageTextInput, StrInput\nfrom lfx.io import DropdownInput, MessageInput, MultilineInput, SecretStrInput, SliderInput\nfrom lfx.log.logger import logger\nfrom lfx.schema.dotdict import dotdict\nfrom lfx.utils.util import transform_localhost_url\n\n# IBM watsonx.ai constants\nIBM_WATSONX_DEFAULT_MODELS = [\"ibm/granite-3-2b-instruct\", \"ibm/granite-3-8b-instruct\", \"ibm/granite-13b-instruct-v2\"]\nIBM_WATSONX_URLS = [\n    \"https://us-south.ml.cloud.ibm.com\",\n    \"https://eu-de.ml.cloud.ibm.com\",\n    \"https://eu-gb.ml.cloud.ibm.com\",\n    \"https://au-syd.ml.cloud.ibm.com\",\n    \"https://jp-tok.ml.cloud.ibm.com\",\n    \"https://ca-tor.ml.cloud.ibm.com\",\n]\n\n# Ollama API constants\nHTTP_STATUS_OK = 200\nJSON_MODELS_KEY = \"models\"\nJSON_NAME_KEY = \"name\"\nJSON_CAPABILITIES_KEY = \"capabilities\"\nDESIRED_CAPABILITY = \"completion\"\nDEFAULT_OLLAMA_URL = \"http://localhost:11434\"\n\n\nclass LanguageModelComponent(LCModelComponent):\n    display_name = \"Language Model\"\n    description = \"Runs a language model given a specified provider.\"\n    documentation: str = \"https://docs.langflow.org/components-models\"\n    icon = \"brain-circuit\"\n    category = \"models\"\n    priority = 0  # Set priority to 0 to make it appear first\n\n    @staticmethod\n    def fetch_ibm_models(base_url: str) -> list[str]:\n        \"\"\"Fetch available models from the watsonx.ai API.\"\"\"\n        try:\n            endpoint = f\"{base_url}/ml/v1/foundation_model_specs\"\n            params = {\"version\": \"2024-09-16\", \"filters\": \"function_text_chat,!lifecycle_withdrawn\"}\n            response = requests.get(endpoint, params=params, timeout=10)\n            response.raise_for_status()\n            data = response.json()\n            models = [model[\"model_id\"] for model in data.get(\"resources\", [])]\n            return sorted(models)\n        except Exception:  # noqa: BLE001\n            logger.exception(\"Error fetching IBM watsonx models. Using default models.\")\n            return IBM_WATSONX_DEFAULT_MODELS\n\n    inputs = [\n        DropdownInput(\n            name=\"provider\",\n            display_name=\"Model Provider\",\n            options=[\"OpenAI\", \"Anthropic\", \"Google\", \"IBM watsonx.ai\", \"Ollama\"],\n            value=\"OpenAI\",\n            info=\"Select the model provider\",\n            real_time_refresh=True,\n            options_metadata=[\n                {\"icon\": \"OpenAI\"},\n                {\"icon\": \"Anthropic\"},\n                {\"icon\": \"GoogleGenerativeAI\"},\n                {\"icon\": \"WatsonxAI\"},\n                {\"icon\": \"Ollama\"},\n            ],\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            options=OPENAI_CHAT_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES,\n            value=OPENAI_CHAT_MODEL_NAMES[0],\n            info=\"Select the model to use\",\n            real_time_refresh=True,\n            refresh_button=True,\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"Model Provider API key\",\n            required=False,\n            show=True,\n            real_time_refresh=True,\n        ),\n        DropdownInput(\n            name=\"base_url_ibm_watsonx\",\n            display_name=\"watsonx API Endpoint\",\n            info=\"The base URL of the API (IBM watsonx.ai only)\",\n            options=IBM_WATSONX_URLS,\n            value=IBM_WATSONX_URLS[0],\n            show=False,\n            real_time_refresh=True,\n        ),\n        StrInput(\n            name=\"project_id\",\n            display_name=\"watsonx Project ID\",\n            info=\"The project ID associated with the foundation model (IBM watsonx.ai only)\",\n            show=False,\n            required=False,\n        ),\n        MessageTextInput(\n            name=\"ollama_base_url\",\n            display_name=\"Ollama API URL\",\n            info=f\"Endpoint of the Ollama API (Ollama only). Defaults to {DEFAULT_OLLAMA_URL}\",\n            value=DEFAULT_OLLAMA_URL,\n            show=False,\n            real_time_refresh=True,\n            load_from_db=True,\n        ),\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Input\",\n            info=\"The input text to send to the model\",\n        ),\n        MultilineInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"A system message that helps set the behavior of the assistant\",\n            advanced=False,\n        ),\n        BoolInput(\n            name=\"stream\",\n            display_name=\"Stream\",\n            info=\"Whether to stream the response\",\n            value=False,\n            advanced=True,\n        ),\n        SliderInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.1,\n            info=\"Controls randomness in responses\",\n            range_spec=RangeSpec(min=0, max=1, step=0.01),\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:\n        provider = self.provider\n        model_name = self.model_name\n        temperature = self.temperature\n        stream = self.stream\n\n        if provider == \"OpenAI\":\n            if not self.api_key:\n                msg = \"OpenAI API key is required when using OpenAI provider\"\n                raise ValueError(msg)\n\n            if model_name in OPENAI_REASONING_MODEL_NAMES:\n                # reasoning models do not support temperature (yet)\n                temperature = None\n\n            return ChatOpenAI(\n                model_name=model_name,\n                temperature=temperature,\n                streaming=stream,\n                openai_api_key=self.api_key,\n            )\n        if provider == \"Anthropic\":\n            if not self.api_key:\n                msg = \"Anthropic API key is required when using Anthropic provider\"\n                raise ValueError(msg)\n            return ChatAnthropic(\n                model=model_name,\n                temperature=temperature,\n                streaming=stream,\n                anthropic_api_key=self.api_key,\n            )\n        if provider == \"Google\":\n            if not self.api_key:\n                msg = \"Google API key is required when using Google provider\"\n                raise ValueError(msg)\n            return ChatGoogleGenerativeAIFixed(\n                model=model_name,\n                temperature=temperature,\n                streaming=stream,\n                google_api_key=self.api_key,\n            )\n        if provider == \"IBM watsonx.ai\":\n            if not self.api_key:\n                msg = \"IBM API key is required when using IBM watsonx.ai provider\"\n                raise ValueError(msg)\n            if not self.base_url_ibm_watsonx:\n                msg = \"IBM watsonx API Endpoint is required when using IBM watsonx.ai provider\"\n                raise ValueError(msg)\n            if not self.project_id:\n                msg = \"IBM watsonx Project ID is required when using IBM watsonx.ai provider\"\n                raise ValueError(msg)\n            return ChatWatsonx(\n                apikey=SecretStr(self.api_key).get_secret_value(),\n                url=self.base_url_ibm_watsonx,\n                project_id=self.project_id,\n                model_id=model_name,\n                params={\n                    \"temperature\": temperature,\n                },\n                streaming=stream,\n            )\n        if provider == \"Ollama\":\n            if not self.ollama_base_url:\n                msg = \"Ollama API URL is required when using Ollama provider\"\n                raise ValueError(msg)\n            if not model_name:\n                msg = \"Model name is required when using Ollama provider\"\n                raise ValueError(msg)\n\n            transformed_base_url = transform_localhost_url(self.ollama_base_url)\n\n            # Check if URL contains /v1 suffix (OpenAI-compatible mode)\n            if transformed_base_url and transformed_base_url.rstrip(\"/\").endswith(\"/v1\"):\n                # Strip /v1 suffix and log warning\n                transformed_base_url = transformed_base_url.rstrip(\"/\").removesuffix(\"/v1\")\n                logger.warning(\n                    \"Detected '/v1' suffix in base URL. The Ollama component uses the native Ollama API, \"\n                    \"not the OpenAI-compatible API. The '/v1' suffix has been automatically removed. \"\n                    \"If you want to use the OpenAI-compatible API, please use the OpenAI component instead. \"\n                    \"Learn more at https://docs.ollama.com/openai#openai-compatibility\"\n                )\n\n            return ChatOllama(\n                base_url=transformed_base_url,\n                model=model_name,\n                temperature=temperature,\n            )\n        msg = f\"Unknown provider: {provider}\"\n        raise ValueError(msg)\n\n    async def update_build_config(\n        self, build_config: dotdict, field_value: Any, field_name: str | None = None\n    ) -> dotdict:\n        if field_name == \"provider\":\n            if field_value == \"OpenAI\":\n                build_config[\"model_name\"][\"options\"] = OPENAI_CHAT_MODEL_NAMES + OPENAI_REASONING_MODEL_NAMES\n                build_config[\"model_name\"][\"value\"] = OPENAI_CHAT_MODEL_NAMES[0]\n                build_config[\"api_key\"][\"display_name\"] = \"OpenAI API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"Anthropic\":\n                build_config[\"model_name\"][\"options\"] = ANTHROPIC_MODELS\n                build_config[\"model_name\"][\"value\"] = ANTHROPIC_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"Anthropic API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"Google\":\n                build_config[\"model_name\"][\"options\"] = GOOGLE_GENERATIVE_AI_MODELS\n                build_config[\"model_name\"][\"value\"] = GOOGLE_GENERATIVE_AI_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"Google API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"IBM watsonx.ai\":\n                build_config[\"model_name\"][\"options\"] = IBM_WATSONX_DEFAULT_MODELS\n                build_config[\"model_name\"][\"value\"] = IBM_WATSONX_DEFAULT_MODELS[0]\n                build_config[\"api_key\"][\"display_name\"] = \"IBM API Key\"\n                build_config[\"api_key\"][\"show\"] = True\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = True\n                build_config[\"project_id\"][\"show\"] = True\n                build_config[\"ollama_base_url\"][\"show\"] = False\n            elif field_value == \"Ollama\":\n                # Fetch Ollama models from the API\n                build_config[\"api_key\"][\"show\"] = False\n                build_config[\"base_url_ibm_watsonx\"][\"show\"] = False\n                build_config[\"project_id\"][\"show\"] = False\n                build_config[\"ollama_base_url\"][\"show\"] = True\n\n                # Try multiple sources to get the URL (in order of preference):\n                # 1. Instance attribute (already resolved from global/db)\n                # 2. Build config value (may be a global variable reference)\n                # 3. Default value\n                ollama_url = getattr(self, \"ollama_base_url\", None)\n                if not ollama_url:\n                    config_value = build_config[\"ollama_base_url\"].get(\"value\", DEFAULT_OLLAMA_URL)\n                    # If config_value looks like a variable name (all caps with underscores), use default\n                    is_variable_ref = (\n                        config_value\n                        and isinstance(config_value, str)\n                        and config_value.isupper()\n                        and \"_\" in config_value\n                    )\n                    if is_variable_ref:\n                        await logger.adebug(\n                            f\"Config value appears to be a variable reference: {config_value}, using default\"\n                        )\n                        ollama_url = DEFAULT_OLLAMA_URL\n                    else:\n                        ollama_url = config_value\n\n                await logger.adebug(f\"Fetching Ollama models for provider switch. URL: {ollama_url}\")\n                if await is_valid_ollama_url(url=ollama_url):\n                    try:\n                        models = await get_ollama_models(\n                            base_url_value=ollama_url,\n                            desired_capability=DESIRED_CAPABILITY,\n                            json_models_key=JSON_MODELS_KEY,\n                            json_name_key=JSON_NAME_KEY,\n                            json_capabilities_key=JSON_CAPABILITIES_KEY,\n                        )\n                        build_config[\"model_name\"][\"options\"] = models\n                        build_config[\"model_name\"][\"value\"] = models[0] if models else \"\"\n                    except ValueError:\n                        await logger.awarning(\"Failed to fetch Ollama models. Setting empty options.\")\n                        build_config[\"model_name\"][\"options\"] = []\n                        build_config[\"model_name\"][\"value\"] = \"\"\n                else:\n                    await logger.awarning(f\"Invalid Ollama URL: {ollama_url}\")\n                    build_config[\"model_name\"][\"options\"] = []\n                    build_config[\"model_name\"][\"value\"] = \"\"\n        elif (\n            field_name == \"base_url_ibm_watsonx\"\n            and field_value\n            and hasattr(self, \"provider\")\n            and self.provider == \"IBM watsonx.ai\"\n        ):\n            # Fetch IBM models when base_url changes\n            try:\n                models = self.fetch_ibm_models(base_url=field_value)\n                build_config[\"model_name\"][\"options\"] = models\n                build_config[\"model_name\"][\"value\"] = models[0] if models else IBM_WATSONX_DEFAULT_MODELS[0]\n                info_message = f\"Updated model options: {len(models)} models found in {field_value}\"\n                logger.info(info_message)\n            except Exception:  # noqa: BLE001\n                logger.exception(\"Error updating IBM model options.\")\n        elif field_name == \"ollama_base_url\":\n            # Fetch Ollama models when ollama_base_url changes\n            # Use the field_value directly since this is triggered when the field changes\n            logger.debug(\n                f\"Fetching Ollama models from updated URL: {build_config['ollama_base_url']} \\\n                and value {self.ollama_base_url}\",\n            )\n            await logger.adebug(f\"Fetching Ollama models from updated URL: {self.ollama_base_url}\")\n            if await is_valid_ollama_url(url=self.ollama_base_url):\n                try:\n                    models = await get_ollama_models(\n                        base_url_value=self.ollama_base_url,\n                        desired_capability=DESIRED_CAPABILITY,\n                        json_models_key=JSON_MODELS_KEY,\n                        json_name_key=JSON_NAME_KEY,\n                        json_capabilities_key=JSON_CAPABILITIES_KEY,\n                    )\n                    build_config[\"model_name\"][\"options\"] = models\n                    build_config[\"model_name\"][\"value\"] = models[0] if models else \"\"\n                    info_message = f\"Updated model options: {len(models)} models found in {self.ollama_base_url}\"\n                    await logger.ainfo(info_message)\n                except ValueError:\n                    await logger.awarning(\"Error updating Ollama model options.\")\n                    build_config[\"model_name\"][\"options\"] = []\n                    build_config[\"model_name\"][\"value\"] = \"\"\n            else:\n                await logger.awarning(f\"Invalid Ollama URL: {self.ollama_base_url}\")\n                build_config[\"model_name\"][\"options\"] = []\n                build_config[\"model_name\"][\"value\"] = \"\"\n        elif field_name == \"model_name\":\n            # Refresh Ollama models when model_name field is accessed\n            if hasattr(self, \"provider\") and self.provider == \"Ollama\":\n                ollama_url = getattr(self, \"ollama_base_url\", DEFAULT_OLLAMA_URL)\n                if await is_valid_ollama_url(url=ollama_url):\n                    try:\n                        models = await get_ollama_models(\n                            base_url_value=ollama_url,\n                            desired_capability=DESIRED_CAPABILITY,\n                            json_models_key=JSON_MODELS_KEY,\n                            json_name_key=JSON_NAME_KEY,\n                            json_capabilities_key=JSON_CAPABILITIES_KEY,\n                        )\n                        build_config[\"model_name\"][\"options\"] = models\n                    except ValueError:\n                        await logger.awarning(\"Failed to refresh Ollama models.\")\n                        build_config[\"model_name\"][\"options\"] = []\n                else:\n                    build_config[\"model_name\"][\"options\"] = []\n\n            # Hide system_message for o1 models - currently unsupported\n            if field_value and field_value.startswith(\"o1\") and hasattr(self, \"provider\") and self.provider == \"OpenAI\":\n                if \"system_message\" in build_config:\n                    build_config[\"system_message\"][\"show\"] = False\n            elif \"system_message\" in build_config:\n                build_config[\"system_message\"][\"show\"] = True\n        return build_config\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "The input text to send to the model",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "is_refresh": false,
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Name",
                "dynamic": false,
                "external_options": {},
                "info": "Select the model to use",
                "name": "model_name",
                "options": [
                  "gpt-4o-mini",
                  "gpt-4o",
                  "gpt-4.1",
                  "gpt-4.1-mini",
                  "gpt-4.1-nano",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "gpt-5.1",
                  "gpt-5",
                  "gpt-5-mini",
                  "gpt-5-nano",
                  "gpt-5-chat-latest",
                  "o1",
                  "o3-mini",
                  "o3",
                  "o3-pro",
                  "o4-mini",
                  "o4-mini-high"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "refresh_button": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "gpt-4o-mini"
              },
              "ollama_base_url": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Ollama API URL",
                "dynamic": false,
                "info": "Endpoint of the Ollama API (Ollama only). Defaults to http://localhost:11434",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "ollama_base_url",
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "project_id": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "watsonx Project ID",
                "dynamic": false,
                "info": "The project ID associated with the foundation model (IBM watsonx.ai only)",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "project_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "provider": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {},
                "info": "Select the model provider",
                "name": "provider",
                "options": [
                  "OpenAI",
                  "Anthropic",
                  "Google",
                  "IBM watsonx.ai",
                  "Ollama"
                ],
                "options_metadata": [
                  {
                    "icon": "OpenAI"
                  },
                  {
                    "icon": "Anthropic"
                  },
                  {
                    "icon": "GoogleGenerativeAI"
                  },
                  {
                    "icon": "WatsonxAI"
                  },
                  {
                    "icon": "Ollama"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "OpenAI"
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Whether to stream the response",
                "list": false,
                "list_add_label": "Add More",
                "name": "stream",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "A system message that helps set the behavior of the assistant",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "system_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "SliderInput",
                "advanced": true,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "Controls randomness in responses",
                "max_label": "",
                "max_label_icon": "",
                "min_label": "",
                "min_label_icon": "",
                "name": "temperature",
                "override_skip": false,
                "placeholder": "",
                "range_spec": {
                  "max": 1,
                  "min": 0,
                  "step": 0.01,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "slider_buttons": false,
                "slider_buttons_options": [],
                "slider_input": false,
                "title_case": false,
                "tool_mode": false,
                "track_in_telemetry": false,
                "type": "slider",
                "value": 0.1
              }
            },
            "tool_mode": false
          },
          "selected_output": "model_output",
          "showNode": true,
          "type": "LanguageModelComponent"
        },
        "dragging": false,
        "id": "LanguageModelComponent-xejSI",
        "measured": {
          "height": 531,
          "width": 320
        },
        "position": {
          "x": -7.035265788730783,
          "y": 256.4215103223143
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ParserComponent-nUZ5M",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Extracts text using a template.",
            "display_name": "Parser",
            "documentation": "https://docs.langflow.org/parser",
            "edited": false,
            "field_order": [
              "input_data",
              "mode",
              "pattern",
              "sep"
            ],
            "frozen": false,
            "icon": "braces",
            "legacy": false,
            "metadata": {
              "code_hash": "3cda25c3f7b5",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 1
              },
              "module": "lfx.components.processing.parser.ParserComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Parsed Text",
                "group_outputs": false,
                "method": "parse_combined_text",
                "name": "parsed_text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from lfx.custom.custom_component.component import Component\nfrom lfx.helpers.data import safe_convert\nfrom lfx.inputs.inputs import BoolInput, HandleInput, MessageTextInput, MultilineInput, TabInput\nfrom lfx.schema.data import Data\nfrom lfx.schema.dataframe import DataFrame\nfrom lfx.schema.message import Message\nfrom lfx.template.field.base import Output\n\n\nclass ParserComponent(Component):\n    display_name = \"Parser\"\n    description = \"Extracts text using a template.\"\n    documentation: str = \"https://docs.langflow.org/parser\"\n    icon = \"braces\"\n\n    inputs = [\n        HandleInput(\n            name=\"input_data\",\n            display_name=\"Data or DataFrame\",\n            input_types=[\"DataFrame\", \"Data\"],\n            info=\"Accepts either a DataFrame or a Data object.\",\n            required=True,\n        ),\n        TabInput(\n            name=\"mode\",\n            display_name=\"Mode\",\n            options=[\"Parser\", \"Stringify\"],\n            value=\"Parser\",\n            info=\"Convert into raw string instead of using a template.\",\n            real_time_refresh=True,\n        ),\n        MultilineInput(\n            name=\"pattern\",\n            display_name=\"Template\",\n            info=(\n                \"Use variables within curly brackets to extract column values for DataFrames \"\n                \"or key values for Data.\"\n                \"For example: `Name: {Name}, Age: {Age}, Country: {Country}`\"\n            ),\n            value=\"Text: {text}\",  # Example default\n            dynamic=True,\n            show=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"sep\",\n            display_name=\"Separator\",\n            advanced=True,\n            value=\"\\n\",\n            info=\"String used to separate rows/items.\",\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Parsed Text\",\n            name=\"parsed_text\",\n            info=\"Formatted text output.\",\n            method=\"parse_combined_text\",\n        ),\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name=None):\n        \"\"\"Dynamically hide/show `template` and enforce requirement based on `stringify`.\"\"\"\n        if field_name == \"mode\":\n            build_config[\"pattern\"][\"show\"] = self.mode == \"Parser\"\n            build_config[\"pattern\"][\"required\"] = self.mode == \"Parser\"\n            if field_value:\n                clean_data = BoolInput(\n                    name=\"clean_data\",\n                    display_name=\"Clean Data\",\n                    info=(\n                        \"Enable to clean the data by removing empty rows and lines \"\n                        \"in each cell of the DataFrame/ Data object.\"\n                    ),\n                    value=True,\n                    advanced=True,\n                    required=False,\n                )\n                build_config[\"clean_data\"] = clean_data.to_dict()\n            else:\n                build_config.pop(\"clean_data\", None)\n\n        return build_config\n\n    def _clean_args(self):\n        \"\"\"Prepare arguments based on input type.\"\"\"\n        input_data = self.input_data\n\n        match input_data:\n            case list() if all(isinstance(item, Data) for item in input_data):\n                msg = \"List of Data objects is not supported.\"\n                raise ValueError(msg)\n            case DataFrame():\n                return input_data, None\n            case Data():\n                return None, input_data\n            case dict() if \"data\" in input_data:\n                try:\n                    if \"columns\" in input_data:  # Likely a DataFrame\n                        return DataFrame.from_dict(input_data), None\n                    # Likely a Data object\n                    return None, Data(**input_data)\n                except (TypeError, ValueError, KeyError) as e:\n                    msg = f\"Invalid structured input provided: {e!s}\"\n                    raise ValueError(msg) from e\n            case _:\n                msg = f\"Unsupported input type: {type(input_data)}. Expected DataFrame or Data.\"\n                raise ValueError(msg)\n\n    def parse_combined_text(self) -> Message:\n        \"\"\"Parse all rows/items into a single text or convert input to string if `stringify` is enabled.\"\"\"\n        # Early return for stringify option\n        if self.mode == \"Stringify\":\n            return self.convert_to_string()\n\n        df, data = self._clean_args()\n\n        lines = []\n        if df is not None:\n            for _, row in df.iterrows():\n                formatted_text = self.pattern.format(**row.to_dict())\n                lines.append(formatted_text)\n        elif data is not None:\n            # Use format_map with a dict that returns default_value for missing keys\n            class DefaultDict(dict):\n                def __missing__(self, key):\n                    return data.default_value or \"\"\n\n            formatted_text = self.pattern.format_map(DefaultDict(data.data))\n            lines.append(formatted_text)\n\n        combined_text = self.sep.join(lines)\n        self.status = combined_text\n        return Message(text=combined_text)\n\n    def convert_to_string(self) -> Message:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        result = \"\"\n        if isinstance(self.input_data, list):\n            result = \"\\n\".join([safe_convert(item, clean_data=self.clean_data or False) for item in self.input_data])\n        else:\n            result = safe_convert(self.input_data or False)\n        self.log(f\"Converted to string with length: {len(result)}\")\n\n        message = Message(text=result)\n        self.status = message\n        return message\n"
              },
              "input_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Data or DataFrame",
                "dynamic": false,
                "info": "Accepts either a DataFrame or a Data object.",
                "input_types": [
                  "DataFrame",
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_data",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "mode": {
                "_input_type": "TabInput",
                "advanced": false,
                "display_name": "Mode",
                "dynamic": false,
                "info": "Convert into raw string instead of using a template.",
                "name": "mode",
                "options": [
                  "Parser",
                  "Stringify"
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "tab",
                "value": "Parser"
              },
              "pattern": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Template",
                "dynamic": true,
                "info": "Use variables within curly brackets to extract column values for DataFrames or key values for Data.For example: `Name: {Name}, Age: {Age}, Country: {Country}`",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "pattern",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "audio_script"
              },
              "sep": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "String used to separate rows/items.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sep",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ParserComponent"
        },
        "dragging": false,
        "id": "ParserComponent-nUZ5M",
        "measured": {
          "height": 327,
          "width": 320
        },
        "position": {
          "x": 760.1102083492889,
          "y": 828.4494077409549
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ParserComponent-ro3V7",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Extracts text using a template.",
            "display_name": "Parser",
            "documentation": "https://docs.langflow.org/parser",
            "edited": false,
            "field_order": [
              "input_data",
              "mode",
              "pattern",
              "sep"
            ],
            "frozen": false,
            "icon": "braces",
            "legacy": false,
            "metadata": {
              "code_hash": "3cda25c3f7b5",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 1
              },
              "module": "lfx.components.processing.parser.ParserComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Parsed Text",
                "group_outputs": false,
                "method": "parse_combined_text",
                "name": "parsed_text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from lfx.custom.custom_component.component import Component\nfrom lfx.helpers.data import safe_convert\nfrom lfx.inputs.inputs import BoolInput, HandleInput, MessageTextInput, MultilineInput, TabInput\nfrom lfx.schema.data import Data\nfrom lfx.schema.dataframe import DataFrame\nfrom lfx.schema.message import Message\nfrom lfx.template.field.base import Output\n\n\nclass ParserComponent(Component):\n    display_name = \"Parser\"\n    description = \"Extracts text using a template.\"\n    documentation: str = \"https://docs.langflow.org/parser\"\n    icon = \"braces\"\n\n    inputs = [\n        HandleInput(\n            name=\"input_data\",\n            display_name=\"Data or DataFrame\",\n            input_types=[\"DataFrame\", \"Data\"],\n            info=\"Accepts either a DataFrame or a Data object.\",\n            required=True,\n        ),\n        TabInput(\n            name=\"mode\",\n            display_name=\"Mode\",\n            options=[\"Parser\", \"Stringify\"],\n            value=\"Parser\",\n            info=\"Convert into raw string instead of using a template.\",\n            real_time_refresh=True,\n        ),\n        MultilineInput(\n            name=\"pattern\",\n            display_name=\"Template\",\n            info=(\n                \"Use variables within curly brackets to extract column values for DataFrames \"\n                \"or key values for Data.\"\n                \"For example: `Name: {Name}, Age: {Age}, Country: {Country}`\"\n            ),\n            value=\"Text: {text}\",  # Example default\n            dynamic=True,\n            show=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"sep\",\n            display_name=\"Separator\",\n            advanced=True,\n            value=\"\\n\",\n            info=\"String used to separate rows/items.\",\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Parsed Text\",\n            name=\"parsed_text\",\n            info=\"Formatted text output.\",\n            method=\"parse_combined_text\",\n        ),\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name=None):\n        \"\"\"Dynamically hide/show `template` and enforce requirement based on `stringify`.\"\"\"\n        if field_name == \"mode\":\n            build_config[\"pattern\"][\"show\"] = self.mode == \"Parser\"\n            build_config[\"pattern\"][\"required\"] = self.mode == \"Parser\"\n            if field_value:\n                clean_data = BoolInput(\n                    name=\"clean_data\",\n                    display_name=\"Clean Data\",\n                    info=(\n                        \"Enable to clean the data by removing empty rows and lines \"\n                        \"in each cell of the DataFrame/ Data object.\"\n                    ),\n                    value=True,\n                    advanced=True,\n                    required=False,\n                )\n                build_config[\"clean_data\"] = clean_data.to_dict()\n            else:\n                build_config.pop(\"clean_data\", None)\n\n        return build_config\n\n    def _clean_args(self):\n        \"\"\"Prepare arguments based on input type.\"\"\"\n        input_data = self.input_data\n\n        match input_data:\n            case list() if all(isinstance(item, Data) for item in input_data):\n                msg = \"List of Data objects is not supported.\"\n                raise ValueError(msg)\n            case DataFrame():\n                return input_data, None\n            case Data():\n                return None, input_data\n            case dict() if \"data\" in input_data:\n                try:\n                    if \"columns\" in input_data:  # Likely a DataFrame\n                        return DataFrame.from_dict(input_data), None\n                    # Likely a Data object\n                    return None, Data(**input_data)\n                except (TypeError, ValueError, KeyError) as e:\n                    msg = f\"Invalid structured input provided: {e!s}\"\n                    raise ValueError(msg) from e\n            case _:\n                msg = f\"Unsupported input type: {type(input_data)}. Expected DataFrame or Data.\"\n                raise ValueError(msg)\n\n    def parse_combined_text(self) -> Message:\n        \"\"\"Parse all rows/items into a single text or convert input to string if `stringify` is enabled.\"\"\"\n        # Early return for stringify option\n        if self.mode == \"Stringify\":\n            return self.convert_to_string()\n\n        df, data = self._clean_args()\n\n        lines = []\n        if df is not None:\n            for _, row in df.iterrows():\n                formatted_text = self.pattern.format(**row.to_dict())\n                lines.append(formatted_text)\n        elif data is not None:\n            # Use format_map with a dict that returns default_value for missing keys\n            class DefaultDict(dict):\n                def __missing__(self, key):\n                    return data.default_value or \"\"\n\n            formatted_text = self.pattern.format_map(DefaultDict(data.data))\n            lines.append(formatted_text)\n\n        combined_text = self.sep.join(lines)\n        self.status = combined_text\n        return Message(text=combined_text)\n\n    def convert_to_string(self) -> Message:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        result = \"\"\n        if isinstance(self.input_data, list):\n            result = \"\\n\".join([safe_convert(item, clean_data=self.clean_data or False) for item in self.input_data])\n        else:\n            result = safe_convert(self.input_data or False)\n        self.log(f\"Converted to string with length: {len(result)}\")\n\n        message = Message(text=result)\n        self.status = message\n        return message\n"
              },
              "input_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Data or DataFrame",
                "dynamic": false,
                "info": "Accepts either a DataFrame or a Data object.",
                "input_types": [
                  "DataFrame",
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_data",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "mode": {
                "_input_type": "TabInput",
                "advanced": false,
                "display_name": "Mode",
                "dynamic": false,
                "info": "Convert into raw string instead of using a template.",
                "name": "mode",
                "options": [
                  "Parser",
                  "Stringify"
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "tab",
                "value": "Parser"
              },
              "pattern": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Template",
                "dynamic": true,
                "info": "Use variables within curly brackets to extract column values for DataFrames or key values for Data.For example: `Name: {Name}, Age: {Age}, Country: {Country}`",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "pattern",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "video_script"
              },
              "sep": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "String used to separate rows/items.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sep",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ParserComponent"
        },
        "dragging": false,
        "id": "ParserComponent-ro3V7",
        "measured": {
          "height": 327,
          "width": 320
        },
        "position": {
          "x": 760.1102083492889,
          "y": 1719.9836072904975
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ParserComponent-E7pD3",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Extracts text using a template.",
            "display_name": "Parser",
            "documentation": "https://docs.langflow.org/parser",
            "edited": false,
            "field_order": [
              "input_data",
              "mode",
              "pattern",
              "sep"
            ],
            "frozen": false,
            "icon": "braces",
            "legacy": false,
            "metadata": {
              "code_hash": "3cda25c3f7b5",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 1
              },
              "module": "lfx.components.processing.parser.ParserComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Parsed Text",
                "group_outputs": false,
                "method": "parse_combined_text",
                "name": "parsed_text",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from lfx.custom.custom_component.component import Component\nfrom lfx.helpers.data import safe_convert\nfrom lfx.inputs.inputs import BoolInput, HandleInput, MessageTextInput, MultilineInput, TabInput\nfrom lfx.schema.data import Data\nfrom lfx.schema.dataframe import DataFrame\nfrom lfx.schema.message import Message\nfrom lfx.template.field.base import Output\n\n\nclass ParserComponent(Component):\n    display_name = \"Parser\"\n    description = \"Extracts text using a template.\"\n    documentation: str = \"https://docs.langflow.org/parser\"\n    icon = \"braces\"\n\n    inputs = [\n        HandleInput(\n            name=\"input_data\",\n            display_name=\"Data or DataFrame\",\n            input_types=[\"DataFrame\", \"Data\"],\n            info=\"Accepts either a DataFrame or a Data object.\",\n            required=True,\n        ),\n        TabInput(\n            name=\"mode\",\n            display_name=\"Mode\",\n            options=[\"Parser\", \"Stringify\"],\n            value=\"Parser\",\n            info=\"Convert into raw string instead of using a template.\",\n            real_time_refresh=True,\n        ),\n        MultilineInput(\n            name=\"pattern\",\n            display_name=\"Template\",\n            info=(\n                \"Use variables within curly brackets to extract column values for DataFrames \"\n                \"or key values for Data.\"\n                \"For example: `Name: {Name}, Age: {Age}, Country: {Country}`\"\n            ),\n            value=\"Text: {text}\",  # Example default\n            dynamic=True,\n            show=True,\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"sep\",\n            display_name=\"Separator\",\n            advanced=True,\n            value=\"\\n\",\n            info=\"String used to separate rows/items.\",\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Parsed Text\",\n            name=\"parsed_text\",\n            info=\"Formatted text output.\",\n            method=\"parse_combined_text\",\n        ),\n    ]\n\n    def update_build_config(self, build_config, field_value, field_name=None):\n        \"\"\"Dynamically hide/show `template` and enforce requirement based on `stringify`.\"\"\"\n        if field_name == \"mode\":\n            build_config[\"pattern\"][\"show\"] = self.mode == \"Parser\"\n            build_config[\"pattern\"][\"required\"] = self.mode == \"Parser\"\n            if field_value:\n                clean_data = BoolInput(\n                    name=\"clean_data\",\n                    display_name=\"Clean Data\",\n                    info=(\n                        \"Enable to clean the data by removing empty rows and lines \"\n                        \"in each cell of the DataFrame/ Data object.\"\n                    ),\n                    value=True,\n                    advanced=True,\n                    required=False,\n                )\n                build_config[\"clean_data\"] = clean_data.to_dict()\n            else:\n                build_config.pop(\"clean_data\", None)\n\n        return build_config\n\n    def _clean_args(self):\n        \"\"\"Prepare arguments based on input type.\"\"\"\n        input_data = self.input_data\n\n        match input_data:\n            case list() if all(isinstance(item, Data) for item in input_data):\n                msg = \"List of Data objects is not supported.\"\n                raise ValueError(msg)\n            case DataFrame():\n                return input_data, None\n            case Data():\n                return None, input_data\n            case dict() if \"data\" in input_data:\n                try:\n                    if \"columns\" in input_data:  # Likely a DataFrame\n                        return DataFrame.from_dict(input_data), None\n                    # Likely a Data object\n                    return None, Data(**input_data)\n                except (TypeError, ValueError, KeyError) as e:\n                    msg = f\"Invalid structured input provided: {e!s}\"\n                    raise ValueError(msg) from e\n            case _:\n                msg = f\"Unsupported input type: {type(input_data)}. Expected DataFrame or Data.\"\n                raise ValueError(msg)\n\n    def parse_combined_text(self) -> Message:\n        \"\"\"Parse all rows/items into a single text or convert input to string if `stringify` is enabled.\"\"\"\n        # Early return for stringify option\n        if self.mode == \"Stringify\":\n            return self.convert_to_string()\n\n        df, data = self._clean_args()\n\n        lines = []\n        if df is not None:\n            for _, row in df.iterrows():\n                formatted_text = self.pattern.format(**row.to_dict())\n                lines.append(formatted_text)\n        elif data is not None:\n            # Use format_map with a dict that returns default_value for missing keys\n            class DefaultDict(dict):\n                def __missing__(self, key):\n                    return data.default_value or \"\"\n\n            formatted_text = self.pattern.format_map(DefaultDict(data.data))\n            lines.append(formatted_text)\n\n        combined_text = self.sep.join(lines)\n        self.status = combined_text\n        return Message(text=combined_text)\n\n    def convert_to_string(self) -> Message:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        result = \"\"\n        if isinstance(self.input_data, list):\n            result = \"\\n\".join([safe_convert(item, clean_data=self.clean_data or False) for item in self.input_data])\n        else:\n            result = safe_convert(self.input_data or False)\n        self.log(f\"Converted to string with length: {len(result)}\")\n\n        message = Message(text=result)\n        self.status = message\n        return message\n"
              },
              "input_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Data or DataFrame",
                "dynamic": false,
                "info": "Accepts either a DataFrame or a Data object.",
                "input_types": [
                  "DataFrame",
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_data",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "mode": {
                "_input_type": "TabInput",
                "advanced": false,
                "display_name": "Mode",
                "dynamic": false,
                "info": "Convert into raw string instead of using a template.",
                "name": "mode",
                "options": [
                  "Parser",
                  "Stringify"
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "tab",
                "value": "Parser"
              },
              "pattern": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "ai_enabled": false,
                "copy_field": false,
                "display_name": "Template",
                "dynamic": true,
                "info": "Use variables within curly brackets to extract column values for DataFrames or key values for Data.For example: `Name: {Name}, Age: {Age}, Country: {Country}`",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "pattern",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "image_prompt"
              },
              "sep": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Separator",
                "dynamic": false,
                "info": "String used to separate rows/items.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sep",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "\n"
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ParserComponent"
        },
        "dragging": false,
        "id": "ParserComponent-E7pD3",
        "measured": {
          "height": 327,
          "width": 320
        },
        "position": {
          "x": 760.1102083492889,
          "y": 1232.1064923441656
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ModalConverterComponent-XOkQp",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": true,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Convert text to audio, image, or video using AI models.",
            "display_name": "Modal Converter",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_text",
              "output_type",
              "audio_image_provider",
              "video_provider",
              "openai_api_key",
              "gemini_api_key",
              "voice",
              "audio_format",
              "speed",
              "image_model",
              "image_size",
              "num_images",
              "video_model",
              "aspect_ratio"
            ],
            "frozen": false,
            "icon": "repeat",
            "last_updated": "2026-01-15T18:35:46.843Z",
            "legacy": false,
            "metadata": {
              "code_hash": "4f721424c63d",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "requests",
                    "version": "2.32.5"
                  },
                  {
                    "name": "openai",
                    "version": "1.109.1"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  },
                  {
                    "name": "google",
                    "version": "1.56.0"
                  }
                ],
                "total_dependencies": 4
              },
              "module": "custom_components.modal_converter"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Video Data",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "generate_video",
                "name": "video_output",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Video URLs",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "generate_video_urls",
                "name": "video_urls",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Markdown",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "generate_markdown",
                "name": "markdown_output",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_frontend_node_flow_id": {
                "value": "48ce6b32-e37c-4ff6-9b83-95c1569bba78"
              },
              "_frontend_node_folder_id": {
                "value": "3f1d0bf7-bd50-4fa9-a5d3-0974cfa99642"
              },
              "_type": "Component",
              "aspect_ratio": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Aspect Ratio",
                "dynamic": false,
                "external_options": {},
                "info": "Video format ratio",
                "name": "aspect_ratio",
                "options": [
                  "16:9",
                  "9:16"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "16:9"
              },
              "audio_format": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Audio Format",
                "dynamic": false,
                "external_options": {},
                "info": "Select audio format",
                "name": "audio_format",
                "options": [
                  "mp3",
                  "opus",
                  "aac",
                  "flac",
                  "wav",
                  "pcm"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "mp3"
              },
              "audio_image_provider": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "audio_image_provider",
                "options": [
                  "OpenAI"
                ],
                "options_metadata": [
                  {
                    "icon": "OpenAI"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "OpenAI"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\r\nimport base64\r\nimport requests\r\nimport time\r\nfrom openai import OpenAI\r\n\r\nfrom lfx.custom import Component\r\nfrom lfx.io import HandleInput, Output, TabInput, MessageTextInput, SecretStrInput, DropdownInput, IntInput\r\nfrom lfx.schema import Data, Message\r\n\r\n# Import Google GenAI for Veo video generation\r\nfrom google import genai\r\nfrom google.genai import types\r\n\r\n\r\nclass ModalConverterComponent(Component):\r\n    display_name = \"Modal Converter\"\r\n    description = \"Convert text to audio, image, or video using AI models.\"\r\n    icon = \"repeat\"\r\n    beta = True\r\n\r\n    inputs = [\r\n        HandleInput(\r\n            name=\"input_text\",\r\n            display_name=\"Input\",\r\n            input_types=[\"Message\", \"Data\", \"DataFrame\"],\r\n            info=\"Text input to convert to audio, image, or video\",\r\n            required=True,\r\n        ),\r\n        TabInput(\r\n            name=\"output_type\",\r\n            display_name=\"Output Type\",\r\n            options=[\"Audio\", \"Image\", \"Video\"],\r\n            info=\"Select the desired output media type\",\r\n            real_time_refresh=True,\r\n            value=\"Audio\",\r\n        ),\r\n        # Model provider for Audio and Image \r\n        DropdownInput(\r\n            name=\"audio_image_provider\",\r\n            display_name=\"Model Provider\",\r\n            options=[\"OpenAI\"],\r\n            value=\"OpenAI\",\r\n            real_time_refresh=True,\r\n            show=False,\r\n            options_metadata=[{\"icon\": \"OpenAI\"}],\r\n        ),\r\n        # Model provider for Video\r\n        DropdownInput(\r\n            name=\"video_provider\",\r\n            display_name=\"Model Provider\",\r\n            options=[\"Google Generative AI\"],\r\n            value=\"Google Generative AI\",\r\n            real_time_refresh=True,\r\n            show=False,\r\n            options_metadata=[{\"icon\": \"GoogleGenerativeAI\"}],\r\n        ),\r\n        SecretStrInput(\r\n            name=\"openai_api_key\",\r\n            display_name=\"OpenAI API Key\",\r\n            info=\"Your OpenAI API key for generating audio, images, and videos\",\r\n            required=True,\r\n            show=False,\r\n        ),\r\n        SecretStrInput(\r\n            name=\"gemini_api_key\",\r\n            display_name=\"Gemini API Key\",\r\n            info=\"Your Google Gemini API key for generating videos with Veo\",\r\n            required=True,\r\n            show=False,\r\n        ),\r\n        # Audio-specific options\r\n        DropdownInput(\r\n            name=\"voice\",\r\n            display_name=\"Voice\",\r\n            info=\"Select the voice for audio generation\",\r\n            options=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\r\n            value=\"alloy\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        DropdownInput(\r\n            name=\"audio_format\",\r\n            display_name=\"Audio Format\",\r\n            info=\"Select audio format\",\r\n            options=[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"],\r\n            value=\"mp3\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        IntInput(\r\n            name=\"speed\",\r\n            display_name=\"Speed\",\r\n            info=\"Speed of the generated audio. Values range from 0.25 to 4.0.\",\r\n            value=1,\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        # Image-specific options\r\n        DropdownInput(\r\n            name=\"image_model\",\r\n            display_name=\"Image Model\",\r\n            options=[\"dall-e-2\", \"dall-e-3\"],\r\n            value=\"dall-e-3\",\r\n            info=\"The DALL·E model version to use\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        DropdownInput(\r\n            name=\"image_size\",\r\n            display_name=\"Image Size\",\r\n            value=\"1024x1024\",\r\n            info=\"Size of the generated image\",\r\n            options=[\"256x256\", \"512x512\", \"1024x1024\", \"1792x1024\", \"1024x1792\"],\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        IntInput(\r\n            name=\"num_images\",\r\n            display_name=\"Number of Images\",\r\n            value=1,\r\n            info=\"Number of images to generate\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        # Video-specific options for Google Veo\r\n        DropdownInput(\r\n            name=\"video_model\",\r\n            display_name=\"Model\",\r\n            options=[\r\n                \"veo-3.0-generate-preview\",  # Latest Veo 3.0 model\r\n                \"veo-2.0-generate-001\",  # Veo 2.0 model (requires GCP billing)\r\n                \"models/veo-2.0-generate-001\",  # Full format for Veo 2.0\r\n            ],\r\n            value=\"veo-3.0-generate-preview\",\r\n            info=\"Veo model to use for video generation\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        DropdownInput(\r\n            name=\"aspect_ratio\",\r\n            display_name=\"Aspect Ratio\",\r\n            options=[\r\n                \"16:9\",  # Widescreen\r\n                \"9:16\",  # Portrait/Vertical\r\n            ],\r\n            value=\"16:9\",\r\n            info=\"Video format ratio\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Audio Data\",\r\n            name=\"audio_output\",\r\n            method=\"generate_audio\",\r\n        )\r\n    ]\r\n\r\n    def update_outputs(self, frontend_node: dict, field_name: str, field_value: Any) -> dict:\r\n        \"\"\"Dynamically show only the relevant output based on the selected output type.\"\"\"\r\n        if field_name == \"output_type\":\r\n            # Start with empty outputs\r\n            frontend_node[\"outputs\"] = []\r\n\r\n            # Add only the selected output type\r\n            if field_value == \"Audio\":\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Audio Data\",\r\n                        name=\"audio_output\",\r\n                        method=\"generate_audio\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Audio Base64\",\r\n                        name=\"audio_base64\",\r\n                        method=\"generate_audio_base64\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Markdown\",\r\n                        name=\"markdown_output\",\r\n                        method=\"generate_markdown\",\r\n                    ).to_dict()\r\n                )\r\n            elif field_value == \"Image\":\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Image Data\",\r\n                        name=\"image_output\",\r\n                        method=\"generate_image\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Image URL\",\r\n                        name=\"image_url\",\r\n                        method=\"generate_image_url\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Markdown\",\r\n                        name=\"markdown_output\",\r\n                        method=\"generate_markdown\",\r\n                    ).to_dict()\r\n                )\r\n            elif field_value == \"Video\":\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Video Data\",\r\n                        name=\"video_output\",\r\n                        method=\"generate_video\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Video URLs\",\r\n                        name=\"video_urls\",\r\n                        method=\"generate_video_urls\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Markdown\",\r\n                        name=\"markdown_output\",\r\n                        method=\"generate_markdown\",\r\n                    ).to_dict()\r\n                )\r\n\r\n        return frontend_node\r\n\r\n    def update_build_config(self, build_config, field_value, field_name=None):\r\n        if field_name == \"output_type\":\r\n            # Extract output type from the selected value\r\n            output_type = field_value if isinstance(field_value, str) else \"Audio\"\r\n\r\n            # Define field visibility map\r\n            field_map = {\r\n                \"Audio\": [\"audio_image_provider\", \"openai_api_key\", \"voice\", \"audio_format\", \"speed\"],\r\n                \"Image\": [\"audio_image_provider\", \"openai_api_key\", \"image_model\", \"image_size\", \"num_images\"],\r\n                \"Video\": [\"video_provider\", \"gemini_api_key\", \"video_model\", \"aspect_ratio\"],\r\n            }\r\n\r\n            # Hide all dynamic fields first\r\n            for field_name in [\"audio_image_provider\", \"video_provider\", \"openai_api_key\", \"gemini_api_key\", \"voice\", \"audio_format\", \"speed\", \"image_model\", \"image_size\", \"num_images\", \"video_model\", \"aspect_ratio\"]:\r\n                if field_name in build_config:\r\n                    build_config[field_name][\"show\"] = False\r\n\r\n            # Show fields based on selected output type\r\n            if output_type in field_map:\r\n                for field_name in field_map[output_type]:\r\n                    if field_name in build_config:\r\n                        build_config[field_name][\"show\"] = True\r\n\r\n        elif field_name == \"audio_image_provider\":\r\n            # For audio and image, always show OpenAI API key\r\n            if field_value == \"OpenAI\":\r\n                build_config[\"openai_api_key\"][\"display_name\"] = \"OpenAI API Key\"\r\n                build_config[\"openai_api_key\"][\"info\"] = \"Your OpenAI API key for generating audio and images\"\r\n                build_config[\"openai_api_key\"][\"show\"] = True\r\n                if \"gemini_api_key\" in build_config:\r\n                    build_config[\"gemini_api_key\"][\"show\"] = False\r\n\r\n        elif field_name == \"video_provider\":\r\n            # For video mode, only show gemini_api_key when Google Generative AI is selected\r\n            if field_value == \"Google Generative AI\":\r\n                if \"gemini_api_key\" in build_config:\r\n                    build_config[\"gemini_api_key\"][\"display_name\"] = \"Gemini API Key\"\r\n                    build_config[\"gemini_api_key\"][\"info\"] = \"Your Google Gemini API key for generating videos with Veo\"\r\n                    build_config[\"gemini_api_key\"][\"show\"] = True\r\n\r\n        return build_config\r\n\r\n    def _extract_text_from_input(self):\r\n        \"\"\"Extract text from various input types.\"\"\"\r\n        input_value = self.input_text[0] if isinstance(self.input_text, list) else self.input_text\r\n\r\n        # Handle string input\r\n        if isinstance(input_value, str):\r\n            return input_value\r\n\r\n        # Handle Message input\r\n        if hasattr(input_value, 'text'):\r\n            return input_value.text\r\n\r\n        # Handle Data input\r\n        if hasattr(input_value, 'data'):\r\n            if isinstance(input_value.data, dict) and 'text' in input_value.data:\r\n                return input_value.data['text']\r\n            elif isinstance(input_value.data, str):\r\n                return input_value.data\r\n\r\n        # Handle DataFrame input\r\n        if hasattr(input_value, 'to_message'):\r\n            message = input_value.to_message()\r\n            return message.text if hasattr(message, 'text') else str(message)\r\n\r\n        # Fallback\r\n        return str(input_value)\r\n\r\n    def _get_api_key(self):\r\n        \"\"\"Get API key based on selected provider and output type.\"\"\"\r\n        # For video generation, always use Gemini API\r\n        if hasattr(self, 'output_type') and self.output_type == \"Video\":\r\n            api_key = self.gemini_api_key\r\n        else:\r\n            # For audio and image, always use OpenAI\r\n            api_key = self.openai_api_key\r\n            \r\n        if hasattr(api_key, 'get_secret_value'):\r\n            return api_key.get_secret_value()\r\n        return str(api_key)\r\n\r\n    def generate_audio(self) -> Data:\r\n        \"\"\"Generate audio from text using OpenAI TTS API.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Data(data={\"error\": \"No text provided for audio generation\"})\r\n\r\n            api_key = self._get_api_key()\r\n            voice = getattr(self, 'voice', 'alloy') or 'alloy'\r\n            format_ = getattr(self, 'audio_format', 'mp3') or 'mp3'\r\n            speed = getattr(self, 'speed', 1) or 1\r\n\r\n            url = \"https://api.openai.com/v1/audio/speech\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {api_key}\",\r\n                \"Content-Type\": \"application/json\"\r\n            }\r\n            payload = {\r\n                \"model\": \"tts-1\",\r\n                \"input\": text,\r\n                \"voice\": voice,\r\n                \"response_format\": format_,\r\n                \"speed\": float(speed)\r\n            }\r\n\r\n            self.status = f\"Generating audio with voice '{voice}' in '{format_}' format at speed {speed}x...\"\r\n            self.log(f\"Making TTS request with voice: {voice}, format: {format_}, speed: {speed}\")\r\n\r\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\r\n            \r\n            if response.status_code == 200 and response.content:\r\n                audio_base64 = base64.b64encode(response.content).decode('utf-8')\r\n                audio_size_kb = len(response.content) / 1024\r\n                \r\n                self.status = f\"Audio generated successfully! Size: {audio_size_kb:.1f} KB\"\r\n                self.log(f\"Audio generated successfully. Size: {len(response.content)} bytes\")\r\n                \r\n                return Data(data={\r\n                    \"audio_base64\": audio_base64,\r\n                    \"format\": format_,\r\n                    \"voice\": voice,\r\n                    \"speed\": speed,\r\n                    \"size_bytes\": len(response.content),\r\n                    \"text\": text[:100] + \"...\" if len(text) > 100 else text,\r\n                })\r\n            else:\r\n                error_msg = f\"API Error {response.status_code}: {response.text}\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Data(data={\"error\": error_msg})\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating audio: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Data(data={\"error\": error_msg})\r\n\r\n    def generate_audio_base64(self) -> Message:\r\n        \"\"\"Generate audio and return only the base64 as text message.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Message(text=\"Error: No text provided for audio generation\")\r\n\r\n            api_key = self._get_api_key()\r\n            voice = getattr(self, 'voice', 'alloy') or 'alloy'\r\n            format_ = getattr(self, 'audio_format', 'mp3') or 'mp3'\r\n            speed = getattr(self, 'speed', 1) or 1\r\n\r\n            url = \"https://api.openai.com/v1/audio/speech\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {api_key}\",\r\n                \"Content-Type\": \"application/json\"\r\n            }\r\n            payload = {\r\n                \"model\": \"tts-1\",\r\n                \"input\": text,\r\n                \"voice\": voice,\r\n                \"response_format\": format_,\r\n                \"speed\": float(speed)\r\n            }\r\n\r\n            self.status = f\"Generating audio with voice '{voice}' in '{format_}' format at speed {speed}x...\"\r\n            self.log(f\"Making TTS request with voice: {voice}, format: {format_}, speed: {speed}\")\r\n\r\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\r\n            \r\n            if response.status_code == 200 and response.content:\r\n                audio_base64 = base64.b64encode(response.content).decode('utf-8')\r\n                audio_size_kb = len(response.content) / 1024\r\n                \r\n                self.status = f\"Audio generated successfully! Size: {audio_size_kb:.1f} KB\"\r\n                self.log(f\"Audio generated successfully. Size: {len(response.content)} bytes\")\r\n                \r\n                return Message(text=audio_base64)\r\n            else:\r\n                error_msg = f\"API Error {response.status_code}: {response.text}\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating audio: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def generate_markdown(self) -> Message:\r\n        \"\"\"Generate markdown output based on the selected output type.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Message(text=\"Error: No text provided for markdown generation\")\r\n\r\n            output_type = getattr(self, 'output_type', 'Audio') or 'Audio'\r\n\r\n            if output_type == \"Audio\":\r\n                return self._generate_audio_markdown(text)\r\n            elif output_type == \"Image\":\r\n                return self._generate_image_markdown(text)\r\n            elif output_type == \"Video\":\r\n                return self._generate_video_markdown(text)\r\n            else:\r\n                return Message(text=\"Error: Unknown output type\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def _generate_audio_markdown(self, text: str) -> Message:\r\n        \"\"\"Generate HTML audio player code for the generated audio.\"\"\"\r\n        try:\r\n            api_key = self._get_api_key()\r\n            voice = getattr(self, 'voice', 'alloy') or 'alloy'\r\n            format_ = getattr(self, 'audio_format', 'mp3') or 'mp3'\r\n            speed = getattr(self, 'speed', 1) or 1\r\n\r\n            url = \"https://api.openai.com/v1/audio/speech\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {api_key}\",\r\n                \"Content-Type\": \"application/json\"\r\n            }\r\n            payload = {\r\n                \"model\": \"tts-1\",\r\n                \"input\": text,\r\n                \"voice\": voice,\r\n                \"response_format\": format_,\r\n                \"speed\": float(speed)\r\n            }\r\n\r\n            self.status = f\"Generating audio with voice '{voice}' in '{format_}' format at speed {speed}x...\"\r\n            self.log(f\"Making TTS request with voice: {voice}, format: {format_}, speed: {speed}\")\r\n\r\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\r\n            \r\n            if response.status_code == 200 and response.content:\r\n                audio_base64 = base64.b64encode(response.content).decode('utf-8')\r\n                audio_size_kb = len(response.content) / 1024\r\n                \r\n                self.status = f\"Audio generated successfully! Size: {audio_size_kb:.1f} KB\"\r\n                self.log(f\"Audio generated successfully. Size: {len(response.content)} bytes\")\r\n                \r\n                # Generate HTML audio player code\r\n                html_code = f'<audio controls>\\n  <source src=\"data:audio/{format_};base64,{audio_base64}\" type=\"audio/{format_}\">\\n</audio>'\r\n                \r\n                return Message(text=html_code)\r\n            else:\r\n                error_msg = f\"API Error {response.status_code}: {response.text}\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating audio markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def _generate_image_markdown(self, text: str) -> Message:\r\n        \"\"\"Generate markdown image code for the generated image.\"\"\"\r\n        try:\r\n            # Check if only one image is requested\r\n            n = getattr(self, 'num_images', 1) or 1\r\n            if n != 1:\r\n                return Message(text=\"Markdown Image output is only available when generating a single image (Number of Images = 1)\")\r\n\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'image_model', 'dall-e-3') or 'dall-e-3'\r\n            size = getattr(self, 'image_size', '1024x1024') or '1024x1024'\r\n\r\n            client = OpenAI(api_key=api_key)\r\n\r\n            self.status = f\"Generating image using {model} model...\"\r\n            self.log(f\"Making image generation request with model: {model}, size: {size}, count: {n}\")\r\n\r\n            response = client.images.generate(\r\n                model=model,\r\n                prompt=text,\r\n                n=n,\r\n                size=size\r\n            )\r\n\r\n            # Return markdown image code for the first image\r\n            if response.data and len(response.data) > 0:\r\n                image_url = response.data[0].url\r\n                # Create a description from the prompt\r\n                description = text[:50] + \"...\" if len(text) > 50 else text\r\n                markdown_code = f\"![{description}]({image_url})\"\r\n                \r\n                self.status = f\"Generated image markdown successfully!\"\r\n                self.log(f\"Generated image markdown: {markdown_code}\")\r\n                \r\n                return Message(text=markdown_code)\r\n            else:\r\n                error_msg = \"No image URL generated\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating image markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def _generate_video_markdown(self, text: str) -> Message:\r\n        \"\"\"Generate HTML video player code for the generated video.\"\"\"\r\n        try:\r\n            # First, generate the video using the main method\r\n            video_result = self.generate_video()\r\n            \r\n            # Check if video generation was successful\r\n            if hasattr(video_result, 'data') and isinstance(video_result.data, dict):\r\n                if 'error' in video_result.data:\r\n                    return Message(text=f\"Error: {video_result.data['error']}\")\r\n                \r\n                # Get the primary video URL\r\n                video_url = video_result.data.get('video_url')\r\n                if not video_url:\r\n                    return Message(text=\"Error: No video URL generated\")\r\n                \r\n                # Determine aspect ratio for dimensions\r\n                aspect_ratio = getattr(self, 'aspect_ratio', '16:9') or '16:9'\r\n                if aspect_ratio == \"16:9\":\r\n                    width, height = 640, 360\r\n                elif aspect_ratio == \"9:16\":\r\n                    width, height = 360, 640\r\n                else:\r\n                    width, height = 640, 360  # Default to 16:9\r\n                \r\n                # Generate HTML video player code\r\n                html_code = f'<video width=\"{width}\" height=\"{height}\" controls>\\n  <source src=\"{video_url}\">\\n</video>'\r\n                \r\n                self.status = f\"Generated video HTML for {aspect_ratio} aspect ratio\"\r\n                return Message(text=html_code)\r\n            else:\r\n                return Message(text=\"Error: Invalid video generation result\")\r\n                \r\n        except Exception as e:\r\n            error_msg = f\"Error generating video markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def generate_image(self) -> Data:\r\n        \"\"\"Generate image from text using OpenAI DALL-E API.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Data(data={\"error\": \"No text provided for image generation\"})\r\n\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'image_model', 'dall-e-3') or 'dall-e-3'\r\n            size = getattr(self, 'image_size', '1024x1024') or '1024x1024'\r\n            n = getattr(self, 'num_images', 1) or 1\r\n\r\n            client = OpenAI(api_key=api_key)\r\n\r\n            self.status = f\"Generating image using {model} model...\"\r\n            self.log(f\"Making image generation request with model: {model}, size: {size}, count: {n}\")\r\n\r\n            response = client.images.generate(\r\n                model=model,\r\n                prompt=text,\r\n                n=n,\r\n                size=size\r\n            )\r\n\r\n            image_urls = [data.url for data in response.data]\r\n            self.status = f\"Generated {len(image_urls)} image(s) successfully!\"\r\n            self.log(f\"Generated {len(image_urls)} images\")\r\n\r\n            # Se apenas uma imagem foi gerada, retornar a URL diretamente\r\n            if n == 1 and len(image_urls) == 1:\r\n                return Data(data={\r\n                    \"image_url\": image_urls[0],\r\n                    \"model\": model,\r\n                    \"size\": size,\r\n                    \"count\": n,\r\n                    \"prompt\": text[:100] + \"...\" if len(text) > 100 else text,\r\n                })\r\n            else:\r\n                return Data(data={\r\n                    \"image_urls\": image_urls,\r\n                    \"model\": model,\r\n                    \"size\": size,\r\n                    \"count\": n,\r\n                    \"prompt\": text[:100] + \"...\" if len(text) > 100 else text,\r\n                })\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating image: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Data(data={\"error\": error_msg})\r\n\r\n    def generate_image_url(self) -> Message:\r\n        \"\"\"Generate image and return only the URL as text message.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Message(text=\"Error: No text provided for image generation\")\r\n\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'image_model', 'dall-e-3') or 'dall-e-3'\r\n            size = getattr(self, 'image_size', '1024x1024') or '1024x1024'\r\n            n = getattr(self, 'num_images', 1) or 1\r\n\r\n            client = OpenAI(api_key=api_key)\r\n\r\n            self.status = f\"Generating image using {model} model...\"\r\n            self.log(f\"Making image generation request with model: {model}, size: {size}, count: {n}\")\r\n\r\n            response = client.images.generate(\r\n                model=model,\r\n                prompt=text,\r\n                n=n,\r\n                size=size\r\n            )\r\n\r\n            # Return only the first image URL as text\r\n            if response.data and len(response.data) > 0:\r\n                image_url = response.data[0].url\r\n                self.status = f\"Generated image URL successfully!\"\r\n                self.log(f\"Generated image URL: {image_url}\")\r\n                \r\n                return Message(text=image_url)\r\n            else:\r\n                error_msg = \"No image URL generated\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating image URL: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n\r\n\r\n    def generate_video(self) -> Data:\r\n        \"\"\"Generate video from text using Google Veo.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Data(data={\"error\": \"No text provided for video generation\"})\r\n            \r\n            return self._generate_video_with_veo(text)\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating video: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Data(data={\"error\": error_msg})\r\n\r\n    def _generate_video_with_veo(self, text: str) -> Data:\r\n        \"\"\"Generate video using Google Veo.\"\"\"\r\n        try:\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'video_model', 'veo-3.0-generate-preview') or 'veo-3.0-generate-preview'\r\n            aspect_ratio = getattr(self, 'aspect_ratio', '16:9') or '16:9'\r\n\r\n            # Create client with API key\r\n            client = genai.Client(api_key=api_key)\r\n\r\n            self.status = f\"Generating video using {model} model...\"\r\n            self.log(f\"Making video generation request with model: {model}, aspect_ratio: {aspect_ratio}\")\r\n\r\n            # Generate video using the selected model\r\n            operation = client.models.generate_videos(\r\n                model=model,\r\n                prompt=text,\r\n                config=types.GenerateVideosConfig(\r\n                    aspect_ratio=aspect_ratio,\r\n                ),\r\n            )\r\n\r\n            self.status = f\"Waiting for video generation completion using {model}...\"\r\n            \r\n            # Poll for completion with proper interval (20 seconds as per documentation)\r\n            while not operation.done:\r\n                time.sleep(20)\r\n                operation = client.operations.get(operation)\r\n\r\n            # Process generated videos\r\n            video_urls = []\r\n            video_data = []\r\n            \r\n            for n, generated_video in enumerate(operation.response.generated_videos):\r\n                if hasattr(generated_video, 'video') and generated_video.video:\r\n                    video_info = {\r\n                        \"video_id\": n,\r\n                        \"video_object\": generated_video.video\r\n                    }\r\n                    \r\n                    # Add URI if available (needs API key appended for download)\r\n                    if hasattr(generated_video.video, 'uri'):\r\n                        video_url = f\"{generated_video.video.uri}&key={api_key}\"\r\n                        video_info[\"video_uri\"] = video_url\r\n                        video_urls.append(video_url)\r\n                    \r\n                    video_data.append(video_info)\r\n\r\n            if not video_data:\r\n                raise ValueError(\"No video was generated.\")\r\n\r\n            self.status = f\"Video(s) generated successfully using {model}. Total: {len(video_data)}\"\r\n            \r\n            # Return the first video URL as main output, with detailed data available\r\n            primary_video_url = video_urls[0] if video_urls else None\r\n            \r\n            return Data(data={\r\n                \"video_url\": primary_video_url,  # Direct link to first video\r\n                \"video_urls\": video_urls,        # List of all links\r\n                \"video_count\": len(video_data),\r\n                \"videos\": video_data,            # Complete data\r\n                \"model_used\": model,             # Model used\r\n                \"prompt_used\": text,\r\n                \"aspect_ratio\": aspect_ratio,\r\n                \"provider\": \"Gemini\"\r\n            })\r\n\r\n        except Exception as e:\r\n            error_message = str(e)\r\n            self.status = f\"Error with model {model}: {error_message}\"\r\n            \r\n            # Provide helpful error info\r\n            return Data(data={\r\n                \"error\": error_message,\r\n                \"model_attempted\": model,\r\n                \"video_count\": 0,\r\n                \"provider\": \"Gemini\",\r\n                \"suggestion\": \"Try using veo-3.0-generate-preview for the latest model, or check your API key and billing setup\"\r\n            })\r\n\r\n    def generate_video_urls(self) -> Message:\r\n        \"\"\"Generate video and return only the URLs as text message.\"\"\"\r\n        try:\r\n            # First, generate the video using the main method\r\n            video_result = self.generate_video()\r\n            \r\n            # Check if video generation was successful\r\n            if hasattr(video_result, 'data') and isinstance(video_result.data, dict):\r\n                if 'error' in video_result.data:\r\n                    return Message(text=f\"Error: {video_result.data['error']}\")\r\n                \r\n                # Get video URLs\r\n                video_urls = video_result.data.get('video_urls', [])\r\n                if not video_urls:\r\n                    return Message(text=\"Error: No video URLs generated\")\r\n                \r\n                # Return URLs as comma-separated text\r\n                urls_text = \", \".join(video_urls)\r\n                self.status = f\"Generated {len(video_urls)} video URL(s) successfully!\"\r\n                return Message(text=urls_text)\r\n            else:\r\n                return Message(text=\"Error: Invalid video generation result\")\r\n                \r\n        except Exception as e:\r\n            error_msg = f\"Error generating video URLs: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n\r\n"
              },
              "gemini_api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Gemini API Key",
                "dynamic": false,
                "info": "Your Google Gemini API key for generating videos with Veo",
                "input_types": [],
                "load_from_db": false,
                "name": "gemini_api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "image_model": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Image Model",
                "dynamic": false,
                "external_options": {},
                "info": "The DALL·E model version to use",
                "name": "image_model",
                "options": [
                  "dall-e-2",
                  "dall-e-3"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "dall-e-3"
              },
              "image_size": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Image Size",
                "dynamic": false,
                "external_options": {},
                "info": "Size of the generated image",
                "name": "image_size",
                "options": [
                  "256x256",
                  "512x512",
                  "1024x1024",
                  "1792x1024",
                  "1024x1792"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "1024x1024"
              },
              "input_text": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "Text input to convert to audio, image, or video",
                "input_types": [
                  "Message",
                  "Data",
                  "DataFrame"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_text",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "is_refresh": false,
              "num_images": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Images",
                "dynamic": false,
                "info": "Number of images to generate",
                "list": false,
                "list_add_label": "Add More",
                "name": "num_images",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1
              },
              "openai_api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "Your OpenAI API key for generating audio, images, and videos",
                "input_types": [],
                "load_from_db": true,
                "name": "openai_api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": false,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "output_type": {
                "_input_type": "TabInput",
                "advanced": false,
                "display_name": "Output Type",
                "dynamic": false,
                "info": "Select the desired output media type",
                "name": "output_type",
                "options": [
                  "Audio",
                  "Image",
                  "Video"
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "tab",
                "value": "Video"
              },
              "speed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Speed",
                "dynamic": false,
                "info": "Speed of the generated audio. Values range from 0.25 to 4.0.",
                "list": false,
                "list_add_label": "Add More",
                "name": "speed",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1
              },
              "video_model": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model",
                "dynamic": false,
                "external_options": {},
                "info": "Veo model to use for video generation",
                "name": "video_model",
                "options": [
                  "veo-3.0-generate-preview",
                  "veo-2.0-generate-001",
                  "models/veo-2.0-generate-001"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "veo-3.0-generate-preview"
              },
              "video_provider": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "video_provider",
                "options": [
                  "Google Generative AI"
                ],
                "options_metadata": [
                  {
                    "icon": "GoogleGenerativeAI"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "Google Generative AI"
              },
              "voice": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Voice",
                "dynamic": false,
                "external_options": {},
                "info": "Select the voice for audio generation",
                "name": "voice",
                "options": [
                  "alloy",
                  "echo",
                  "fable",
                  "onyx",
                  "nova",
                  "shimmer"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "alloy"
              }
            },
            "tool_mode": false
          },
          "selected_output": "video_output",
          "showNode": true,
          "type": "ModalConverterComponent"
        },
        "dragging": false,
        "id": "ModalConverterComponent-XOkQp",
        "measured": {
          "height": 425,
          "width": 320
        },
        "position": {
          "x": 1145.6606782762408,
          "y": 1719.9836072904975
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ModalConverterComponent-NViIi",
          "node": {
            "base_classes": [
              "Data"
            ],
            "beta": true,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Convert text to audio, image, or video using AI models.",
            "display_name": "Modal Converter",
            "documentation": "",
            "edited": true,
            "field_order": [
              "input_text",
              "output_type",
              "audio_image_provider",
              "video_provider",
              "openai_api_key",
              "gemini_api_key",
              "voice",
              "audio_format",
              "speed",
              "image_model",
              "image_size",
              "num_images",
              "video_model",
              "aspect_ratio"
            ],
            "frozen": false,
            "icon": "repeat",
            "last_updated": "2026-01-15T18:35:46.015Z",
            "legacy": false,
            "metadata": {
              "code_hash": "4f721424c63d",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "requests",
                    "version": "2.32.5"
                  },
                  {
                    "name": "openai",
                    "version": "1.109.1"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  },
                  {
                    "name": "google",
                    "version": "1.56.0"
                  }
                ],
                "total_dependencies": 4
              },
              "module": "custom_components.modal_converter"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Image Data",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "generate_image",
                "name": "image_output",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Image URL",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "generate_image_url",
                "name": "image_url",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Markdown",
                "group_outputs": false,
                "hidden": null,
                "loop_types": null,
                "method": "generate_markdown",
                "name": "markdown_output",
                "options": null,
                "required_inputs": null,
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_frontend_node_flow_id": {
                "value": "48ce6b32-e37c-4ff6-9b83-95c1569bba78"
              },
              "_frontend_node_folder_id": {
                "value": "3f1d0bf7-bd50-4fa9-a5d3-0974cfa99642"
              },
              "_type": "Component",
              "aspect_ratio": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Aspect Ratio",
                "dynamic": false,
                "external_options": {},
                "info": "Video format ratio",
                "name": "aspect_ratio",
                "options": [
                  "16:9",
                  "9:16"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "16:9"
              },
              "audio_format": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Audio Format",
                "dynamic": false,
                "external_options": {},
                "info": "Select audio format",
                "name": "audio_format",
                "options": [
                  "mp3",
                  "opus",
                  "aac",
                  "flac",
                  "wav",
                  "pcm"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "mp3"
              },
              "audio_image_provider": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "audio_image_provider",
                "options": [
                  "OpenAI"
                ],
                "options_metadata": [
                  {
                    "icon": "OpenAI"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "OpenAI"
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\r\nimport base64\r\nimport requests\r\nimport time\r\nfrom openai import OpenAI\r\n\r\nfrom lfx.custom import Component\r\nfrom lfx.io import HandleInput, Output, TabInput, MessageTextInput, SecretStrInput, DropdownInput, IntInput\r\nfrom lfx.schema import Data, Message\r\n\r\n# Import Google GenAI for Veo video generation\r\nfrom google import genai\r\nfrom google.genai import types\r\n\r\n\r\nclass ModalConverterComponent(Component):\r\n    display_name = \"Modal Converter\"\r\n    description = \"Convert text to audio, image, or video using AI models.\"\r\n    icon = \"repeat\"\r\n    beta = True\r\n\r\n    inputs = [\r\n        HandleInput(\r\n            name=\"input_text\",\r\n            display_name=\"Input\",\r\n            input_types=[\"Message\", \"Data\", \"DataFrame\"],\r\n            info=\"Text input to convert to audio, image, or video\",\r\n            required=True,\r\n        ),\r\n        TabInput(\r\n            name=\"output_type\",\r\n            display_name=\"Output Type\",\r\n            options=[\"Audio\", \"Image\", \"Video\"],\r\n            info=\"Select the desired output media type\",\r\n            real_time_refresh=True,\r\n            value=\"Audio\",\r\n        ),\r\n        # Model provider for Audio and Image \r\n        DropdownInput(\r\n            name=\"audio_image_provider\",\r\n            display_name=\"Model Provider\",\r\n            options=[\"OpenAI\"],\r\n            value=\"OpenAI\",\r\n            real_time_refresh=True,\r\n            show=False,\r\n            options_metadata=[{\"icon\": \"OpenAI\"}],\r\n        ),\r\n        # Model provider for Video\r\n        DropdownInput(\r\n            name=\"video_provider\",\r\n            display_name=\"Model Provider\",\r\n            options=[\"Google Generative AI\"],\r\n            value=\"Google Generative AI\",\r\n            real_time_refresh=True,\r\n            show=False,\r\n            options_metadata=[{\"icon\": \"GoogleGenerativeAI\"}],\r\n        ),\r\n        SecretStrInput(\r\n            name=\"openai_api_key\",\r\n            display_name=\"OpenAI API Key\",\r\n            info=\"Your OpenAI API key for generating audio, images, and videos\",\r\n            required=True,\r\n            show=False,\r\n        ),\r\n        SecretStrInput(\r\n            name=\"gemini_api_key\",\r\n            display_name=\"Gemini API Key\",\r\n            info=\"Your Google Gemini API key for generating videos with Veo\",\r\n            required=True,\r\n            show=False,\r\n        ),\r\n        # Audio-specific options\r\n        DropdownInput(\r\n            name=\"voice\",\r\n            display_name=\"Voice\",\r\n            info=\"Select the voice for audio generation\",\r\n            options=[\"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"],\r\n            value=\"alloy\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        DropdownInput(\r\n            name=\"audio_format\",\r\n            display_name=\"Audio Format\",\r\n            info=\"Select audio format\",\r\n            options=[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"],\r\n            value=\"mp3\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        IntInput(\r\n            name=\"speed\",\r\n            display_name=\"Speed\",\r\n            info=\"Speed of the generated audio. Values range from 0.25 to 4.0.\",\r\n            value=1,\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        # Image-specific options\r\n        DropdownInput(\r\n            name=\"image_model\",\r\n            display_name=\"Image Model\",\r\n            options=[\"dall-e-2\", \"dall-e-3\"],\r\n            value=\"dall-e-3\",\r\n            info=\"The DALL·E model version to use\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        DropdownInput(\r\n            name=\"image_size\",\r\n            display_name=\"Image Size\",\r\n            value=\"1024x1024\",\r\n            info=\"Size of the generated image\",\r\n            options=[\"256x256\", \"512x512\", \"1024x1024\", \"1792x1024\", \"1024x1792\"],\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        IntInput(\r\n            name=\"num_images\",\r\n            display_name=\"Number of Images\",\r\n            value=1,\r\n            info=\"Number of images to generate\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        # Video-specific options for Google Veo\r\n        DropdownInput(\r\n            name=\"video_model\",\r\n            display_name=\"Model\",\r\n            options=[\r\n                \"veo-3.0-generate-preview\",  # Latest Veo 3.0 model\r\n                \"veo-2.0-generate-001\",  # Veo 2.0 model (requires GCP billing)\r\n                \"models/veo-2.0-generate-001\",  # Full format for Veo 2.0\r\n            ],\r\n            value=\"veo-3.0-generate-preview\",\r\n            info=\"Veo model to use for video generation\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n        DropdownInput(\r\n            name=\"aspect_ratio\",\r\n            display_name=\"Aspect Ratio\",\r\n            options=[\r\n                \"16:9\",  # Widescreen\r\n                \"9:16\",  # Portrait/Vertical\r\n            ],\r\n            value=\"16:9\",\r\n            info=\"Video format ratio\",\r\n            advanced=True,\r\n            show=False,\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(\r\n            display_name=\"Audio Data\",\r\n            name=\"audio_output\",\r\n            method=\"generate_audio\",\r\n        )\r\n    ]\r\n\r\n    def update_outputs(self, frontend_node: dict, field_name: str, field_value: Any) -> dict:\r\n        \"\"\"Dynamically show only the relevant output based on the selected output type.\"\"\"\r\n        if field_name == \"output_type\":\r\n            # Start with empty outputs\r\n            frontend_node[\"outputs\"] = []\r\n\r\n            # Add only the selected output type\r\n            if field_value == \"Audio\":\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Audio Data\",\r\n                        name=\"audio_output\",\r\n                        method=\"generate_audio\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Audio Base64\",\r\n                        name=\"audio_base64\",\r\n                        method=\"generate_audio_base64\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Markdown\",\r\n                        name=\"markdown_output\",\r\n                        method=\"generate_markdown\",\r\n                    ).to_dict()\r\n                )\r\n            elif field_value == \"Image\":\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Image Data\",\r\n                        name=\"image_output\",\r\n                        method=\"generate_image\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Image URL\",\r\n                        name=\"image_url\",\r\n                        method=\"generate_image_url\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Markdown\",\r\n                        name=\"markdown_output\",\r\n                        method=\"generate_markdown\",\r\n                    ).to_dict()\r\n                )\r\n            elif field_value == \"Video\":\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Video Data\",\r\n                        name=\"video_output\",\r\n                        method=\"generate_video\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Video URLs\",\r\n                        name=\"video_urls\",\r\n                        method=\"generate_video_urls\",\r\n                    ).to_dict()\r\n                )\r\n                frontend_node[\"outputs\"].append(\r\n                    Output(\r\n                        display_name=\"Markdown\",\r\n                        name=\"markdown_output\",\r\n                        method=\"generate_markdown\",\r\n                    ).to_dict()\r\n                )\r\n\r\n        return frontend_node\r\n\r\n    def update_build_config(self, build_config, field_value, field_name=None):\r\n        if field_name == \"output_type\":\r\n            # Extract output type from the selected value\r\n            output_type = field_value if isinstance(field_value, str) else \"Audio\"\r\n\r\n            # Define field visibility map\r\n            field_map = {\r\n                \"Audio\": [\"audio_image_provider\", \"openai_api_key\", \"voice\", \"audio_format\", \"speed\"],\r\n                \"Image\": [\"audio_image_provider\", \"openai_api_key\", \"image_model\", \"image_size\", \"num_images\"],\r\n                \"Video\": [\"video_provider\", \"gemini_api_key\", \"video_model\", \"aspect_ratio\"],\r\n            }\r\n\r\n            # Hide all dynamic fields first\r\n            for field_name in [\"audio_image_provider\", \"video_provider\", \"openai_api_key\", \"gemini_api_key\", \"voice\", \"audio_format\", \"speed\", \"image_model\", \"image_size\", \"num_images\", \"video_model\", \"aspect_ratio\"]:\r\n                if field_name in build_config:\r\n                    build_config[field_name][\"show\"] = False\r\n\r\n            # Show fields based on selected output type\r\n            if output_type in field_map:\r\n                for field_name in field_map[output_type]:\r\n                    if field_name in build_config:\r\n                        build_config[field_name][\"show\"] = True\r\n\r\n        elif field_name == \"audio_image_provider\":\r\n            # For audio and image, always show OpenAI API key\r\n            if field_value == \"OpenAI\":\r\n                build_config[\"openai_api_key\"][\"display_name\"] = \"OpenAI API Key\"\r\n                build_config[\"openai_api_key\"][\"info\"] = \"Your OpenAI API key for generating audio and images\"\r\n                build_config[\"openai_api_key\"][\"show\"] = True\r\n                if \"gemini_api_key\" in build_config:\r\n                    build_config[\"gemini_api_key\"][\"show\"] = False\r\n\r\n        elif field_name == \"video_provider\":\r\n            # For video mode, only show gemini_api_key when Google Generative AI is selected\r\n            if field_value == \"Google Generative AI\":\r\n                if \"gemini_api_key\" in build_config:\r\n                    build_config[\"gemini_api_key\"][\"display_name\"] = \"Gemini API Key\"\r\n                    build_config[\"gemini_api_key\"][\"info\"] = \"Your Google Gemini API key for generating videos with Veo\"\r\n                    build_config[\"gemini_api_key\"][\"show\"] = True\r\n\r\n        return build_config\r\n\r\n    def _extract_text_from_input(self):\r\n        \"\"\"Extract text from various input types.\"\"\"\r\n        input_value = self.input_text[0] if isinstance(self.input_text, list) else self.input_text\r\n\r\n        # Handle string input\r\n        if isinstance(input_value, str):\r\n            return input_value\r\n\r\n        # Handle Message input\r\n        if hasattr(input_value, 'text'):\r\n            return input_value.text\r\n\r\n        # Handle Data input\r\n        if hasattr(input_value, 'data'):\r\n            if isinstance(input_value.data, dict) and 'text' in input_value.data:\r\n                return input_value.data['text']\r\n            elif isinstance(input_value.data, str):\r\n                return input_value.data\r\n\r\n        # Handle DataFrame input\r\n        if hasattr(input_value, 'to_message'):\r\n            message = input_value.to_message()\r\n            return message.text if hasattr(message, 'text') else str(message)\r\n\r\n        # Fallback\r\n        return str(input_value)\r\n\r\n    def _get_api_key(self):\r\n        \"\"\"Get API key based on selected provider and output type.\"\"\"\r\n        # For video generation, always use Gemini API\r\n        if hasattr(self, 'output_type') and self.output_type == \"Video\":\r\n            api_key = self.gemini_api_key\r\n        else:\r\n            # For audio and image, always use OpenAI\r\n            api_key = self.openai_api_key\r\n            \r\n        if hasattr(api_key, 'get_secret_value'):\r\n            return api_key.get_secret_value()\r\n        return str(api_key)\r\n\r\n    def generate_audio(self) -> Data:\r\n        \"\"\"Generate audio from text using OpenAI TTS API.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Data(data={\"error\": \"No text provided for audio generation\"})\r\n\r\n            api_key = self._get_api_key()\r\n            voice = getattr(self, 'voice', 'alloy') or 'alloy'\r\n            format_ = getattr(self, 'audio_format', 'mp3') or 'mp3'\r\n            speed = getattr(self, 'speed', 1) or 1\r\n\r\n            url = \"https://api.openai.com/v1/audio/speech\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {api_key}\",\r\n                \"Content-Type\": \"application/json\"\r\n            }\r\n            payload = {\r\n                \"model\": \"tts-1\",\r\n                \"input\": text,\r\n                \"voice\": voice,\r\n                \"response_format\": format_,\r\n                \"speed\": float(speed)\r\n            }\r\n\r\n            self.status = f\"Generating audio with voice '{voice}' in '{format_}' format at speed {speed}x...\"\r\n            self.log(f\"Making TTS request with voice: {voice}, format: {format_}, speed: {speed}\")\r\n\r\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\r\n            \r\n            if response.status_code == 200 and response.content:\r\n                audio_base64 = base64.b64encode(response.content).decode('utf-8')\r\n                audio_size_kb = len(response.content) / 1024\r\n                \r\n                self.status = f\"Audio generated successfully! Size: {audio_size_kb:.1f} KB\"\r\n                self.log(f\"Audio generated successfully. Size: {len(response.content)} bytes\")\r\n                \r\n                return Data(data={\r\n                    \"audio_base64\": audio_base64,\r\n                    \"format\": format_,\r\n                    \"voice\": voice,\r\n                    \"speed\": speed,\r\n                    \"size_bytes\": len(response.content),\r\n                    \"text\": text[:100] + \"...\" if len(text) > 100 else text,\r\n                })\r\n            else:\r\n                error_msg = f\"API Error {response.status_code}: {response.text}\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Data(data={\"error\": error_msg})\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating audio: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Data(data={\"error\": error_msg})\r\n\r\n    def generate_audio_base64(self) -> Message:\r\n        \"\"\"Generate audio and return only the base64 as text message.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Message(text=\"Error: No text provided for audio generation\")\r\n\r\n            api_key = self._get_api_key()\r\n            voice = getattr(self, 'voice', 'alloy') or 'alloy'\r\n            format_ = getattr(self, 'audio_format', 'mp3') or 'mp3'\r\n            speed = getattr(self, 'speed', 1) or 1\r\n\r\n            url = \"https://api.openai.com/v1/audio/speech\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {api_key}\",\r\n                \"Content-Type\": \"application/json\"\r\n            }\r\n            payload = {\r\n                \"model\": \"tts-1\",\r\n                \"input\": text,\r\n                \"voice\": voice,\r\n                \"response_format\": format_,\r\n                \"speed\": float(speed)\r\n            }\r\n\r\n            self.status = f\"Generating audio with voice '{voice}' in '{format_}' format at speed {speed}x...\"\r\n            self.log(f\"Making TTS request with voice: {voice}, format: {format_}, speed: {speed}\")\r\n\r\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\r\n            \r\n            if response.status_code == 200 and response.content:\r\n                audio_base64 = base64.b64encode(response.content).decode('utf-8')\r\n                audio_size_kb = len(response.content) / 1024\r\n                \r\n                self.status = f\"Audio generated successfully! Size: {audio_size_kb:.1f} KB\"\r\n                self.log(f\"Audio generated successfully. Size: {len(response.content)} bytes\")\r\n                \r\n                return Message(text=audio_base64)\r\n            else:\r\n                error_msg = f\"API Error {response.status_code}: {response.text}\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating audio: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def generate_markdown(self) -> Message:\r\n        \"\"\"Generate markdown output based on the selected output type.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Message(text=\"Error: No text provided for markdown generation\")\r\n\r\n            output_type = getattr(self, 'output_type', 'Audio') or 'Audio'\r\n\r\n            if output_type == \"Audio\":\r\n                return self._generate_audio_markdown(text)\r\n            elif output_type == \"Image\":\r\n                return self._generate_image_markdown(text)\r\n            elif output_type == \"Video\":\r\n                return self._generate_video_markdown(text)\r\n            else:\r\n                return Message(text=\"Error: Unknown output type\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def _generate_audio_markdown(self, text: str) -> Message:\r\n        \"\"\"Generate HTML audio player code for the generated audio.\"\"\"\r\n        try:\r\n            api_key = self._get_api_key()\r\n            voice = getattr(self, 'voice', 'alloy') or 'alloy'\r\n            format_ = getattr(self, 'audio_format', 'mp3') or 'mp3'\r\n            speed = getattr(self, 'speed', 1) or 1\r\n\r\n            url = \"https://api.openai.com/v1/audio/speech\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {api_key}\",\r\n                \"Content-Type\": \"application/json\"\r\n            }\r\n            payload = {\r\n                \"model\": \"tts-1\",\r\n                \"input\": text,\r\n                \"voice\": voice,\r\n                \"response_format\": format_,\r\n                \"speed\": float(speed)\r\n            }\r\n\r\n            self.status = f\"Generating audio with voice '{voice}' in '{format_}' format at speed {speed}x...\"\r\n            self.log(f\"Making TTS request with voice: {voice}, format: {format_}, speed: {speed}\")\r\n\r\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\r\n            \r\n            if response.status_code == 200 and response.content:\r\n                audio_base64 = base64.b64encode(response.content).decode('utf-8')\r\n                audio_size_kb = len(response.content) / 1024\r\n                \r\n                self.status = f\"Audio generated successfully! Size: {audio_size_kb:.1f} KB\"\r\n                self.log(f\"Audio generated successfully. Size: {len(response.content)} bytes\")\r\n                \r\n                # Generate HTML audio player code\r\n                html_code = f'<audio controls>\\n  <source src=\"data:audio/{format_};base64,{audio_base64}\" type=\"audio/{format_}\">\\n</audio>'\r\n                \r\n                return Message(text=html_code)\r\n            else:\r\n                error_msg = f\"API Error {response.status_code}: {response.text}\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating audio markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def _generate_image_markdown(self, text: str) -> Message:\r\n        \"\"\"Generate markdown image code for the generated image.\"\"\"\r\n        try:\r\n            # Check if only one image is requested\r\n            n = getattr(self, 'num_images', 1) or 1\r\n            if n != 1:\r\n                return Message(text=\"Markdown Image output is only available when generating a single image (Number of Images = 1)\")\r\n\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'image_model', 'dall-e-3') or 'dall-e-3'\r\n            size = getattr(self, 'image_size', '1024x1024') or '1024x1024'\r\n\r\n            client = OpenAI(api_key=api_key)\r\n\r\n            self.status = f\"Generating image using {model} model...\"\r\n            self.log(f\"Making image generation request with model: {model}, size: {size}, count: {n}\")\r\n\r\n            response = client.images.generate(\r\n                model=model,\r\n                prompt=text,\r\n                n=n,\r\n                size=size\r\n            )\r\n\r\n            # Return markdown image code for the first image\r\n            if response.data and len(response.data) > 0:\r\n                image_url = response.data[0].url\r\n                # Create a description from the prompt\r\n                description = text[:50] + \"...\" if len(text) > 50 else text\r\n                markdown_code = f\"![{description}]({image_url})\"\r\n                \r\n                self.status = f\"Generated image markdown successfully!\"\r\n                self.log(f\"Generated image markdown: {markdown_code}\")\r\n                \r\n                return Message(text=markdown_code)\r\n            else:\r\n                error_msg = \"No image URL generated\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating image markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def _generate_video_markdown(self, text: str) -> Message:\r\n        \"\"\"Generate HTML video player code for the generated video.\"\"\"\r\n        try:\r\n            # First, generate the video using the main method\r\n            video_result = self.generate_video()\r\n            \r\n            # Check if video generation was successful\r\n            if hasattr(video_result, 'data') and isinstance(video_result.data, dict):\r\n                if 'error' in video_result.data:\r\n                    return Message(text=f\"Error: {video_result.data['error']}\")\r\n                \r\n                # Get the primary video URL\r\n                video_url = video_result.data.get('video_url')\r\n                if not video_url:\r\n                    return Message(text=\"Error: No video URL generated\")\r\n                \r\n                # Determine aspect ratio for dimensions\r\n                aspect_ratio = getattr(self, 'aspect_ratio', '16:9') or '16:9'\r\n                if aspect_ratio == \"16:9\":\r\n                    width, height = 640, 360\r\n                elif aspect_ratio == \"9:16\":\r\n                    width, height = 360, 640\r\n                else:\r\n                    width, height = 640, 360  # Default to 16:9\r\n                \r\n                # Generate HTML video player code\r\n                html_code = f'<video width=\"{width}\" height=\"{height}\" controls>\\n  <source src=\"{video_url}\">\\n</video>'\r\n                \r\n                self.status = f\"Generated video HTML for {aspect_ratio} aspect ratio\"\r\n                return Message(text=html_code)\r\n            else:\r\n                return Message(text=\"Error: Invalid video generation result\")\r\n                \r\n        except Exception as e:\r\n            error_msg = f\"Error generating video markdown: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n    def generate_image(self) -> Data:\r\n        \"\"\"Generate image from text using OpenAI DALL-E API.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Data(data={\"error\": \"No text provided for image generation\"})\r\n\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'image_model', 'dall-e-3') or 'dall-e-3'\r\n            size = getattr(self, 'image_size', '1024x1024') or '1024x1024'\r\n            n = getattr(self, 'num_images', 1) or 1\r\n\r\n            client = OpenAI(api_key=api_key)\r\n\r\n            self.status = f\"Generating image using {model} model...\"\r\n            self.log(f\"Making image generation request with model: {model}, size: {size}, count: {n}\")\r\n\r\n            response = client.images.generate(\r\n                model=model,\r\n                prompt=text,\r\n                n=n,\r\n                size=size\r\n            )\r\n\r\n            image_urls = [data.url for data in response.data]\r\n            self.status = f\"Generated {len(image_urls)} image(s) successfully!\"\r\n            self.log(f\"Generated {len(image_urls)} images\")\r\n\r\n            # Se apenas uma imagem foi gerada, retornar a URL diretamente\r\n            if n == 1 and len(image_urls) == 1:\r\n                return Data(data={\r\n                    \"image_url\": image_urls[0],\r\n                    \"model\": model,\r\n                    \"size\": size,\r\n                    \"count\": n,\r\n                    \"prompt\": text[:100] + \"...\" if len(text) > 100 else text,\r\n                })\r\n            else:\r\n                return Data(data={\r\n                    \"image_urls\": image_urls,\r\n                    \"model\": model,\r\n                    \"size\": size,\r\n                    \"count\": n,\r\n                    \"prompt\": text[:100] + \"...\" if len(text) > 100 else text,\r\n                })\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating image: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Data(data={\"error\": error_msg})\r\n\r\n    def generate_image_url(self) -> Message:\r\n        \"\"\"Generate image and return only the URL as text message.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Message(text=\"Error: No text provided for image generation\")\r\n\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'image_model', 'dall-e-3') or 'dall-e-3'\r\n            size = getattr(self, 'image_size', '1024x1024') or '1024x1024'\r\n            n = getattr(self, 'num_images', 1) or 1\r\n\r\n            client = OpenAI(api_key=api_key)\r\n\r\n            self.status = f\"Generating image using {model} model...\"\r\n            self.log(f\"Making image generation request with model: {model}, size: {size}, count: {n}\")\r\n\r\n            response = client.images.generate(\r\n                model=model,\r\n                prompt=text,\r\n                n=n,\r\n                size=size\r\n            )\r\n\r\n            # Return only the first image URL as text\r\n            if response.data and len(response.data) > 0:\r\n                image_url = response.data[0].url\r\n                self.status = f\"Generated image URL successfully!\"\r\n                self.log(f\"Generated image URL: {image_url}\")\r\n                \r\n                return Message(text=image_url)\r\n            else:\r\n                error_msg = \"No image URL generated\"\r\n                self.status = f\"Error: {error_msg}\"\r\n                return Message(text=f\"Error: {error_msg}\")\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating image URL: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n\r\n\r\n    def generate_video(self) -> Data:\r\n        \"\"\"Generate video from text using Google Veo.\"\"\"\r\n        try:\r\n            text = self._extract_text_from_input()\r\n            if not text or not text.strip():\r\n                return Data(data={\"error\": \"No text provided for video generation\"})\r\n            \r\n            return self._generate_video_with_veo(text)\r\n\r\n        except Exception as e:\r\n            error_msg = f\"Error generating video: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Data(data={\"error\": error_msg})\r\n\r\n    def _generate_video_with_veo(self, text: str) -> Data:\r\n        \"\"\"Generate video using Google Veo.\"\"\"\r\n        try:\r\n            api_key = self._get_api_key()\r\n            model = getattr(self, 'video_model', 'veo-3.0-generate-preview') or 'veo-3.0-generate-preview'\r\n            aspect_ratio = getattr(self, 'aspect_ratio', '16:9') or '16:9'\r\n\r\n            # Create client with API key\r\n            client = genai.Client(api_key=api_key)\r\n\r\n            self.status = f\"Generating video using {model} model...\"\r\n            self.log(f\"Making video generation request with model: {model}, aspect_ratio: {aspect_ratio}\")\r\n\r\n            # Generate video using the selected model\r\n            operation = client.models.generate_videos(\r\n                model=model,\r\n                prompt=text,\r\n                config=types.GenerateVideosConfig(\r\n                    aspect_ratio=aspect_ratio,\r\n                ),\r\n            )\r\n\r\n            self.status = f\"Waiting for video generation completion using {model}...\"\r\n            \r\n            # Poll for completion with proper interval (20 seconds as per documentation)\r\n            while not operation.done:\r\n                time.sleep(20)\r\n                operation = client.operations.get(operation)\r\n\r\n            # Process generated videos\r\n            video_urls = []\r\n            video_data = []\r\n            \r\n            for n, generated_video in enumerate(operation.response.generated_videos):\r\n                if hasattr(generated_video, 'video') and generated_video.video:\r\n                    video_info = {\r\n                        \"video_id\": n,\r\n                        \"video_object\": generated_video.video\r\n                    }\r\n                    \r\n                    # Add URI if available (needs API key appended for download)\r\n                    if hasattr(generated_video.video, 'uri'):\r\n                        video_url = f\"{generated_video.video.uri}&key={api_key}\"\r\n                        video_info[\"video_uri\"] = video_url\r\n                        video_urls.append(video_url)\r\n                    \r\n                    video_data.append(video_info)\r\n\r\n            if not video_data:\r\n                raise ValueError(\"No video was generated.\")\r\n\r\n            self.status = f\"Video(s) generated successfully using {model}. Total: {len(video_data)}\"\r\n            \r\n            # Return the first video URL as main output, with detailed data available\r\n            primary_video_url = video_urls[0] if video_urls else None\r\n            \r\n            return Data(data={\r\n                \"video_url\": primary_video_url,  # Direct link to first video\r\n                \"video_urls\": video_urls,        # List of all links\r\n                \"video_count\": len(video_data),\r\n                \"videos\": video_data,            # Complete data\r\n                \"model_used\": model,             # Model used\r\n                \"prompt_used\": text,\r\n                \"aspect_ratio\": aspect_ratio,\r\n                \"provider\": \"Gemini\"\r\n            })\r\n\r\n        except Exception as e:\r\n            error_message = str(e)\r\n            self.status = f\"Error with model {model}: {error_message}\"\r\n            \r\n            # Provide helpful error info\r\n            return Data(data={\r\n                \"error\": error_message,\r\n                \"model_attempted\": model,\r\n                \"video_count\": 0,\r\n                \"provider\": \"Gemini\",\r\n                \"suggestion\": \"Try using veo-3.0-generate-preview for the latest model, or check your API key and billing setup\"\r\n            })\r\n\r\n    def generate_video_urls(self) -> Message:\r\n        \"\"\"Generate video and return only the URLs as text message.\"\"\"\r\n        try:\r\n            # First, generate the video using the main method\r\n            video_result = self.generate_video()\r\n            \r\n            # Check if video generation was successful\r\n            if hasattr(video_result, 'data') and isinstance(video_result.data, dict):\r\n                if 'error' in video_result.data:\r\n                    return Message(text=f\"Error: {video_result.data['error']}\")\r\n                \r\n                # Get video URLs\r\n                video_urls = video_result.data.get('video_urls', [])\r\n                if not video_urls:\r\n                    return Message(text=\"Error: No video URLs generated\")\r\n                \r\n                # Return URLs as comma-separated text\r\n                urls_text = \", \".join(video_urls)\r\n                self.status = f\"Generated {len(video_urls)} video URL(s) successfully!\"\r\n                return Message(text=urls_text)\r\n            else:\r\n                return Message(text=\"Error: Invalid video generation result\")\r\n                \r\n        except Exception as e:\r\n            error_msg = f\"Error generating video URLs: {str(e)}\"\r\n            self.status = f\"Error: {error_msg}\"\r\n            return Message(text=f\"Error: {error_msg}\")\r\n\r\n\r\n"
              },
              "gemini_api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "Gemini API Key",
                "dynamic": false,
                "info": "Your Google Gemini API key for generating videos with Veo",
                "input_types": [],
                "load_from_db": false,
                "name": "gemini_api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": false,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "image_model": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Image Model",
                "dynamic": false,
                "external_options": {},
                "info": "The DALL·E model version to use",
                "name": "image_model",
                "options": [
                  "dall-e-2",
                  "dall-e-3"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "dall-e-3"
              },
              "image_size": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Image Size",
                "dynamic": false,
                "external_options": {},
                "info": "Size of the generated image",
                "name": "image_size",
                "options": [
                  "256x256",
                  "512x512",
                  "1024x1024",
                  "1792x1024",
                  "1024x1792"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "1024x1024"
              },
              "input_text": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "Text input to convert to audio, image, or video",
                "input_types": [
                  "Message",
                  "Data",
                  "DataFrame"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_text",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "is_refresh": false,
              "num_images": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Number of Images",
                "dynamic": false,
                "info": "Number of images to generate",
                "list": false,
                "list_add_label": "Add More",
                "name": "num_images",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1
              },
              "openai_api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "Your OpenAI API key for generating audio, images, and videos",
                "input_types": [],
                "load_from_db": false,
                "name": "openai_api_key",
                "override_skip": false,
                "password": true,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "output_type": {
                "_input_type": "TabInput",
                "advanced": false,
                "display_name": "Output Type",
                "dynamic": false,
                "info": "Select the desired output media type",
                "name": "output_type",
                "options": [
                  "Audio",
                  "Image",
                  "Video"
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "tab",
                "value": "Image"
              },
              "speed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Speed",
                "dynamic": false,
                "info": "Speed of the generated audio. Values range from 0.25 to 4.0.",
                "list": false,
                "list_add_label": "Add More",
                "name": "speed",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "int",
                "value": 1
              },
              "video_model": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model",
                "dynamic": false,
                "external_options": {},
                "info": "Veo model to use for video generation",
                "name": "video_model",
                "options": [
                  "veo-3.0-generate-preview",
                  "veo-2.0-generate-001",
                  "models/veo-2.0-generate-001"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "veo-3.0-generate-preview"
              },
              "video_provider": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Model Provider",
                "dynamic": false,
                "external_options": {},
                "info": "",
                "name": "video_provider",
                "options": [
                  "Google Generative AI"
                ],
                "options_metadata": [
                  {
                    "icon": "GoogleGenerativeAI"
                  }
                ],
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "Google Generative AI"
              },
              "voice": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Voice",
                "dynamic": false,
                "external_options": {},
                "info": "Select the voice for audio generation",
                "name": "voice",
                "options": [
                  "alloy",
                  "echo",
                  "fable",
                  "onyx",
                  "nova",
                  "shimmer"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": false,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "alloy"
              }
            },
            "tool_mode": false
          },
          "selected_output": "image_output",
          "showNode": true,
          "type": "ModalConverterComponent"
        },
        "dragging": false,
        "id": "ModalConverterComponent-NViIi",
        "measured": {
          "height": 425,
          "width": 320
        },
        "position": {
          "x": 1145.6606782762408,
          "y": 1232.1064923441656
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "DynamicCreateData-rTbao",
          "node": {
            "base_classes": [
              "Data",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Dynamically create a Data with a specified number of fields.",
            "display_name": "Dynamic Create Data",
            "documentation": "",
            "edited": false,
            "field_order": [
              "form_fields",
              "include_metadata"
            ],
            "frozen": false,
            "icon": "ListFilter",
            "last_updated": "2026-01-27T13:37:24.033Z",
            "legacy": false,
            "metadata": {
              "code_hash": "0457c4acdf45",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 1
              },
              "module": "lfx.components.processing.dynamic_create_data.DynamicCreateDataComponent"
            },
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Data",
                "group_outputs": false,
                "loop_types": null,
                "method": "process_form",
                "name": "form_data",
                "options": null,
                "required_inputs": null,
                "selected": "Data",
                "tool_mode": true,
                "types": [
                  "Data"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Message",
                "group_outputs": false,
                "loop_types": null,
                "method": "get_message",
                "name": "message",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_frontend_node_flow_id": {
                "value": "48ce6b32-e37c-4ff6-9b83-95c1569bba78"
              },
              "_frontend_node_folder_id": {
                "value": "3f1d0bf7-bd50-4fa9-a5d3-0974cfa99642"
              },
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from typing import Any\n\nfrom lfx.custom import Component\nfrom lfx.io import (\n    BoolInput,\n    FloatInput,\n    HandleInput,\n    IntInput,\n    MultilineInput,\n    Output,\n    StrInput,\n    TableInput,\n)\nfrom lfx.schema.data import Data\nfrom lfx.schema.message import Message\n\n\nclass DynamicCreateDataComponent(Component):\n    display_name: str = \"Dynamic Create Data\"\n    description: str = \"Dynamically create a Data with a specified number of fields.\"\n    name: str = \"DynamicCreateData\"\n    MAX_FIELDS = 15  # Define a constant for maximum number of fields\n    icon = \"ListFilter\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    inputs = [\n        TableInput(\n            name=\"form_fields\",\n            display_name=\"Input Configuration\",\n            info=(\n                \"Define the dynamic form fields. Each row creates a new input field \"\n                \"that can connect to other components.\"\n            ),\n            table_schema=[\n                {\n                    \"name\": \"field_name\",\n                    \"display_name\": \"Field Name\",\n                    \"type\": \"str\",\n                    \"description\": \"Name for the field (used as both internal name and display label)\",\n                },\n                {\n                    \"name\": \"field_type\",\n                    \"display_name\": \"Field Type\",\n                    \"type\": \"str\",\n                    \"description\": \"Type of input field to create\",\n                    \"options\": [\"Text\", \"Data\", \"Number\", \"Handle\", \"Boolean\"],\n                    \"value\": \"Text\",\n                },\n            ],\n            value=[],\n            real_time_refresh=True,\n        ),\n        BoolInput(\n            name=\"include_metadata\",\n            display_name=\"Include Metadata\",\n            info=\"Include form configuration metadata in the output.\",\n            value=False,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"form_data\", method=\"process_form\"),\n        Output(display_name=\"Message\", name=\"message\", method=\"get_message\"),\n    ]\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None) -> dict:\n        \"\"\"Update build configuration to add dynamic inputs that can connect to other components.\"\"\"\n        if field_name == \"form_fields\":\n            # Clear existing dynamic inputs from build config\n            keys_to_remove = [key for key in build_config if key.startswith(\"dynamic_\")]\n            for key in keys_to_remove:\n                del build_config[key]\n\n            # Add dynamic inputs based on table configuration\n            # Safety check to ensure field_value is not None and is iterable\n            if field_value is None:\n                field_value = []\n\n            for i, field_config in enumerate(field_value):\n                # Safety check to ensure field_config is not None\n                if field_config is None:\n                    continue\n\n                field_name = field_config.get(\"field_name\", f\"field_{i}\")\n                display_name = field_name  # Use field_name as display_name\n                field_type_option = field_config.get(\"field_type\", \"Text\")\n                default_value = \"\"  # All fields have empty default value\n                required = False  # All fields are optional by default\n                help_text = \"\"  # All fields have empty help text\n\n                # Map field type options to actual field types and input types\n                field_type_mapping = {\n                    \"Text\": {\"field_type\": \"multiline\", \"input_types\": [\"Text\", \"Message\"]},\n                    \"Data\": {\"field_type\": \"data\", \"input_types\": [\"Data\"]},\n                    \"Number\": {\"field_type\": \"number\", \"input_types\": [\"Text\", \"Message\"]},\n                    \"Handle\": {\"field_type\": \"handle\", \"input_types\": [\"Text\", \"Data\", \"Message\"]},\n                    \"Boolean\": {\"field_type\": \"boolean\", \"input_types\": None},\n                }\n\n                field_config_mapped = field_type_mapping.get(\n                    field_type_option, {\"field_type\": \"text\", \"input_types\": []}\n                )\n                if not isinstance(field_config_mapped, dict):\n                    field_config_mapped = {\"field_type\": \"text\", \"input_types\": []}\n                field_type = field_config_mapped[\"field_type\"]\n                input_types_list = field_config_mapped[\"input_types\"]\n\n                # Create the appropriate input type based on field_type\n                dynamic_input_name = f\"dynamic_{field_name}\"\n\n                if field_type == \"text\":\n                    if input_types_list:\n                        build_config[dynamic_input_name] = StrInput(\n                            name=dynamic_input_name,\n                            display_name=display_name,\n                            info=f\"{help_text} (Can connect to: {', '.join(input_types_list)})\",\n                            value=default_value,\n                            required=required,\n                            input_types=input_types_list,\n                        )\n                    else:\n                        build_config[dynamic_input_name] = StrInput(\n                            name=dynamic_input_name,\n                            display_name=display_name,\n                            info=help_text,\n                            value=default_value,\n                            required=required,\n                        )\n\n                elif field_type == \"multiline\":\n                    if input_types_list:\n                        build_config[dynamic_input_name] = MultilineInput(\n                            name=dynamic_input_name,\n                            display_name=display_name,\n                            info=f\"{help_text} (Can connect to: {', '.join(input_types_list)})\",\n                            value=default_value,\n                            required=required,\n                            input_types=input_types_list,\n                        )\n                    else:\n                        build_config[dynamic_input_name] = MultilineInput(\n                            name=dynamic_input_name,\n                            display_name=display_name,\n                            info=help_text,\n                            value=default_value,\n                            required=required,\n                        )\n\n                elif field_type == \"number\":\n                    try:\n                        default_int = int(default_value) if default_value else 0\n                    except ValueError:\n                        default_int = 0\n\n                    if input_types_list:\n                        build_config[dynamic_input_name] = IntInput(\n                            name=dynamic_input_name,\n                            display_name=display_name,\n                            info=f\"{help_text} (Can connect to: {', '.join(input_types_list)})\",\n                            value=default_int,\n                            required=required,\n                            input_types=input_types_list,\n                        )\n                    else:\n                        build_config[dynamic_input_name] = IntInput(\n                            name=dynamic_input_name,\n                            display_name=display_name,\n                            info=help_text,\n                            value=default_int,\n                            required=required,\n                        )\n\n                elif field_type == \"float\":\n                    try:\n                        default_float = float(default_value) if default_value else 0.0\n                    except ValueError:\n                        default_float = 0.0\n\n                    if input_types_list:\n                        build_config[dynamic_input_name] = FloatInput(\n                            name=dynamic_input_name,\n                            display_name=display_name,\n                            info=f\"{help_text} (Can connect to: {', '.join(input_types_list)})\",\n                            value=default_float,\n                            required=required,\n                            input_types=input_types_list,\n                        )\n                    else:\n                        build_config[dynamic_input_name] = FloatInput(\n                            name=dynamic_input_name,\n                            display_name=display_name,\n                            info=help_text,\n                            value=default_float,\n                            required=required,\n                        )\n\n                elif field_type == \"boolean\":\n                    default_bool = default_value.lower() in [\"true\", \"1\", \"yes\"] if default_value else False\n\n                    # Boolean fields don't use input_types parameter to avoid errors\n                    build_config[dynamic_input_name] = BoolInput(\n                        name=dynamic_input_name,\n                        display_name=display_name,\n                        info=help_text,\n                        value=default_bool,\n                        input_types=[],\n                        required=required,\n                    )\n\n                elif field_type == \"handle\":\n                    # HandleInput for generic data connections\n                    build_config[dynamic_input_name] = HandleInput(\n                        name=dynamic_input_name,\n                        display_name=display_name,\n                        info=f\"{help_text} (Accepts: {', '.join(input_types_list) if input_types_list else 'Any'})\",\n                        input_types=input_types_list if input_types_list else [\"Data\", \"Text\", \"Message\"],\n                        required=required,\n                    )\n\n                elif field_type == \"data\":\n                    # Specialized for Data type connections\n                    build_config[dynamic_input_name] = HandleInput(\n                        name=dynamic_input_name,\n                        display_name=display_name,\n                        info=f\"{help_text} (Data input)\",\n                        input_types=input_types_list if input_types_list else [\"Data\"],\n                        required=required,\n                    )\n\n                else:\n                    # Default to text input for unknown types\n                    build_config[dynamic_input_name] = StrInput(\n                        name=dynamic_input_name,\n                        display_name=display_name,\n                        info=f\"{help_text} (Unknown type '{field_type}', defaulting to text)\",\n                        value=default_value,\n                        required=required,\n                    )\n\n        return build_config\n\n    def get_dynamic_values(self) -> dict[str, Any]:\n        \"\"\"Extract simple values from all dynamic inputs, handling both manual and connected inputs.\"\"\"\n        dynamic_values = {}\n        connection_info = {}\n        form_fields = getattr(self, \"form_fields\", [])\n\n        for field_config in form_fields:\n            # Safety check to ensure field_config is not None\n            if field_config is None:\n                continue\n\n            field_name = field_config.get(\"field_name\", \"\")\n            if field_name:\n                dynamic_input_name = f\"dynamic_{field_name}\"\n                value = getattr(self, dynamic_input_name, None)\n\n                # Extract simple values from connections or manual input\n                if value is not None:\n                    try:\n                        extracted_value = self._extract_simple_value(value)\n                        dynamic_values[field_name] = extracted_value\n\n                        # Determine connection type for status\n                        if hasattr(value, \"text\") and hasattr(value, \"timestamp\"):\n                            connection_info[field_name] = \"Connected (Message)\"\n                        elif hasattr(value, \"data\"):\n                            connection_info[field_name] = \"Connected (Data)\"\n                        elif isinstance(value, (str, int, float, bool, list, dict)):\n                            connection_info[field_name] = \"Manual input\"\n                        else:\n                            connection_info[field_name] = \"Connected (Object)\"\n\n                    except (AttributeError, TypeError, ValueError):\n                        # Fallback to string representation if all else fails\n                        dynamic_values[field_name] = str(value)\n                        connection_info[field_name] = \"Error\"\n                else:\n                    # Use empty default value if nothing connected\n                    dynamic_values[field_name] = \"\"\n                    connection_info[field_name] = \"Empty default\"\n\n        # Store connection info for status output\n        self._connection_info = connection_info\n        return dynamic_values\n\n    def _extract_simple_value(self, value: Any) -> Any:\n        \"\"\"Extract the simplest, most useful value from any input type.\"\"\"\n        # Handle None\n        if value is None:\n            return None\n\n        # Handle simple types directly\n        if isinstance(value, (str, int, float, bool)):\n            return value\n\n        # Handle lists and tuples - keep simple\n        if isinstance(value, (list, tuple)):\n            return [self._extract_simple_value(item) for item in value]\n\n        # Handle dictionaries - keep simple\n        if isinstance(value, dict):\n            return {str(k): self._extract_simple_value(v) for k, v in value.items()}\n\n        # Handle Message objects - extract only the text\n        if hasattr(value, \"text\"):\n            return str(value.text) if value.text is not None else \"\"\n\n        # Handle Data objects - extract the data content\n        if hasattr(value, \"data\") and value.data is not None:\n            return self._extract_simple_value(value.data)\n\n        # For any other object, convert to string\n        return str(value)\n\n    def process_form(self) -> Data:\n        \"\"\"Process all dynamic form inputs and return clean data with just field values.\"\"\"\n        # Get all dynamic values (just the key:value pairs)\n        dynamic_values = self.get_dynamic_values()\n\n        # Update status with connection info\n        connected_fields = len([v for v in getattr(self, \"_connection_info\", {}).values() if \"Connected\" in v])\n        total_fields = len(dynamic_values)\n\n        self.status = f\"Form processed successfully. {connected_fields}/{total_fields} fields connected to components.\"\n\n        # Return clean Data object with just the field values\n        return Data(data=dynamic_values)\n\n    def get_message(self) -> Message:\n        \"\"\"Return form data as a formatted text message.\"\"\"\n        # Get all dynamic values\n        dynamic_values = self.get_dynamic_values()\n\n        if not dynamic_values:\n            return Message(text=\"No form data available\")\n\n        # Format as text message\n        message_lines = [\"📋 Form Data:\"]\n        message_lines.append(\"=\" * 40)\n\n        for field_name, value in dynamic_values.items():\n            # Use field_name as display_name\n            display_name = field_name\n\n            message_lines.append(f\"• {display_name}: {value}\")\n\n        message_lines.append(\"=\" * 40)\n        message_lines.append(f\"Total fields: {len(dynamic_values)}\")\n\n        message_text = \"\\n\".join(message_lines)\n        self.status = f\"Message formatted with {len(dynamic_values)} fields\"\n\n        return Message(text=message_text)\n"
              },
              "dynamic_audio_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "audio_data",
                "dynamic": false,
                "helper_text": null,
                "info": " (Data input)",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "dynamic_audio_data",
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": null,
                "refresh_button": null,
                "refresh_button_text": null,
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "dynamic_image_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "image_data",
                "dynamic": false,
                "helper_text": null,
                "info": " (Data input)",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "dynamic_image_data",
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": null,
                "refresh_button": null,
                "refresh_button_text": null,
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "dynamic_video_data": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "video_data",
                "dynamic": false,
                "helper_text": null,
                "info": " (Data input)",
                "input_types": [
                  "Data"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "dynamic_video_data",
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": null,
                "refresh_button": null,
                "refresh_button_text": null,
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "form_fields": {
                "_input_type": "TableInput",
                "advanced": false,
                "display_name": "Input Configuration",
                "dynamic": false,
                "info": "Define the dynamic form fields. Each row creates a new input field that can connect to other components.",
                "is_list": true,
                "list_add_label": "Add More",
                "name": "form_fields",
                "override_skip": false,
                "placeholder": "",
                "real_time_refresh": true,
                "required": false,
                "show": true,
                "table_icon": "Table",
                "table_schema": [
                  {
                    "description": "Name for the field (used as both internal name and display label)",
                    "display_name": "Field Name",
                    "formatter": "text",
                    "name": "field_name",
                    "type": "str"
                  },
                  {
                    "description": "Type of input field to create",
                    "display_name": "Field Type",
                    "formatter": "text",
                    "name": "field_type",
                    "options": [
                      "Text",
                      "Data",
                      "Number",
                      "Handle",
                      "Boolean"
                    ],
                    "type": "str",
                    "value": "Text"
                  }
                ],
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "trigger_icon": "Table",
                "trigger_text": "Open table",
                "type": "table",
                "value": [
                  {
                    "field_name": "audio_data",
                    "field_type": "Data"
                  },
                  {
                    "field_name": "video_data",
                    "field_type": "Data"
                  },
                  {
                    "field_name": "image_data",
                    "field_type": "Data"
                  }
                ]
              },
              "include_metadata": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Include Metadata",
                "dynamic": false,
                "info": "Include form configuration metadata in the output.",
                "list": false,
                "list_add_label": "Add More",
                "name": "include_metadata",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": false
              },
              "is_refresh": false
            },
            "tool_mode": false
          },
          "selected_output": "form_data",
          "showNode": true,
          "type": "DynamicCreateData"
        },
        "dragging": false,
        "id": "DynamicCreateData-rTbao",
        "measured": {
          "height": 353,
          "width": 320
        },
        "position": {
          "x": 1531.3790481875335,
          "y": 828.4494077409549
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ChatOutput-oDJcX",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/chat-input-and-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "context_id",
              "data_template",
              "clean_data"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "metadata": {
              "code_hash": "8c87e536cca4",
              "dependencies": {
                "dependencies": [
                  {
                    "name": "orjson",
                    "version": "3.10.15"
                  },
                  {
                    "name": "fastapi",
                    "version": "0.127.0"
                  },
                  {
                    "name": "lfx",
                    "version": "0.2.1"
                  }
                ],
                "total_dependencies": 3
              },
              "module": "lfx.components.input_output.chat_output.ChatOutput"
            },
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean data before converting to string.",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom lfx.base.io.chat import ChatComponent\nfrom lfx.helpers.data import safe_convert\nfrom lfx.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom lfx.schema.data import Data\nfrom lfx.schema.dataframe import DataFrame\nfrom lfx.schema.message import Message\nfrom lfx.schema.properties import Source\nfrom lfx.template.field.base import Output\nfrom lfx.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/chat-input-and-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"context_id\",\n            display_name=\"Context ID\",\n            info=\"The context ID of the chat. Adds an extra layer to the local memory.\",\n            value=\"\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            advanced=True,\n            info=\"Whether to clean data before converting to string.\",\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, _, display_name, source_id = self.get_properties_from_source_component()\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message) and not self.is_connected_to_chat_input():\n            message = self.input_value\n            # Update message properties\n            message.text = text\n            # Preserve existing session_id from the incoming message if it exists\n            existing_session_id = message.session_id\n        else:\n            message = Message(text=text)\n            existing_session_id = None\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        # Preserve session_id from incoming message, or use component/graph session_id\n        message.session_id = (\n            self.session_id or existing_session_id or (self.graph.session_id if hasattr(self, \"graph\") else None) or \"\"\n        )\n        message.context_id = self.context_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n\n        # Store message if needed\n        if message.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            clean_data: bool = getattr(self, \"clean_data\", False)\n            return \"\\n\".join([safe_convert(item, clean_data=clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "context_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Context ID",
                "dynamic": false,
                "info": "The context ID of the chat. Adds an extra layer to the local memory.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "context_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "override_skip": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "external_options": {},
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "track_in_telemetry": false,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "override_skip": false,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "track_in_telemetry": true,
                "type": "bool",
                "value": true
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-oDJcX",
        "measured": {
          "height": 165,
          "width": 320
        },
        "position": {
          "x": 1915.654946729057,
          "y": 828.4494077409549
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": 802.6265257428076,
      "y": -147.4718100539136,
      "zoom": 0.61898911120501
    }
  },
  "description": "Graph Your Way to Great Conversations.",
  "endpoint_name": null,
  "id": "48ce6b32-e37c-4ff6-9b83-95c1569bba78",
  "is_component": false,
  "last_tested_version": "1.7.1",
  "name": "content_cascade_flow",
  "tags": []
}